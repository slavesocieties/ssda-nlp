{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#dependencies\n",
    "\n",
    "#nlp packages\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "#manipulation of tables/arrays/files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json as json\n",
    "import copy\n",
    "\n",
    "#internal imports\n",
    "from ssda_nlp.collate import *\n",
    "from ssda_nlp.split_data import *\n",
    "from ssda_nlp.modeling import *\n",
    "from ssda_nlp.model_performance_utils import *\n",
    "from ssda_nlp.xml_parser import *\n",
    "from ssda_nlp.unstructured2markup import *\n",
    "from ssda_nlp.utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#ideally this function will eventually query the database live, but for now we'll use static snapshots\n",
    "#of the vocabularies as they currently exist\n",
    "#should also eventually be enhanced to return equivalents and a requested language\n",
    "\n",
    "def retrieve_controlled_vocabularies(language = None, types = []):\n",
    "    '''\n",
    "    returns a dictionary containing current version of controlled vocabularies for characteristics\n",
    "    '''\n",
    "    # load in data from synonyms.json\n",
    "    # controlled vocabs are within this file\n",
    "    with open(\"synonyms.json\", encoding=\"utf-8\") as infile:\n",
    "        data = json.load(infile)\n",
    "\n",
    "    # make a dictionary of vocabs\n",
    "    # pair the vocab type with the list of terms\n",
    "    vocabs = {}\n",
    "    for vocab in data[\"controlled_vocabularies\"]:\n",
    "        t = vocab[\"type\"]\n",
    "        if((language is None or vocab[\"language\"] == language)\n",
    "        and (types == [] or t in types)):\n",
    "            vocabs[t] = vocab[\"terms\"]\n",
    "    return vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# refactored and debugged, needs check\n",
    "\n",
    "def build_reciprocal_relationship(people, from_person, to_person, relationship_type):\n",
    "    '''\n",
    "    helper function that adds a reciprocal relationship of a specified type to the records of two people        \n",
    "        people: list of dictionaries, each of which represents one mention of a person in the entry\n",
    "        from_person: unique_id of person that relationship \"comes from\" (i.e. the parent in a \"parent\"-type relationship)\n",
    "        to_person: unique_id of person that relationship \"goes to\" (i.e. the child in a \"parent\"-type relationship)\n",
    "        relationship_type: currently accepts \"parent\", \"godparent\", \"enslaver\", and \"spouse\"\n",
    "    \n",
    "        returns: updated version of people with interpersonal relationship added\n",
    "    '''\n",
    "\n",
    "    #dictionary of relationships\n",
    "    relPairs = {\"godparent\":\"godchild\", \"parent\":\"child\", \"grandparent\":\"grandchild\",\n",
    "    \"enslaver\":\"slave\", \"spouse\":\"spouse\"}\n",
    "\n",
    "    # for each person, match corresponding pair\n",
    "    for person in people:\n",
    "        if person['id'] == to_person:\n",
    "            if person['relationships'] is None:\n",
    "                person['relationships'] = [{\"related_person\": from_person, \"relationship_type\": relationship_type}]\n",
    "            else:\n",
    "                person['relationships'].append({\"related_person\": from_person, \"relationship_type\": relationship_type})\n",
    "        elif person['id'] == from_person:\n",
    "            if person['relationships'] is None:\n",
    "                person['relationships'] = [{\"related_person\": to_person, \"relationship_type\": relPairs[to_person]}]\n",
    "            else:\n",
    "                person['relationships'].append({\"related_person\": to_person, \"relationship_type\": relPairs[to_person]})\n",
    "\n",
    "    return people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def alt_assign_relationships(entry_text, entities, people_df, people, volume_metadata):\n",
    "    '''\n",
    "    matches all labeled relationships to the correct individuals and builds triples\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "        entities: df containing all entities extracted from that entry by an NER model\n",
    "        people_df: entities given the label \"PER\" from a single entry by an NER model with unique ids\n",
    "        people: list of dictionaries, each of which represents one mention of a person in the entry\n",
    "        (as produced by assign_characteristics)\n",
    "        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata        \n",
    "    \n",
    "        returns: updated version of people with interpersonal relationships added\n",
    "    '''\n",
    "    # retrieve relationships from all languages\n",
    "    rel_types = retrieve_controlled_vocabularies(None, [\"relationships\"])\n",
    "\n",
    "    relationships = copy.deepcopy(entities.loc[entities['pred_label'] == 'REL'])\n",
    "    relationships.reset_index(inplace=True)\n",
    "    characteristics = copy.deepcopy(entities.loc[entities['pred_label'] == 'CHAR'])\n",
    "    characteristics.reset_index(inplace=True)    \n",
    "    cat_char, uncat_char = categorize_characteristics(entities, characteristics)    \n",
    "    entities.reset_index(inplace=True)  \n",
    "\n",
    "    principal = 'Unknown Principal' if determine_principals(entry_text, 1) is None else determine_principals(entry_text, 1)[0]\n",
    "\n",
    "    principal_id = False\n",
    "    for person in people:\n",
    "        if person[\"name\"] == principal:\n",
    "            principal_id = person['id']\n",
    "            break\n",
    "\n",
    "    found_parents = False\n",
    "    godparents = []\n",
    "    found_paternal_grandparents = False\n",
    "    found_maternal_grandparents = False\n",
    "    found_enslaver = False\n",
    "    enslaver_id = None\n",
    "             \n",
    "    #build godparent/godchild relationships    \n",
    "    #future improvement: add logic to look for spousal relationship between godparents\n",
    "    if (len(entities) != 0) and (len(relationships) != 0):\n",
    "        for index in range(len(entities)):\n",
    "            if entities['pred_label'][index] == \"REL\":                \n",
    "                if ((entities['pred_entity'][index].lower() == \"madrina\") or (entities['pred_entity'][index].lower() == \"padrino\") or (entities['pred_entity'][index].lower() == \"padryno\")) and (found_godparents == False):                    \n",
    "                    if (len(entities) > (index + 1)) and (entities['pred_label'][index + 1] == \"PER\"):\n",
    "                        for j in range(len(people_df)):\n",
    "                            if people_df['pred_start'][j] == entities['pred_start'][index + 1]:\n",
    "                                from_person = people_df['unique_id'][j]\n",
    "                                people = build_reciprocal_relationship(people, from_person, principal_id, \"godparent\")\n",
    "                                godparent = {\"name\": people_df[\"pred_entity\"][j], \"id\": people_df[\"unique_id\"][j]}\n",
    "                                godparents.append(godparent)\n",
    "                elif ((entities['pred_entity'][index].lower() == \"padrinos\") or (entities['pred_entity'][index].lower() == \"p.p.\")) and (found_godparents == False):\n",
    "                    if (len(entities) > (index + 1)) and (entities['pred_label'][index + 1] == \"PER\"):\n",
    "                        for j in range(len(people_df)):\n",
    "                            if people_df['pred_start'][j] == entities['pred_start'][index + 1]:\n",
    "                                from_person = people_df['unique_id'][j]\n",
    "                                people = build_reciprocal_relationship(people, from_person, principal_id, \"godparent\")\n",
    "                                found_godparents = True\n",
    "                                godparent = {\"name\": people_df[\"pred_entity\"][j], \"id\": people_df[\"unique_id\"][j]}\n",
    "                                godparents.append(godparent)\n",
    "                    if (len(entities) > (index + 2)) and (entities['pred_label'][index + 2] == \"PER\"):\n",
    "                        for j in range(len(people_df)):\n",
    "                            if people_df['pred_start'][j] == entities['pred_start'][index + 2]:\n",
    "                                from_person = people_df['unique_id'][j]\n",
    "                        people = build_reciprocal_relationship(people, from_person, principal_id, \"godparent\")\n",
    "                        godparent = {\"name\": people_df[\"pred_entity\"][j], \"id\": people_df[\"unique_id\"][j]}\n",
    "                        godparents.append(godparent)\n",
    "                elif (\"p.\" in entities['pred_entity'][index].lower()) and len(godparents) < 1:\n",
    "                    if (len(entities) > (index + 1)) and not (\"p.\" in entities['pred_entity'][index + 1].lower()):                        \n",
    "                        if (len(entities) > (index + 1)) and (entities['pred_label'][index + 1] == \"PER\"):\n",
    "                            for j in range(len(people_df)):\n",
    "                                if people_df['pred_start'][j] == entities['pred_start'][index + 1]:\n",
    "                                    from_person = people_df['unique_id'][j]\n",
    "                                    people = build_reciprocal_relationship(people, from_person, principal_id, \"godparent\")\n",
    "                                    godparent = {\"name\": people_df[\"pred_entity\"][j], \"id\": people_df[\"unique_id\"][j]}\n",
    "                                    godparents.append(godparent)\n",
    "                    elif len(entities) > (index + 1):\n",
    "                        if (len(entities) > (index + 2)) and (entities['pred_label'][index + 2] == \"PER\"):\n",
    "                            for j in range(len(people_df)):\n",
    "                                if people_df['pred_start'][j] == entities['pred_start'][index + 2]:\n",
    "                                    from_person = people_df['unique_id'][j]\n",
    "                                    people = build_reciprocal_relationship(people, from_person, principal_id, \"godparent\")\n",
    "                                    godparent = {\"name\": people_df[\"pred_entity\"][j], \"id\": people_df[\"unique_id\"][j]}\n",
    "                                    godparents.append(godparent)\n",
    "                        if (len(entities) > (index + 3)) and (entities['pred_label'][index + 3] == \"PER\"):\n",
    "                            for j in range(len(people_df)):\n",
    "                                if people_df['pred_start'][j] == entities['pred_start'][index + 3]:\n",
    "                                    from_person = people_df['unique_id'][j]\n",
    "                                    people = build_reciprocal_relationship(people, from_person, principal_id, \"godparent\")\n",
    "                                    godparent = {\"name\": people_df[\"pred_entity\"][j], \"id\": people_df[\"unique_id\"][j]}\n",
    "                                    godparents.append(godparent)\n",
    "                #build grandparents\n",
    "                elif \"abuelos\" in entities[\"pred_entity\"][index].lower():\n",
    "                    if (\"paternos\" in entities[\"pred_entity\"][index].lower()) and (found_paternal_grandparents == False):                        \n",
    "                        paternal_grandfather = ''\n",
    "                        paternal_grandmother = ''\n",
    "                        if entities[\"pred_label\"][index + 1] == \"PER\":\n",
    "                            for j in range(len(people_df)):\n",
    "                                if people_df['pred_start'][j] == entities['pred_start'][index + 1]:\n",
    "                                    grandparent_id = people_df['unique_id'][j]\n",
    "                                    break\n",
    "                            if determine_sex(entities[\"pred_entity\"][index + 1].split(' ')[0], name_list=\"names.json\") == \"male\":\n",
    "                                paternal_grandfather = grandparent_id\n",
    "                                paternal_grandfather_index = j\n",
    "                            else: #could add logic to handle other input for sex\n",
    "                                paternal_grandmother = grandparent_id\n",
    "                                paternal_grandmother_index = j\n",
    "                            if entities[\"pred_label\"][index + 2] == \"PER\":\n",
    "                                for j in range(len(people_df)):\n",
    "                                    if people_df['pred_start'][j] == entities['pred_start'][index + 2]:\n",
    "                                        grandparent_id = people_df['unique_id'][j]\n",
    "                                        break\n",
    "                                if (determine_sex(entities[\"pred_entity\"][index + 2].split(' ')[0], name_list=\"names.json\") == \"male\") and (paternal_grandfather == ''):\n",
    "                                    paternal_grandfather = grandparent_id\n",
    "                                    paternal_grandfather_index = j\n",
    "                                elif (determine_sex(entities[\"pred_entity\"][index + 2].split(' ')[0], name_list=\"names.json\") == \"female\") and (paternal_grandmother == ''):\n",
    "                                    paternal_grandmother = grandparent_id\n",
    "                                    paternal_grandmother_index = j\n",
    "                                elif paternal_grandmother == '':\n",
    "                                    paternal_grandmother = grandparent_id\n",
    "                                    paternal_grandmother_index = j\n",
    "                                else:\n",
    "                                    paternal_grandfather = grandparent_id\n",
    "                                    paternal_grandfather_index = j\n",
    "                        if paternal_grandfather != '':\n",
    "                            found_paternal_grandparents = True\n",
    "                            people = build_reciprocal_relationship(people, paternal_grandfather, principal_id, \"grandparent\")\n",
    "                        if paternal_grandmother != '':\n",
    "                            found_paternal_grandparents = True\n",
    "                            people = build_reciprocal_relationship(people, paternal_grandmother, principal_id, \"grandparent\")\n",
    "                        if (paternal_grandfather != '') and (paternal_grandmother != ''):\n",
    "                            people = build_reciprocal_relationship(people, paternal_grandfather, paternal_grandmother, \"spouse\")\n",
    "                    elif (\"matern\" in entities[\"pred_entity\"][index].lower()) and (found_maternal_grandparents == False):                        \n",
    "                        maternal_grandfather = ''\n",
    "                        maternal_grandmother = ''\n",
    "                        if entities[\"pred_label\"][index + 1] == \"PER\":\n",
    "                            for j in range(len(people_df)):\n",
    "                                if people_df['pred_start'][j] == entities['pred_start'][index + 1]:\n",
    "                                    grandparent_id = people_df['unique_id'][j]\n",
    "                                    break\n",
    "                            if determine_sex(entities[\"pred_entity\"][index + 1].split(' ')[0], name_list=\"names.json\") == \"male\":\n",
    "                                maternal_grandfather = grandparent_id\n",
    "                                maternal_grandfather_index = j\n",
    "                            elif determine_sex(entities[\"pred_entity\"][index + 1].split(' ')[0], name_list=\"names.json\") == \"female\":\n",
    "                                maternal_grandmother = grandparent_id\n",
    "                                maternal_grandmother_index = j\n",
    "                            else:\n",
    "                                maternal_grandmother = grandparent_id\n",
    "                                maternal_grandmother_index = j\n",
    "                            if entities[\"pred_label\"][index + 2] == \"PER\":\n",
    "                                for j in range(len(people_df)):\n",
    "                                    if people_df['pred_start'][j] == entities['pred_start'][index + 2]:\n",
    "                                        grandparent_id = people_df['unique_id'][j]\n",
    "                                        break\n",
    "                                if (determine_sex(entities[\"pred_entity\"][index + 2].split(' ')[0], name_list=\"names.json\") == \"male\") and (maternal_grandfather == ''):\n",
    "                                    maternal_grandfather = grandparent_id\n",
    "                                    maternal_grandfather_index = j\n",
    "                                elif (determine_sex(entities[\"pred_entity\"][index + 2].split(' ')[0], name_list=\"names.json\") == \"female\") and (maternal_grandmother == ''):\n",
    "                                    maternal_grandmother = grandparent_id\n",
    "                                    maternal_grandmother_index = j\n",
    "                                elif maternal_grandmother == '':\n",
    "                                    maternal_grandmother = grandparent_id\n",
    "                                    maternal_grandmother_index = j\n",
    "                                else:\n",
    "                                    maternal_grandfather = grandparent_id\n",
    "                                    maternal_grandfather_index = j\n",
    "                        if maternal_grandfather != '':\n",
    "                            found_maternal_grandparents = True\n",
    "                            people = build_reciprocal_relationship(people, maternal_grandfather, principal_id, \"grandparent\")\n",
    "                        if maternal_grandmother != '':\n",
    "                            found_maternal_grandparents = True\n",
    "                            people = build_reciprocal_relationship(people, maternal_grandmother, principal_id, \"grandparent\")\n",
    "                        if (maternal_grandfather != '') and (maternal_grandmother != ''):                            \n",
    "                            people = build_reciprocal_relationship(people, maternal_grandfather, maternal_grandmother, \"spouse\")\n",
    "                elif (\"matern\" in entities[\"pred_entity\"][index].lower()) and found_paternal_grandparents and (found_maternal_grandparents == False):\n",
    "                    maternal_grandfather = ''\n",
    "                    maternal_grandmother = ''\n",
    "                    if entities[\"pred_label\"][index + 1] == \"PER\":\n",
    "                        for j in range(len(people_df)):\n",
    "                            if people_df['pred_start'][j] == entities['pred_start'][index + 1]:\n",
    "                                grandparent_id = people_df['unique_id'][j]\n",
    "                                break\n",
    "                        if determine_sex(entities[\"pred_entity\"][index + 1].split(' ')[0], name_list=\"names.json\") == \"male\":\n",
    "                            maternal_grandfather = grandparent_id\n",
    "                            maternal_grandfather_index = j\n",
    "                        else:\n",
    "                            maternal_grandmother = grandparent_id\n",
    "                            maternal_grandmother_index = j\n",
    "\n",
    "                        if entities[\"pred_label\"][index + 2] == \"PER\":\n",
    "                            for j in range(len(people_df)):\n",
    "                                if people_df['pred_start'][j] == entities['pred_start'][index + 2]:\n",
    "                                    grandparent_id = people_df['unique_id'][j]\n",
    "                                    break\n",
    "                            if (determine_sex(entities[\"pred_entity\"][index + 2].split(' ')[0], name_list=\"names.json\") == \"male\") and (maternal_grandfather == ''):\n",
    "                                maternal_grandfather = grandparent_id\n",
    "                                maternal_grandfather_index = j\n",
    "                            elif (determine_sex(entities[\"pred_entity\"][index + 2].split(' ')[0], name_list=\"names.json\") == \"female\") and (maternal_grandmother == ''):\n",
    "                                maternal_grandmother = grandparent_id\n",
    "                                maternal_grandmother_index = j\n",
    "                            elif maternal_grandmother == '':\n",
    "                                maternal_grandmother = grandparent_id\n",
    "                                maternal_grandmother_index = j\n",
    "                            else:\n",
    "                                maternal_grandfather = grandparent_id\n",
    "                                maternal_grandfather_index = j\n",
    "                    if maternal_grandfather != '':\n",
    "                        found_maternal_grandparents = True\n",
    "                        people = build_reciprocal_relationship(people, maternal_grandfather, principal_id, \"grandparent\")\n",
    "                    if maternal_grandmother != '':\n",
    "                        found_maternal_grandparents = True\n",
    "                        people = build_reciprocal_relationship(people, maternal_grandmother, principal_id, \"grandparent\")\n",
    "                    if (maternal_grandfather != '') and (maternal_grandmother != ''):\n",
    "                            people = build_reciprocal_relationship(people, maternal_grandfather, maternal_grandmother, \"spouse\")\n",
    "                \n",
    "                elif (found_parents == False) and (found_godparents == False) and (found_paternal_grandparents == False) and (found_maternal_grandparents == False) and (found_enslaver == False):\n",
    "                    #ie if after all these checks, there are still no relationships found, then we have a case where we have a relationship but no assignment\n",
    "                    #Note that this relies on setting ALL to FOUND by default, so I don't have to add to the code above each time\n",
    "                    #Thus, we only flip it in the case that no relationships are found\n",
    "\n",
    "                    entities.loc[index, \"assigned\"] = False\n",
    "                    #print(\"Failed to find a category for relationship: \" + entities[\"pred_entity\"][index])\n",
    "                    \n",
    "                \n",
    "                \n",
    "    if len(godparents) == 2:\n",
    "        first_godparent_sex = determine_sex(godparents[0][\"name\"].split(' ')[0], name_list=\"names.json\")\n",
    "        second_godparent_sex = determine_sex(godparents[1][\"name\"].split(' ')[0], name_list=\"names.json\")\n",
    "        #if (first_godparent_sex != second_godparent_sex) or (first_godparent_sex == \"unknown\" and second_godparent_sex == \"unknown\"):\n",
    "            #print(\"found possible godparent couple: \")\n",
    "            #print(godparents[0][\"name\"])\n",
    "            #print(godparents[1][\"name\"])\n",
    "    \n",
    "    for i in range(len(cat_char)):\n",
    "        #build enslaver/enslaved person relationships\n",
    "        if cat_char[\"category\"][i] == \"status\":            \n",
    "            #skip if associated with first mention of principal\n",
    "            char_start = cat_char['pred_start'][i]\n",
    "            if char_start <= 25:\n",
    "                continue            \n",
    "            \n",
    "            #match enslaved couple to owner\n",
    "            if cat_char[\"pred_entity\"][i].lower()[len(cat_char[\"pred_entity\"][i]) - 1] == 's':\n",
    "                close_ep = -1\n",
    "                far_ep = -1\n",
    "                ens = -1\n",
    "                for j in range(len(people_df)):\n",
    "                    pers_start = people_df[\"pred_start\"][j]\n",
    "                    poss_diff = char_start - pers_start\n",
    "                    if (ens == -1) and (poss_diff < 0) and (abs(poss_diff) < 25):\n",
    "                        ens = j\n",
    "                    elif (ens != -1) and (poss_diff < 0) and (abs(poss_diff) < abs(char_start - people_df[\"pred_start\"][ens])):\n",
    "                        ens = j\n",
    "                    elif (close_ep == -1) and (poss_diff > 0) and (poss_diff < 50):\n",
    "                        close_ep = j\n",
    "                    elif (close_ep != -1) and (far_ep == -1) and (poss_diff > 0) and (poss_diff < char_start - people_df[\"pred_start\"][close_ep]):\n",
    "                        far_ep = close_ep\n",
    "                        close_ep = j\n",
    "                    elif (close_ep != -1) and (far_ep == -1) and (poss_diff > 0) and (poss_diff < 50):\n",
    "                        far_ep = j\n",
    "                    elif (close_ep != -1) and (far_ep != -1) and (poss_diff > 0) and (poss_diff < char_start - people_df[\"pred_start\"][close_ep]):\n",
    "                        far_ep = close_ep\n",
    "                        close_ep = j\n",
    "                    elif (close_ep != -1) and (far_ep != -1) and (poss_diff > 0) and (poss_diff < char_start - people_df[\"pred_start\"][far_ep]):\n",
    "                        far_ep = j\n",
    "                if (ens != -1) and (close_ep != -1) and (far_ep != -1):\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][ens], people_df[\"unique_id\"][close_ep], \"enslaver\")\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][ens], people_df[\"unique_id\"][far_ep], \"enslaver\")\n",
    "                elif (ens != -1) and (close_ep != -1):\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][ens], people_df[\"unique_id\"][close_ep], \"enslaver\")\n",
    "            #match enslaved person to owner        \n",
    "            elif \"propiedad\" in cat_char[\"pred_entity\"][i].lower():\n",
    "                for j in range(len(entities)):\n",
    "                    if entities[\"pred_start\"][j] == cat_char[\"pred_start\"][i]:\n",
    "                        signal_entity_index = j\n",
    "                        break                \n",
    "                if found_enslaver and (entry_text.rfind(\"misma\", cat_char[\"pred_start\"][i] - 25, cat_char[\"pred_start\"][i]) != -1):\n",
    "                    if (entities[\"pred_label\"][signal_entity_index - 1] == \"PER\") and (cat_char[\"pred_start\"][i] - entities[\"pred_end\"][signal_entity_index - 1] <= 20):\n",
    "                        people = build_reciprocal_relationship(people, enslaver_id, entities[\"unique_id\"][signal_entity_index - 1], \"enslaver\")\n",
    "                        if (entities[\"pred_label\"][signal_entity_index - 2] == \"PER\") and (entities[\"pred_end\"][signal_entity_index - 2] - entities[\"pred_start\"][signal_entity_index - 1] <= 5):\n",
    "                            people = build_reciprocal_relationship(people, enslaver_id, entities[\"unique_id\"][signal_entity_index - 2], \"enslaver\")\n",
    "                elif (entities[\"pred_label\"][signal_entity_index + 1] == \"PER\") and ((entities[\"pred_start\"][signal_entity_index + 1] - cat_char[\"pred_start\"][i]) <= 25):\n",
    "                    for j in range(len(people_df)):\n",
    "                        if people_df['pred_start'][j] == entities['pred_start'][signal_entity_index + 1]:\n",
    "                            found_enslaver = True\n",
    "                            enslaver_id = people_df[\"unique_id\"][j]\n",
    "                            people = build_reciprocal_relationship(people, enslaver_id, principal_id, \"enslaver\")\n",
    "                            break               \n",
    "            else:\n",
    "                ep = -1\n",
    "                ens = -1\n",
    "                for k in range(len(people_df)):\n",
    "                    pers_start = people_df[\"pred_start\"][k]\n",
    "                    poss_diff = char_start - pers_start\n",
    "                    if (ep == -1) and (poss_diff > 0) and (poss_diff < 50):\n",
    "                        ep = k\n",
    "                    elif (ens == -1) and (poss_diff < 0) and (abs(poss_diff) < 25):\n",
    "                        ens = k                        \n",
    "                    elif (ep != -1) and (poss_diff > 0) and (poss_diff < char_start - people_df[\"pred_start\"][ep]):\n",
    "                        ep = k\n",
    "                    elif (ens != -1) and (poss_diff < 0) and (abs(poss_diff) < abs(char_start - people_df[\"pred_start\"][ens])):\n",
    "                        ens = k\n",
    "                if (ep != -1) and (ens != -1):\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][ens], people_df[\"unique_id\"][ep], \"enslaver\")\n",
    "        #build parent/child relationships\n",
    "        elif (((cat_char[\"category\"][i] == \"relationships\") and ((cat_char[\"pred_entity\"][i] == \"hijo\") or (cat_char[\"pred_entity\"][i] == \"hija\") or (cat_char[\"pred_entity\"][i] == \"h\") or (cat_char[\"pred_entity\"][i] == \"h.\"))) or (cat_char[\"category\"][i] == \"legitimacy\")) and (found_parents == False):\n",
    "            rel_start = cat_char[\"pred_start\"][i]\n",
    "            close_parent = -1\n",
    "            far_parent = -1\n",
    "            for l in range(len(people_df)):\n",
    "                pers_start = people_df[\"pred_start\"][l]\n",
    "                poss_diff = rel_start - pers_start\n",
    "                if (close_parent == -1) and (poss_diff < 0) and (abs(poss_diff) < 25):\n",
    "                    close_parent = l\n",
    "                elif (close_parent != -1) and (far_parent == -1) and (poss_diff < 0) and (abs(poss_diff) < abs(rel_start - people_df[\"pred_start\"][close_parent])):\n",
    "                    far_parent = close_parent\n",
    "                    close_parent = l\n",
    "                elif (close_parent != -1) and (far_parent == -1) and (poss_diff < 0) and ((pers_start - people_df[\"pred_end\"][close_parent]) < 10):\n",
    "                    far_parent = l\n",
    "                elif (close_parent != -1) and (far_parent != -1) and (poss_diff > 0) and (abs(poss_diff) < abs(rel_start - people_df[\"pred_start\"][close_parent])):\n",
    "                    far_parent = close_parent\n",
    "                    close_parent = l\n",
    "                elif (close_parent != -1) and (far_parent != -1) and (poss_diff > 0) and (abs(poss_diff) < abs(rel_start - people_df[\"pred_start\"][far_parent])):\n",
    "                    far_parent = l\n",
    "            if (close_parent != -1) and (far_parent != -1):\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][close_parent], principal_id, \"parent\")\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][far_parent], principal_id, \"parent\")\n",
    "                if ((cat_char[\"category\"][i] == \"legitimacy\") and ('r' not in cat_char[\"pred_entity\"][i])) or ((cat_char[\"category\"][i] == \"relationships\") and ((cat_char['pred_entity'][i] == 'h') or (cat_char['pred_entity'][i] == 'h.')) and ((entry_text[cat_char[\"pred_end\"][i]] == 'l') or (entry_text[cat_char[\"pred_end\"][i] + 1] == 'l'))):\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][close_parent], people_df[\"unique_id\"][far_parent], \"spouse\")\n",
    "                #future improvement (after normalization) if both parents enslaved and child not free, make sure child's status is enslaved\n",
    "                #future improvement (after normalization) if child is enslaved, make sure reciprocal enslaver/enslaved person relationship exists with mother's enslaver                \n",
    "                found_parents = True\n",
    "            elif close_parent != -1:\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][close_parent], principal_id, \"parent\")\n",
    "                #future improvement (after normalization) if single parent is mother and she is enslaved and child not free, make sure child's status is enslaved\n",
    "                #future improvement (after normalization) if child is enslaved, make sure reciprocal enslaver/enslaved person relationship exists with mother's enslaver\n",
    "                found_parents = True\n",
    "                \n",
    "    #build parent-child relationships between parents and grandparents\n",
    "    if found_parents and found_paternal_grandparents:\n",
    "        if (far_parent != -1) and (determine_sex(people_df[\"pred_entity\"][far_parent].split(' ')[0]) == \"male\"):\n",
    "            if (paternal_grandmother != '') and (paternal_grandfather != ''):\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandmother_index], people_df[\"unique_id\"][far_parent], \"parent\")\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandfather_index], people_df[\"unique_id\"][far_parent], \"parent\")\n",
    "            elif paternal_grandmother != '':\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandmother_index], people_df[\"unique_id\"][far_parent], \"parent\")        \n",
    "            else:\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandfather_index], people_df[\"unique_id\"][far_parent], \"parent\")\n",
    "        elif (close_parent != -1) and (determine_sex(people_df[\"pred_entity\"][close_parent].split(' ')[0]) == \"male\"):\n",
    "            if (paternal_grandmother != '') and (paternal_grandfather != ''):\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandmother_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandfather_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "            elif paternal_grandmother != '':\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandmother_index], people_df[\"unique_id\"][close_parent], \"parent\")        \n",
    "            else:                \n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandfather_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "    if found_parents and found_maternal_grandparents:\n",
    "        if (close_parent != -1)  and (determine_sex(people_df[\"pred_entity\"][close_parent].split(' ')[0]) == \"female\"):\n",
    "            if (maternal_grandmother != '') and (maternal_grandfather != ''):\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandmother_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandfather_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "            elif maternal_grandmother != '':\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandmother_index], people_df[\"unique_id\"][close_parent], \"parent\")        \n",
    "            else:                \n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandfather_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "        elif (far_parent != -1) and (determine_sex(people_df[\"pred_entity\"][far_parent].split(' ')[0]) == \"female\"):\n",
    "            if (maternal_grandmother != '') and (maternal_grandfather != ''):\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandmother_index], people_df[\"unique_id\"][far_parent], \"parent\")\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandfather_index], people_df[\"unique_id\"][far_parent], \"parent\")\n",
    "            elif maternal_grandmother != '':\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandmother_index], people_df[\"unique_id\"][far_parent], \"parent\")        \n",
    "            else:                \n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandfather_index], people_df[\"unique_id\"][far_parent], \"parent\")\n",
    "        elif (close_parent != -1) and (far_parent == -1):\n",
    "            if (maternal_grandmother != '') and (maternal_grandfather != ''):\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandmother_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandfather_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "            elif maternal_grandmother != '':\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandmother_index], people_df[\"unique_id\"][close_parent], \"parent\")        \n",
    "            else:                \n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandfather_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "    \n",
    "    return people, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# refactored and debugged, needs check\n",
    "\n",
    "def categorize_characteristics(entities_df, characteristics_df):\n",
    "    '''\n",
    "    determines which category each labeled characteristic belongs to        \n",
    "        characteristics_df: entities given the label \"CHAR\" from a single entry by an NER model        \n",
    "    \n",
    "        returns: the same dataframe with an additional column containing a characteristic category\n",
    "    '''\n",
    "    \n",
    "    vocabs = retrieve_controlled_vocabularies() #retrieves all vocabs\n",
    "    categories = []\n",
    "    uncategorized_characteristics = pd.DataFrame({\"entry_no\": pd.Series([], dtype=\"str\"), \"pred_entity\": pd.Series([], dtype=\"str\"), \"pred_label\": pd.Series([], dtype=\"str\"), \"pred_start\": pd.Series([], dtype=\"int\"), \"pred_end\": pd.Series([], dtype=\"int\"), \"assigned\": pd.Series([], dtype=\"bool\")}) \n",
    "    entities_df.reset_index(inplace = True, drop = True)\n",
    "    naturalList = [\"Natural\", \"nral\", \"Nat.l\", \"N.l\", \"nat.l\", \"natural\"]\n",
    "    \n",
    "    for index, characteristic in entities_df.iterrows():\n",
    "        pred_entity = characteristic['pred_entity']\n",
    "        if characteristic[\"pred_label\"] != \"CHAR\":\n",
    "            continue\n",
    "        category = None       \n",
    "        if (pred_entity in naturalList) and (entities_df.iloc[index + 1][\"pred_label\"] == \"LOC\"):\n",
    "            category = \"origin\"\n",
    "        elif pred_entity in naturalList:\n",
    "            category = \"legitimacy\"\n",
    "\n",
    "        # iterate over categories, looking for match to relationships or status\n",
    "        for cat in vocabs:\n",
    "            if pred_entity in [\"h\",\"h.\"]:\n",
    "                category = \"relationships\"\n",
    "            elif pred_entity == \"propiedad\":\n",
    "                category = \"status\"\n",
    "            # break if category is found\n",
    "            if category is not None:\n",
    "                break\n",
    "            # iterate over each term searching for a match\n",
    "            for term in vocabs[cat]:\n",
    "                if term in pred_entity.lower():\n",
    "                    category = cat\n",
    "                    break\n",
    "                        \n",
    "        if category is None:\n",
    "            uncategorized_characteristics = uncategorized_characteristics.append(entities_df.iloc[index])\n",
    "            \n",
    "        categories.append(category)\n",
    "        \n",
    "    characteristics_df[\"category\"] = categories\n",
    "    uncategorized_characteristics[\"category\"] = None    \n",
    "    \n",
    "    return characteristics_df, uncategorized_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#this is currently configured specifically for baptisms/burials\n",
    "\n",
    "def assign_characteristics(entry_text, entities_df, characteristics_df, unique_individuals, volume_metadata):\n",
    "    '''\n",
    "    matches all labeled characteristics to the correct individual(s) and builds triples\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "        characteristics_df: entities given the label \"CHAR\" from a single entry by an NER model\n",
    "        unique_individuals: as determined by id_unique_individuals and/or meta-function of disambig pipeline\n",
    "        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata        \n",
    "    \n",
    "        returns: structured representation (a list of dictionaries)\n",
    "    '''\n",
    "\n",
    "    # added minor fixes (accessing curTrait once for clarity, simplifying chained ORs)\n",
    "    people = []\n",
    "    ethnicities = retrieve_controlled_vocabularies(None, [\"ethnicities\"])\n",
    "    categorized_characteristics, uncat_char = categorize_characteristics(entities_df, characteristics_df)\n",
    "    assignments = [None] * len(characteristics_df.index)    \n",
    "    categorized_characteristics.reset_index(inplace=True)\n",
    "    unique_individuals.reset_index(inplace=True)    \n",
    "    \n",
    "    for index in range(len(categorized_characteristics)):\n",
    "        #development\n",
    "        #if categorized_characteristics[\"pred_entity\"][index] == \"libre\":\n",
    "            #print(\"libre\")\n",
    "        curTrait = categorized_characteristics[\"category\"][index]\n",
    "        if curTrait in [\"age\",\"legitimacy\"] and volume_metadata[\"type\"] == \"baptism\":\n",
    "            principal = determine_principals(entry_text, unique_individuals, 1)\n",
    "            if principal is not None:\n",
    "                principal = determine_principals(entry_text, unique_individuals, 1)[0]\n",
    "            else:\n",
    "                principal = \"Unknown principal\"\n",
    "            princ_loc = unique_individuals.index[unique_individuals[\"pred_entity\"] == principal].tolist()\n",
    "            for loc in princ_loc:\n",
    "                if assignments[index] is None:\n",
    "                    assignments[index] = unique_individuals[\"unique_id\"][loc]\n",
    "                else:\n",
    "                    assignments[index] += ';' + unique_individuals[\"unique_id\"][loc]\n",
    "        elif curTrait in [\"occupation\", \"phenotype\", \"ethnicities\"] or (curTrait == \"status\" and categorized_characteristics[\"pred_entity\"][index].lower()[-1] != 's'):\n",
    "            char_start = categorized_characteristics[\"pred_start\"][index]\n",
    "            lowest_diff = 50\n",
    "            assign = None\n",
    "            for i, person in unique_individuals.iterrows():\n",
    "                person_start = person[\"pred_start\"]\n",
    "                diff = char_start - person_start\n",
    "                if (diff > 0) and (diff < lowest_diff):\n",
    "                    lowest_diff = diff\n",
    "                    assign = i\n",
    "            if assign is not None:\n",
    "                assignments[index] = unique_individuals[\"unique_id\"][assign]\n",
    "        elif curTrait == \"status\":\n",
    "            char_start = categorized_characteristics[\"pred_start\"][index]\n",
    "            lowest_diff = 30\n",
    "            second_lowest_diff = 50\n",
    "            assign = [None, None]\n",
    "            for i, person in unique_individuals.iterrows():\n",
    "                person_start = person[\"pred_start\"]\n",
    "                diff = char_start - person_start\n",
    "                if (diff > 0) and (diff < lowest_diff):\n",
    "                    lowest_diff = diff\n",
    "                    if assign[0] is not None:\n",
    "                        assign[1] = assign[0]\n",
    "                        second_lowest_diff = lowest_diff\n",
    "                    assign[0] = i\n",
    "                elif (diff > 0) and (diff < second_lowest_diff) and (assign[0] is not None):\n",
    "                    second_lowest_diff = diff\n",
    "                    assign[1] = i\n",
    "            ids = []\n",
    "            for a in assign:\n",
    "                if a is not None:\n",
    "                    ids.append(unique_individuals[\"unique_id\"][a])\n",
    "            if len(ids) == 2:\n",
    "                assignments[index] = ids[0] + ';' + ids[1]\n",
    "            elif len(ids) == 1:\n",
    "                assignments[index] = ids[0]\n",
    "        elif curTrait == \"origin\":\n",
    "            for i, entity in entities_df.iterrows():                \n",
    "                if entity[\"pred_start\"] == categorized_characteristics[\"pred_start\"][index]:\n",
    "                    signal_entity_index = i\n",
    "                    break            \n",
    "            if signal_entity_index != 0 and (len(entities_df[\"pred_label\"]) > signal_entity_index + 1) and entities_df[\"pred_label\"][signal_entity_index - 1] == \"PER\" and entities_df[\"pred_label\"][signal_entity_index + 1] == \"LOC\" and entities_df[\"pred_start\"][signal_entity_index + 1] - entities_df[\"pred_end\"][signal_entity_index - 1] <= 20:\n",
    "                place = entities_df[\"pred_entity\"][signal_entity_index + 1]\n",
    "                multiple = False\n",
    "                if categorized_characteristics[\"pred_entity\"][index] == \"naturales\":\n",
    "                    multiple = True\n",
    "                categorized_characteristics.at[index, \"pred_entity\"] = place\n",
    "                for i, person in unique_individuals.iterrows():\n",
    "                    if person[\"pred_start\"] == entities_df[\"pred_start\"][signal_entity_index - 1]:\n",
    "                        assignments[index] = person[\"unique_id\"]\n",
    "                        break\n",
    "                if multiple and (entities_df[\"pred_label\"][signal_entity_index - 2] == \"PER\") and (entities_df[\"pred_start\"][signal_entity_index - 1] - entities_df[\"pred_end\"][signal_entity_index - 2] <= 10):\n",
    "                    for i, person in unique_individuals.iterrows():\n",
    "                        if person[\"pred_start\"] == entities_df[\"pred_start\"][signal_entity_index - 2]:\n",
    "                            assignments[index] += ';' + person[\"unique_id\"]                            \n",
    "                            break\n",
    "            elif (len(entities_df[\"pred_label\"]) > (signal_entity_index + 1)) and (entities_df[\"pred_label\"][signal_entity_index + 1] == \"LOC\"):\n",
    "                place = entities_df[\"pred_entity\"][signal_entity_index + 1]                \n",
    "                categorized_characteristics.at[index, \"pred_entity\"] = place\n",
    "                principal = determine_principals(entry_text, unique_individuals, 1)\n",
    "                if principal is not None:\n",
    "                    principal = determine_principals(entry_text, unique_individuals, 1)[0]\n",
    "                else:\n",
    "                    principal = \"Unknown principal\"\n",
    "                princ_loc = unique_individuals.index[unique_individuals[\"pred_entity\"] == principal].tolist()\n",
    "                for loc in princ_loc:\n",
    "                    if assignments[index] is None:\n",
    "                        assignments[index] = unique_individuals[\"unique_id\"][loc]\n",
    "                    else:\n",
    "                        assignments[index] += ';' + unique_individuals[\"unique_id\"][loc]\n",
    "\n",
    "            \n",
    "    categorized_characteristics[\"assignment\"] = assignments    \n",
    "    \n",
    "    for i in range(len(unique_individuals.index)):        \n",
    "        \n",
    "        characteristics = {\"origin\": None, \"ethnicities\":[], \"age\":None, \"legitimacy\":None,\"occupation\":[], \"phenotype\":[], \"status\":None, \"titles\":None, \"ranks\":None, \"relationships\":None}\n",
    "        \n",
    "        for eth in ethnicities:\n",
    "            if eth in unique_individuals[\"pred_entity\"][i].lower():                \n",
    "                characteristics[\"ethnicities\"].append(eth[0].upper() + eth[1:])        \n",
    "        \n",
    "        for j in range(len(categorized_characteristics.index)):\n",
    "            if categorized_characteristics[\"assignment\"][j] is None:\n",
    "                continue\n",
    "            if unique_individuals[\"unique_id\"][i] in categorized_characteristics[\"assignment\"][j]:\n",
    "                if categorized_characteristics[\"category\"][j] in [\"origin\",\"age\",\"legitimacy\",\"status\"]:\n",
    "                    characteristics[categorized_characteristics[\"category\"][j]] = categorized_characteristics[\"pred_entity\"][j]\n",
    "                else:\n",
    "                    characteristics[categorized_characteristics[\"category\"][j]].append(categorized_characteristics[\"pred_entity\"][j])\n",
    "        \n",
    "        person_record = {\"id\": unique_individuals[\"unique_id\"][i], \"name\": unique_individuals[\"pred_entity\"][i]}\n",
    "        \n",
    "        for key in characteristics:\n",
    "            if key in [\"ethnicities\",\"occupation\",\"phenotype\"] and len(characteristics[key]) > 0:\n",
    "                person_record[key] = characteristics[key][0]\n",
    "                if len(characteristics[key]) > 1:\n",
    "                    for char in range(1,len(characteristics[key])):\n",
    "                        person_record[key] += ';' + characteristics[key][char]\n",
    "            elif (characteristics[key] is not None) and (characteristics[key] != []):\n",
    "                person_record[key] = characteristics[key]\n",
    "            else:\n",
    "                person_record[key] = None\n",
    "        \n",
    "        people.append(person_record)\n",
    "    \n",
    "    return people, categorized_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def id_unique_individuals(entry_text, entities, volume_metadata):\n",
    "    '''\n",
    "    identifies all unique individuals that appear in an entry (i.e. removing all multiple mentions of the same person)\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "        entities: entities of all kinds extracted from that entry by an NER model        \n",
    "        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata\n",
    "        \n",
    "        returns: a list of the unique individuals who appear in an entry AND (temporary?) unique IDs for each individual\n",
    "    '''\n",
    "    event_id = volume_metadata[\"id\"] + '-' + entities.iloc[0]['entry_no']\n",
    "    \n",
    "    people_df = entities.loc[entities['pred_label'] == 'PER']\n",
    "    people_df.reset_index(inplace=True)\n",
    "    people_df = people_df.drop('index',axis=1)\n",
    "    \n",
    "    unique_individuals = people_df['pred_entity'].unique()\n",
    "    unique_individuals = np.vstack([unique_individuals, [None] * len(unique_individuals)])    \n",
    "    \n",
    "    for i in range(len(unique_individuals[0])):        \n",
    "        unique_individuals[1][i] = event_id + '-P' + str(i + 1)        \n",
    "    \n",
    "    return unique_individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the first row of unique_individuals to contain the \"best\" (i.e. most complete/most accurate) name for each disambiguated individual. Once we have the ability to drop non-identical string references, we'll need to add a third row to unique_individuals in which each element is a list/array containing any/all disambiguated name strings since we'll need these to correctly attach characteristic/relationship references. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find_sus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def find_sus(entry_text, entities, sus_df, index):\n",
    "    '''\n",
    "    identifies corner cases: all entries where there are multiple entities that 1) have the same first name appearing \n",
    "        multiple times, 2) have compound names and then a segment of that name appearing, and 3) have a full name with \n",
    "        the first name by itself appearing\n",
    "    Note that this should not be used in tandem with id_unique_individuals, as that function just drops the duplicate names\n",
    "    \n",
    "    params:\n",
    "        entry_text: actual text for comparison\n",
    "        entities: df of entities identified\n",
    "        sus_df: either the empty df body or the df from previously loop iterations\n",
    "        i: current row that the loop is on in DEMO_DF\n",
    "    \n",
    "    returns: df of all the entries that may be corner cases, in the same form demo_df, but with two added id columns\n",
    "    '''\n",
    "    #Set up\n",
    "    people_df = entities.loc[entities['pred_label'] == 'PER']\n",
    "    people_df.reset_index(inplace=True)\n",
    "    people_df = people_df.drop('index',axis=1)\n",
    "    \n",
    "    my_rows = len(people_df.index)\n",
    "    hold = my_rows * [0]\n",
    "    people_df['name_status'] = hold\n",
    "    first_names = []\n",
    "    check_against = []\n",
    "    dups = 0\n",
    "    sus = 0\n",
    "    \n",
    "    #Get a list of all the first names that appear in the entities/people_df\n",
    "    #  This is definitely not the most computationally efficient way to do this\n",
    "    for i in range(my_rows):\n",
    "        #Separate people based on whether it is a first name or a full/compound name\n",
    "        if (\" \" in people_df.iloc[i,1]) or (\"-\" in people_df.iloc[i,1]):\n",
    "            check_against.append(people_df.iloc[i,1])\n",
    "        elif ~(\" \" in people_df.iloc[i,1]): #No spaces thus we are assuming it is a first name\n",
    "            first_names.append(people_df.iloc[i,1])\n",
    "    #Check to see whether they are subsets of full/compound names\n",
    "    if len(first_names)>0 and len(check_against)>0:\n",
    "        for j in range(len(first_names)):\n",
    "            for k in range(len(check_against)):\n",
    "                if first_names[j] in check_against[k]:\n",
    "                    #Mark this entire entry as sus\n",
    "                    sus = 1\n",
    "    #Generally check to see if there are any duplicate entities (same name) in the entry\n",
    "    if people_df['pred_entity'].duplicated().any():\n",
    "        dups = 1\n",
    "    #Set the status column\n",
    "    status = 0\n",
    "    if sus and dups:\n",
    "        status = 11 #ie both sus and dups are true\n",
    "    elif sus:\n",
    "        status = 10 #ie sus true, dups false\n",
    "    elif dups:\n",
    "        status = 0.01 #ie sus false, dups true\n",
    "\n",
    "    #ie if the entry is suspect or has duplicates, then add it to sus_df\n",
    "    if status > 0:\n",
    "        if len(sus_df.index)<1:\n",
    "            data = [{'vol_titl':demo_df.iloc[index,0], 'vol_id':demo_df.iloc[index,1], 'fol_id':demo_df.iloc[index,2],\n",
    "                    'text':demo_df.iloc[index,3],'entry_no':entry_no,'suspect':status}]\n",
    "            sus_df = pd.DataFrame(data)\n",
    "        else:\n",
    "            sus_df = sus_df.append({'vol_titl':demo_df.iloc[index,0], 'vol_id':demo_df.iloc[index,1], 'fol_id':demo_df.iloc[index,2],\n",
    "                    'text':demo_df.iloc[index,3],'entry_no':entry_no,'suspect':status},ignore_index=True)\n",
    "    return sus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split_name_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def split_name_col(people_df):\n",
    "    '''\n",
    "    from the fed in entities, strips DF to only include people, then separates based on if it is a first name or a full name\n",
    "    \n",
    "    \n",
    "    ### Functionality is not fully realized yet, could probably be generalized further, but this entire task may not be necessary\n",
    "    '''\n",
    "    #Set up\n",
    "    my_rows = len(people_df.index)\n",
    "    hold = my_rows * [0]\n",
    "    people_df['name_status'] = hold\n",
    "    \n",
    "    #Separate into two based on first/single and full name status\n",
    "    for i in range(my_rows):\n",
    "        if \"-\" in people_df.iloc[i,1]:\n",
    "            people_df.iloc[i,5] = 2 #2 therefore represents compound name\n",
    "        elif \" \" in people_df.iloc[i,1]:\n",
    "            people_df.iloc[i,5] = 1 #1 therefore represents a full name\n",
    "        else: #Must be a single name\n",
    "            #0 therefore represents a full name\n",
    "            pass\n",
    "    first_n = people_df[people_df.name_status == 0]\n",
    "    full_n = people_df[people_df.name_status == 1]\n",
    "    cmpd_n = people_df[people_df.name_status == 2]\n",
    "    \n",
    "    print(\"DF of first names\")\n",
    "    display(first_n.head())\n",
    "    print(\"DF of full names\")\n",
    "    display(full_n.head())\n",
    "    print(\"DF of compound names\")\n",
    "    display(cmpd_n.head())\n",
    "    print(\"---------------------\")\n",
    "    \n",
    "    return first_n, full_n, cmpd_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### disambiguate\n",
    "1. Doesn't do anything once entities are separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def disambiguate(entities):\n",
    "    '''\n",
    "    goes through the problem cases previously identified and then applies split_name_col to break the entities down into\n",
    "        the ones that may be \n",
    "    '''\n",
    "    people_df = entities.loc[entities['pred_label'] == 'PER']\n",
    "    people_df.reset_index(inplace=True)\n",
    "    people_df = people_df.drop('index',axis=1)\n",
    "    \n",
    "    first_n, full_n, cmpd_n = split_name_col(people_df)\n",
    "\n",
    "#DUNN NO CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def determine_principals(entry_text, entities, n_principals):\n",
    "    '''\n",
    "    determines the principal of a single-principal event\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "        entities: entities of all kinds extracted from that entry by an NER model\n",
    "        n_principals: expected number of principals\n",
    "        \n",
    "        returns: the principal(s) of the event in question, or None if no principal can be identified\n",
    "    '''\n",
    "    \n",
    "    entry_text = entry_text.lower()\n",
    "    principals = None\n",
    "    \n",
    "    if n_principals == 1:\n",
    "        \n",
    "        for index, entity in entities.iterrows():\n",
    "            if (entity['pred_label'] == 'PER') and (entity['pred_start'] <= 20):\n",
    "                principals = [entity['pred_entity']]\n",
    "                \n",
    "        if principals is None:\n",
    "            prox = entry_text.find('oleos')\n",
    "            if prox == -1:\n",
    "                prox = entry_text.find('óleos')\n",
    "            if prox != -1:\n",
    "                for index, entity in entities.iterrows():\n",
    "                    if (entity['pred_label'] == 'PER') and (abs(entity['pred_start'] - prox) <= 10):\n",
    "                        principals = [entity['pred_entity']]\n",
    "                        \n",
    "        if principals is None:\n",
    "            prox = entry_text.find('nombre')\n",
    "            if prox != -1:\n",
    "                for index, entity in entities.iterrows():\n",
    "                    if (entity['pred_label'] == 'PER') and (abs(entity['pred_start'] - prox) <= 10):\n",
    "                        principals = [entity['pred_entity']]                      \n",
    "        \n",
    "    elif n_principals == 2:\n",
    "        print(\"That number of principals is not supported yet.\")        \n",
    "        #process marriage principals\n",
    "    else:\n",
    "        print(\"Invalid number of principals.\")        \n",
    "    \n",
    "    return principals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def assign_relationships(entry_text, entities, unique_individuals):\n",
    "    '''\n",
    "    Relationship types:\n",
    "        parent/child --> P. and P.s are parents\n",
    "        godparents/godchildren --> P.P and p.s are godparents\n",
    "        slaveholders/enslaved\n",
    "        spouses\n",
    "        grandparents    \n",
    "    Note that this function also calls the determine_principals function to help with rel assignment\n",
    "    Returns: the relationship in forms of triples (subject, REL, relation) i.e. (me, godmother, my_godmother)\n",
    "    '''    \n",
    "    rel_df = entities.loc[entities['pred_label'] == 'REL']\n",
    "    rel_df.reset_index(inplace=True)\n",
    "    rel_df = rel_df.drop('index',axis=1)\n",
    "    #display(rel_df.head()) #Comment this out in final function, this is just for quick verification\n",
    "    \n",
    "    principal = determine_principals(entry_text, unique_individuals, 1)[0]\n",
    "    principal_ID = unique_individuals.loc[unique_individuals.pred_entity==principal,\"unique_id\"].item()\n",
    "    status = retrieve_controlled_vocabularies(None,[\"status\"])\n",
    "\n",
    "    rel = 0 #Variable telling us later whether or not this entry has any identified relationships\n",
    "    previous = 0 #Variable telling us whether or not the previous REL combined two entities \n",
    "    #(i.e. P. and P. into P.P. and thus can skip the second P. entity)\n",
    "    event_id = volume_metadata[\"id\"] + '-' + entities.iloc[0]['entry_no']\n",
    "    my_relations = []\n",
    "    m,n = entities.shape\n",
    "    for i in range(m):\n",
    "        if entities.iloc[i, 2]== 'REL':\n",
    "            rel = 1 #Relationship present\n",
    "            #We must check to make sure the first entity isn't a REL or it breaks the func due to positional index error\n",
    "            if i==0 or i==(m-1):\n",
    "                print(\"First/last entity is a REL, this functionality is not yet supported.\")\n",
    "            elif entities.iloc[i,1]=='P.P.':\n",
    "                try:\n",
    "                    #This gathers the first name, probably the padrino\n",
    "                    my_ID = unique_individuals.loc[unique_individuals.pred_entity==entities.iloc[i+1,1],\"unique_id\"].item()\n",
    "                    my_triple = (principal_ID,'Padrino',my_ID)\n",
    "                    my_relations.append(my_triple)\n",
    "                    #This should be the second name, probably the madrina\n",
    "                    my_ID = unique_individuals.loc[unique_individuals.pred_entity==entities.iloc[i+2,1],\"unique_id\"].item()\n",
    "                    my_triple = (principal_ID,'Madrina',my_ID)\n",
    "                    my_relations.append(my_triple)\n",
    "                except:\n",
    "                    print(\"Exception: had last entity in DF as a REL and thus out of bounds in current form of function\") \n",
    "            #Checking if we have back-to-back entities in the form of 'P.' followed by 'P.'\n",
    "            elif (entities.iloc[i + 1, 2] == 'REL') and ('P' in entities.iloc[i + 1, 1]) and (entities.iloc[i + 2, 2] == 'PER'):\n",
    "                previous = 1\n",
    "                my_ID = unique_individuals.loc[unique_individuals.pred_entity==entities.iloc[i+2,1],\"unique_id\"].item()\n",
    "                my_triple = (principal_ID,'Padrino',my_ID)\n",
    "                my_relations.append(my_triple)\n",
    "                my_ID = unique_individuals.loc[unique_individuals.pred_entity==entities.iloc[i+3,1],\"unique_id\"].item()\n",
    "                my_triple = (principal_ID,'Madrina',my_ID)\n",
    "                my_relations.append(my_triple)\n",
    "            #Skipping the second entity from the above case in the next iteration\n",
    "            elif previous:\n",
    "                previous = 0\n",
    "            elif entities.iloc[i + 1, 2]== 'PER':\n",
    "                try:\n",
    "                    my_ID = unique_individuals.loc[unique_individuals.pred_entity==entities.iloc[i+1,1],\"unique_id\"].item()\n",
    "                    my_triple = (principal_ID,(entities.iloc[i,1]),my_ID)\n",
    "                    my_relations.append(my_triple)\n",
    "                except:\n",
    "                    print(\"Exception: had last entity in DF as a REL and thus out of bounds in current form of function\")\n",
    "            elif ((entities.iloc[i,1].strip()=='P.') or (entities.iloc[i,1].strip()=='P')) and (entities.iloc[i+1,2]=='PER'): \n",
    "                try:\n",
    "                    my_ID = unique_individuals.loc[unique_individuals.pred_entity==entities.iloc[i+1,1],\"unique_id\"].item()\n",
    "                    my_triple = (principal_ID,\"Padre\",my_ID)\n",
    "                    my_relations.append(my_triple)\n",
    "                except:\n",
    "                    print(\"Exception: had last entity in DF as a REL and thus out of bounds in current form of function\") \n",
    "            else:\n",
    "                print(\"Relationship found, but not between adjacent people\")\n",
    "        elif (entities.iloc[i, 1] in status) and (entities.iloc[i + 1, 2] == 'PER'): #Identify the slave owner\n",
    "            my_ID = unique_individuals.loc[unique_individuals.pred_entity==entities.iloc[i+1,1],\"unique_id\"].item()\n",
    "            my_triple = (principal_ID,\"Esclavista\",my_ID)\n",
    "            my_relations.append(my_triple)\n",
    "    if rel:\n",
    "        print(entry_text) #Uncomment this for verification\n",
    "        print()\n",
    "        print(my_relations)\n",
    "    print(\"------------------------------------------\")\n",
    "    print()\n",
    "    return my_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def normalize_date(date):\n",
    "    '''\n",
    "    converts a string date to its numerical equivalent\n",
    "        date: string identified by model as a date\n",
    "        \n",
    "        returns: date in the format YYYY-MM-DD (ISO 8601)'''\n",
    "    \n",
    "    if date is None:\n",
    "        return \"????\", \"??\", \"??\"\n",
    "    \n",
    "    date = date.replace('.', '')\n",
    "    date = date.replace(',', '')\n",
    "    date = date.replace(';', '')\n",
    "    date = normalize_text(date.lower(), \"synonyms.json\", context = \"date\")\n",
    "    \n",
    "    date = date.replace(',', '')\n",
    "    day = None\n",
    "    month = None\n",
    "    month_pos = None\n",
    "    year = None\n",
    "    years = [\"un\", \"dos\", \"tres\", \"cuatro\", \"cinco\", \"seis\", \"siete\", \"ocho\", \"nueve\"]\n",
    "    decades = [\"diez\", \"veinte\", \"treinta\", \"cuarenta\", \"cincuenta\", \"sesenta\", \"setenta\", \"ochenta\", \"noventa\"]\n",
    "    month_days = [\"primero\", \"dos\", \"tres\", \"cuatro\", \"cinco\", \"seis\", \"siete\", \"ocho\", \"nueve\", \"diez\", \"once\", \"doce\", \"trece\", \"catorce\", \"quince\", \"diez y seis\", \"diez y siete\", \"diez y ocho\", \"diez y nueve\", \"veinte\", \"veinte y uno\", \"veinte y dos\", \"veinte y tres\", \"veinte y cuatro\", \"veinte y cinco\", \"veinte y seis\", \"veinte y siete\", \"veinte y ocho\", \"veinte y nueve\", \"treinta\", \"treinta y uno\"]\n",
    "    months = [\"enero\", \"febrero\", \"marzo\", \"abril\", \"mayo\", \"junio\", \"julio\", \"agosto\", \"septiembre\", \"octubre\", \"noviembre\", \"diciembre\"]\n",
    "    \n",
    "    for word in (date.split(' ')):\n",
    "        if word.lower() in months:\n",
    "            month = '0' * (2 - len(str(months.index(word.lower()) + 1))) + str(months.index(word.lower()) + 1)\n",
    "            month_pos = date.index(word)\n",
    "            \n",
    "    for n in month_days:\n",
    "        if month_pos is not None:\n",
    "            if (n in date) and (date.index(n) < month_pos):            \n",
    "                day = '0' * (2 - len(str(month_days.index(n) + 1))) + str(month_days.index(n) + 1)\n",
    "        else:\n",
    "            if (n in date) and (date.index(n) < 15):            \n",
    "                day = '0' * (2 - len(str(month_days.index(n) + 1))) + str(month_days.index(n) + 1)\n",
    "                \n",
    "    if day is None:\n",
    "        if month is not None:\n",
    "            month_ind = date.split(' ').index(months[int(month) - 1])\n",
    "            for i in range(0, month_ind):\n",
    "                if date.split(' ')[i].isdigit():\n",
    "                    day = date.split(' ')[i]\n",
    "        else:\n",
    "            for i in range(0, 3):\n",
    "                if len(date.split(' ')) <= i:\n",
    "                    break\n",
    "                if date.split(' ')[i].isdigit():\n",
    "                    day = date.split(' ')[i]\n",
    "                    \n",
    "    if (day is not None) and (len(day) < 2):\n",
    "        day = '0' + day                \n",
    "     \n",
    "    centuries = [\"seiscientos\", \"setecientos\", \"ochocientos\", \"novecientos\"]\n",
    "    century = None\n",
    "    cent_pos = None\n",
    "    for cent in centuries:\n",
    "        if date.find(cent) != -1:                \n",
    "            if cent_pos is None:\n",
    "                century = cent\n",
    "                cent_pos = date.find(century)\n",
    "            elif date.find(cent) < cent_pos:                    \n",
    "                century = cent\n",
    "                cent_pos = date.find(century)        \n",
    "    if century is not None:\n",
    "        year_str = None\n",
    "        dec_str = None\n",
    "        for yr in years:\n",
    "            if date.find(yr, cent_pos + 1) != -1:                    \n",
    "                if year_str is None:\n",
    "                    year_str = yr\n",
    "                else:\n",
    "                    if date.find(yr, cent_pos) < date.find(year_str, cent_pos):\n",
    "                        year_str = yr\n",
    "        for decade in decades:                \n",
    "            if date.find(decade, cent_pos) != -1:                    \n",
    "                dec_str = decade\n",
    "                break\n",
    "        if (year_str is not None) and (dec_str is not None):\n",
    "            year = 1000 + 100 * (centuries.index(century) + 6) + 10 * (decades.index(dec_str) + 1) + years.index(year_str) + 1\n",
    "            year = str(year)\n",
    "        elif (year_str is None) and (dec_str is not None):\n",
    "            year = 1000 + 100 * (centuries.index(century) + 6) + 10 * (decades.index(dec_str) + 1)\n",
    "            year = str(year)\n",
    "        elif (year_str is not None) and (dec_str is None):\n",
    "            year = 1000 + 100 * (centuries.index(century) + 6) + years.index(year_str) + 1\n",
    "            year = str(year)\n",
    "        else:\n",
    "            year = str(1000 + 100 * (centuries.index(century) + 6))\n",
    "                \n",
    "    if year is None:\n",
    "        for token in date.split(' '):\n",
    "            if token.isdigit() and (int(token) > 1000) and (int(token) < 2000):\n",
    "                year = token    \n",
    "    \n",
    "    if year is None:\n",
    "        year = \"????\"\n",
    "    if month is None:\n",
    "        month = \"??\"\n",
    "    if day is None:\n",
    "        day = \"??\"\n",
    "    \n",
    "    return year, month, day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def determine_event_date(entry_text, entities, event_type, volume_metadata, event_ref_pos=None):\n",
    "    '''\n",
    "    determines the date of a specific event\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "        entities: entities of all kinds extracted from that entry by an NER model\n",
    "        event_type: this could be either a valid record_type OR a secondary event like a birth\n",
    "        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata\n",
    "        event_ref_pos: optional index for reference to secondary event (to determine most likely date by proximity)\n",
    "        \n",
    "        returns: the date of the event in question, or None if no date can be identified\n",
    "    '''\n",
    "    date = None\n",
    "    date_start = None\n",
    "    \n",
    "    if event_type != volume_metadata[\"type\"]:        \n",
    "        primary_event_date = determine_event_date(entry_text, entities, volume_metadata[\"type\"], volume_metadata)\n",
    "        for index, entity in entities.iterrows():\n",
    "            if (entity['pred_label'] == 'DATE') and (entity['pred_entity'] != primary_event_date) and (date is None):\n",
    "                date = entity['pred_entity']\n",
    "                date_start = entity['pred_start']\n",
    "            elif (entity['pred_label'] == 'DATE') and (entity['pred_entity'] != primary_event_date):\n",
    "                if event_ref_pos is None:\n",
    "                    date = entity['pred_entity']\n",
    "                else:\n",
    "                    if abs(event_ref_pos - entity['pred_start']) < abs(event_ref_pos - date_start):\n",
    "                        date = entity['pred_entity']\n",
    "                        date_start = entity['pred_start']\n",
    "    \n",
    "    elif volume_metadata[\"type\"] == \"baptism\":\n",
    "        entry_length = len(entry_text)\n",
    "        \n",
    "        for index, entity in entities.iterrows():\n",
    "            if (entity['pred_label'] == 'DATE') and (entity['pred_start'] <= (entry_length / 3)):\n",
    "                date = entity['pred_entity']        \n",
    "                \n",
    "    else:\n",
    "        print(\"That event type is not supported yet.\")\n",
    "        \n",
    "    year, month, day = normalize_date(date)    \n",
    "    \n",
    "    return year + '-' + month + '-' + day\n",
    "\n",
    "#DUNN NO CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def determine_event_location(entry_text, entities, event_type, volume_metadata, event_ref_pos=None):\n",
    "    '''\n",
    "    determines the location of a specific event\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "        entities: entities of all kinds extracted from that entry by an NER model\n",
    "        event_type: this could be either a valid record_type OR a secondary event like a birth\n",
    "        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata\n",
    "        event_ref_pos: optional index for reference to secondary event (to determine most likely date by proximity)\n",
    "        \n",
    "        returns: the location of the event in question, or None if no date can be identified\n",
    "    '''\n",
    "    location = None\n",
    "    \n",
    "    if event_type == volume_metadata[\"type\"]:\n",
    "        location = volume_metadata[\"institution\"]    \n",
    "    \n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def identify_cleric(entry_text, entities):\n",
    "    '''\n",
    "    identifies the cleric(s) associated with a sacramental entry\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "        entities: entities of all kinds extracted from that entry by an NER model        \n",
    "        \n",
    "        returns: the associated cleric(s), or None if no date can be identified\n",
    "    '''\n",
    "    clerics = None\n",
    "    \n",
    "    for index, entity in entities.iterrows():\n",
    "            if (entity['pred_label'] == 'PER') and ((len(entry_text) - entity['pred_end']) <= 10) and (len(entry_text) > 100):\n",
    "                clerics = entity['pred_entity']\n",
    "            #going to keep this condition for now, but it can create false positives when long, incorrect entities are extracted\n",
    "            #from short and/or garbled entries\n",
    "            elif (entity['pred_entity'] is not None) and (len(entry_text) - entity['pred_end'] <= 2) and (entity['pred_label'] == 'PER'):\n",
    "                clerics = entity['pred_entity']                                                 \n",
    "                \n",
    "    if clerics is None:\n",
    "        pvs_label = None\n",
    "        pvs_end = None\n",
    "        for index, entity in entities.iterrows():\n",
    "            if entity['pred_label'] == 'PER' and pvs_label == 'DATE' and (entity['pred_start'] - pvs_end) <= 15:\n",
    "                clerics = entity['pred_entity']                \n",
    "            pvs_label = entity['pred_label']\n",
    "            pvs_end = entity['pred_end']\n",
    "    \n",
    "    if clerics is None:\n",
    "        entry_text = entry_text.lower()\n",
    "        for index, entity in entities.iterrows():\n",
    "            if entity['pred_label'] == 'PER' and entry_text.find(\"cura\", entity['pred_start'] + len(entity['pred_entity'])) != -1 and ((entry_text.find(\"cura\", entity['pred_start'] + len(entity['pred_entity']))) - entity['pred_end']) <= 15:\n",
    "                clerics = entity['pred_entity']                \n",
    "    \n",
    "    return clerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# refactored and debugged\n",
    "\n",
    "def build_event(entry_text, entities, event_type, principals, volume_metadata, n_event_within_entry, unique_individuals):\n",
    "    '''\n",
    "    builds out relationships related to a baptism or burial event\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "        entities: entities of all kinds extracted from that entry by an NER model\n",
    "        event_type: this could be either a valid record_type OR a secondary event like a birth\n",
    "        principals: the principal(s) of the event, as indicated by determine_principals\n",
    "        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata\n",
    "        unique_individuals: as determined by id_unique_individuals and/or meta-function of disambig pipeline\n",
    "        \n",
    "        n_event_within_entry: event number within entry\n",
    "        \n",
    "        returns: structured representation of these relationships, including (but not necessarily limited to)\n",
    "        the event's principal, the date of the event, the location of the event, and the associated cleric\n",
    "    '''   \n",
    "    event_id = volume_metadata[\"id\"] + '-' + entities.iloc[0]['entry_no'] + '-E' + str(n_event_within_entry)    \n",
    "    #it's possible that this function should also be returning an event iterator,\n",
    "    #but for now I'm planning to do that in build_relationships\n",
    "    if event_type == \"baptism\" or event_type == \"birth\":\n",
    "        date = determine_event_date(entry_text, entities, event_type, volume_metadata)\n",
    "        location = determine_event_location(entry_text, entities, event_type, volume_metadata)\n",
    "\n",
    "        # search for principal\n",
    "        principal = None\n",
    "        if principals is not None:\n",
    "            for index, entity in unique_individuals.iterrows():\n",
    "                if entity['pred_entity'] == principals[0]:\n",
    "                    principal = entity['unique_id']\n",
    "                    break\n",
    "\n",
    "        cleric = None\n",
    "        # if this is a baptism, search for cleric\n",
    "        if event_type == \"baptism\":\n",
    "            possibleCleric = identify_cleric(entry_text, entities)\n",
    "            for index, entity in unique_individuals.iterrows():\n",
    "                if entity['pred_entity'] == possibleCleric:\n",
    "                    cleric = entity['unique_id']\n",
    "    else:\n",
    "        print(\"That event type can't be built yet.\")\n",
    "        return\n",
    "    \n",
    "    event_relationships = {\"id\": event_id, \"type\": event_type, \"principal\": principal, \"date\": date, \"location\": location, \"cleric\": cleric}\n",
    "        \n",
    "    return event_relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def drop_obvious_duplicates(people, principals, cleric):\n",
    "    '''\n",
    "    first-pass disambiguation that drops multiple mentions of cleric and principal(s)\n",
    "        people: df containing all entities labeled as people in the entry\n",
    "        principals: as indicated by determine_principals\n",
    "        \n",
    "        returns: people df with obvious duplicates dropped\n",
    "    '''   \n",
    "    found_principal = False\n",
    "    found_cleric = False       \n",
    "    \n",
    "    if len(principals) == 1:\n",
    "        for index, person in people.iterrows():\n",
    "            if (person['pred_entity'] == principals[0]) and (found_principal == False):\n",
    "                found_principal = True\n",
    "            elif person['pred_entity'] == principals[0]:                \n",
    "                people.drop(index, inplace=True)\n",
    "                \n",
    "            if cleric is not None:\n",
    "                if (person['pred_entity'] == cleric) and (found_cleric == False):\n",
    "                    found_cleric = True\n",
    "                elif person['pred_entity'] == cleric:                \n",
    "                    people.drop(index, inplace=True)\n",
    "   \n",
    "    people.reset_index(inplace=True)\n",
    "    \n",
    "    return people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def id_obvious_duplicates(people, principals, cleric):\n",
    "    '''\n",
    "    first-pass disambiguation that identifies multiple mentions of cleric and principal(s)\n",
    "        people: df containing all entities labeled as people in the entry with unique ids\n",
    "        principals: as indicated by determine_principals\n",
    "        cleric: as identified by identify_cleric\n",
    "        \n",
    "        returns: dictionary with two keys, each containing list of ids corresponding to each mention of individual in question \n",
    "    '''   \n",
    "   \n",
    "    obv_dups = {\"principal\":[], \"cleric\":[]}\n",
    "    \n",
    "    #clumsy fix for corner case where principal and cleric share first name    \n",
    "    if (len(principals) > 0) and (cleric is not None):\n",
    "        for principal in principals:        \n",
    "            for p in principal.split(' '):\n",
    "                if p in cleric:\n",
    "                    return obv_dups\n",
    "    \n",
    "    if principals is not None and len(principals) == 1:\n",
    "        for index, person in people.iterrows():\n",
    "            if (person['pred_entity'] == principals[0]) or ((len(person[\"pred_entity\"].split(' ')) == 1) and (person[\"pred_entity\"] in principals[0])):\n",
    "                obv_dups[\"principal\"].append(person[\"unique_id\"])           \n",
    "            \n",
    "            if person['pred_entity'] == cleric:\n",
    "                    obv_dups[\"cleric\"].append(person[\"unique_id\"])\n",
    "    \n",
    "    return obv_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def assign_unique_ids(people, volume_metadata, entry_number=None):\n",
    "    '''\n",
    "    assigns unique ids to each person in an entry\n",
    "        people: df containing all entities labeled as people in the entry that has received first-pass disambiguation\n",
    "        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata\n",
    "        entry_number: compound id containing folio and folio-specific entry ids\n",
    "        \n",
    "        returns: people df with column containing unique ids appended, next available id\n",
    "    '''\n",
    "    size = len(people.index)    \n",
    "    \n",
    "    if size == 0:\n",
    "        return people, volume_metadata[\"id\"] + '-' + entry_number + \"-P1\"\n",
    "    \n",
    "    unique_ids = []\n",
    "    entry_id = volume_metadata[\"id\"] + '-' + people.iloc[0]['entry_no']\n",
    "    \n",
    "    for i in range(size):\n",
    "        unique_ids.append(entry_id + '-P' + str(i+1))\n",
    "        \n",
    "    people['unique_id'] = unique_ids\n",
    "    next_id = entry_id + '-P' + str(i+2)\n",
    "    \n",
    "    return people, next_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# refactored and debugged\n",
    "\n",
    "def merge_records(records_to_merge):\n",
    "    '''\n",
    "    merge two or more dictionaries with some (but possibly not all) shared keys\n",
    "        records_to_merge: list containing two or more dictionaries to merge\n",
    "        \n",
    "        returns: single, merged dictionary\n",
    "    '''\n",
    "    # if one record or less, return and don't try to merge\n",
    "    if len(records_to_merge) < 1:\n",
    "        return records_to_merge\n",
    "\n",
    "    #otherwise, take first record\n",
    "    merged_record = records_to_merge[0]\n",
    "\n",
    "    #iterating over the rest of the records,\n",
    "    #skip over 'None', fill in missing keys, and merge when possible\n",
    "    for i in range(1, len(records_to_merge)):\n",
    "        record = records_to_merge[i]\n",
    "        for key in record:\n",
    "            if record[key] is None:\n",
    "                continue\n",
    "            if merged_record[key] is None:\n",
    "                merged_record[key] = record[key]\n",
    "            else:\n",
    "                if key == 'relationships':\n",
    "                    for rel in record[key]:\n",
    "                        if rel not in merged_record[key]:\n",
    "                            merged_record[key].append(rel)\n",
    "                else:\n",
    "                    values = record[key].split(';')\n",
    "                    for value in values:\n",
    "                        if value.lower() not in merged_record[key].lower():\n",
    "                            merged_record[key] += ';' + value\n",
    "                \n",
    "    return merged_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# refactored and debugged\n",
    "\n",
    "def merge_duplicates(people, duplicates):\n",
    "    '''\n",
    "    merge two or more dictionaries with some (but possibly not all) shared keys\n",
    "        people: dataframe in which each row is a person\n",
    "        duplicates: dictionary containing keys \"principal\" and \"cleric\";\n",
    "        the value of each key is a list containing unique ids for each\n",
    "        mention of the appropriate individual\n",
    "        \n",
    "        returns: dataframe with duplicate mentions of each individual type merged\n",
    "    '''\n",
    "    types = [\"principal\", \"cleric\"] # list of person types\n",
    "\n",
    "    # put types with duplicates in a list\n",
    "    dupTypes = [] # list of person types with duplicates\n",
    "    for t in types:\n",
    "        if len(duplicates[t]) > 1:\n",
    "            dupTypes.append(t)\n",
    "\n",
    "    # for each type with a duplicate, merge\n",
    "    for d in dupTypes:\n",
    "        dups = []\n",
    "        del_indices = []\n",
    "        for person in people:\n",
    "            if person['id'] in duplicates[d]:\n",
    "                dups.append(person)\n",
    "                del_indices.append(people.index(person))\n",
    "        minus = 0\n",
    "        del_indices.sort()\n",
    "        for index in del_indices:\n",
    "            del people[index - minus]\n",
    "            minus += 1\n",
    "        people.append(merge_records(dups))\n",
    "    \n",
    "    return people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def build_new_person(people_df, next_id, person_type):\n",
    "    '''\n",
    "    appends a row representing a new person to an existing people df\n",
    "        people_df: df containing all entities labeled as people in the entry with unique ids\n",
    "        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata\n",
    "        person_type: type of person being added, e.g. \"principal\" or \"cleric\"\n",
    "        \n",
    "        returns: updated df with new person added and updated next available id\n",
    "    '''    \n",
    "    \n",
    "    curr_entry = next_id[next_id.find('-') + 1:next_id.find('P') - 1]\n",
    "    person_name = \"Unknown \" + person_type\n",
    "    person_number = next_id[next_id.find('P') + 1:]\n",
    "    person_number = int(person_number)\n",
    "    \n",
    "    people_df = people_df.append({\"entry_no\": curr_entry, \"pred_entity\": person_name, \"pred_label\": \"PER\", \"unique_id\": next_id}, ignore_index=True)\n",
    "    \n",
    "    next_id = next_id[:next_id.find('P') + 1] + str(person_number + 1)\n",
    "    \n",
    "    return people_df, next_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def build_entry_metadata(entry_text, entities, path_to_volume_xml, entry_number=None):\n",
    "    '''\n",
    "    applies rules-based engine for relationship linking to the transcription of a single entry\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity        \n",
    "        entities: entities of all kinds extracted from that entry by an NER model\n",
    "        path_to_volume_xml: path to xml file containing full volume transcription and volume-level metadata\n",
    "        entry_number: entry number, also from spaCy\n",
    "            \n",
    "        returns: three lists containing structured data about the people, places, and events that appear in the entry\n",
    "    '''\n",
    "        \n",
    "    people = []\n",
    "    places = []\n",
    "    events = []\n",
    "    \n",
    "    volume_metadata = retrieve_volume_metadata(path_to_volume_xml)\n",
    "    people_df = copy.deepcopy(entities.loc[entities['pred_label'] == 'PER'])\n",
    "    people_df.reset_index(inplace=True)\n",
    "    people_df, next_id = assign_unique_ids(people_df, volume_metadata, entry_number)\n",
    "    characteristics_df = copy.deepcopy(entities.loc[entities['pred_label'] == 'CHAR'])\n",
    "    characteristics_df.reset_index(inplace=True)\n",
    "    dates_df = copy.deepcopy(entities.loc[entities['pred_label'] == 'DATE'])\n",
    "    dates_df.reset_index(inplace=True)\n",
    "    \n",
    "    if volume_metadata[\"type\"] == \"baptism\":\n",
    "        principal = determine_principals(entry_text, entities, 1)\n",
    "        \n",
    "        if principal == None:            \n",
    "            people_df, next_id = build_new_person(people_df, next_id, \"principal\")\n",
    "            principal = [\"Unknown principal\"]            \n",
    "            \n",
    "        cleric = identify_cleric(entry_text, entities)                 \n",
    "        \n",
    "        events.append(build_event(entry_text, entities, \"baptism\", principal, volume_metadata, 1, people_df))\n",
    "        \n",
    "        if (len(dates_df.index) > 1):\n",
    "            events.append(build_event(entry_text, entities, \"birth\", principal, volume_metadata, 2, people_df))\n",
    "        \n",
    "        characteristics_df, uncategorized_characteristics = categorize_characteristics(entities, characteristics_df)\n",
    "        people, categorized_characteristics = assign_characteristics(entry_text, entities, characteristics_df, people_df, volume_metadata)       \n",
    "        #############################################################\n",
    "        ### KAI EDIT: added entities here as output ###\n",
    "        #############################################################\n",
    "        people, entities = alt_assign_relationships(entry_text, entities, people_df, people, volume_metadata)      \n",
    "        obvious_duplicates = id_obvious_duplicates(people_df, principal, cleric)       \n",
    "        people = merge_duplicates(people, obvious_duplicates)\n",
    "        \n",
    "        #perform more sophisticated disambiguation\n",
    "        \n",
    "        for event in events:\n",
    "            if (event[\"location\"] != None) and (not (event[\"location\"] in places)):\n",
    "                places.append(event[\"location\"])\n",
    "    \n",
    "    elif volume_metadata[\"type\"] == \"marriage\":        \n",
    "        #process marriage record\n",
    "        print(\"That record type is not supported yet.\")\n",
    "        return None\n",
    "    elif volume_metadata[\"type\"] == \"burial\":\n",
    "        #process burial record\n",
    "        print(\"That record type is not supported yet.\")\n",
    "        return None\n",
    "    else:\n",
    "        print(\"That record type is not supported yet.\")\n",
    "        return None    \n",
    "\n",
    "    return people, places, events, entities, characteristics_df, categorized_characteristics, uncategorized_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nbdev'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a3cf00c55010>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#no_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnbdev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotebook2script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnotebook2script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nbdev'"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
