{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#dependencies\n",
    "\n",
    "#nlp packages\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "#manipulation of tables/arrays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "#internal imports\n",
    "from ssda_nlp.collate import *\n",
    "from ssda_nlp.split_data import *\n",
    "from ssda_nlp.modeling import *\n",
    "from ssda_nlp.model_performance_utils import *\n",
    "from ssda_nlp.xml_parser import *\n",
    "from ssda_nlp.unstructured2markup import *\n",
    "from ssda_nlp.utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#ideally this function will eventually query the database live, but for now we'll use static snapshots\n",
    "#of the vocabularies as they currently exist\n",
    "#should also eventually be enhanced to return equivalents and a requested language\n",
    "\n",
    "def retrieve_controlled_vocabularies():\n",
    "    '''\n",
    "    returns a dictionary containing current version of controlled vocabularies for characteristics\n",
    "    '''\n",
    "    age = [\"parvola\", \"parvolo\", \"parvulo\", \"parvula\", \"parv\", \"adulto\", \"adulta\", \"adul\", \"niño\", \"niña\", \"nino\", \"nina\", \"dulto\", \"dulta\"]\n",
    "    occupation = [\"religioso\", \"ingen.o\", \"sacristan\", \"sac.\", \"sachristan\", \"cura\", \"vicario\", \"eclesiastico\", \"clerigo\", \"estudiante\"]\n",
    "    phenotype = [\"negro\", \"negra\", \"preto\", \"moreno\", \"morena\", \"indio\", \"india\", \"pardo\", \"parda\", \"mestizo\", \"mestiza\", \"mulato\", \"mulata\", \"blanco\", \"blanca\", \"criollo\", \"criolla\", \"branco\", \"branca\"]\n",
    "    titles = [\"frai\", \"sr\", \"sr.\", \"rexidor\", \"regr\", \"probin sial\", \"probinsial\", \"herm.o\", \"hermano\", \"dr.\", \"doc tor\", \"dna\", \"difin.dor\", \"difin.or\", \"difinidor\", \"d.a\", \"aiudante\", \"doctor\", \"d.r\", \"dor\", \"dr\", \"d.or\", \"b.er\", \"br\", \"ber\", \"don\", \"doña\", \"da\", \"padre\", \"pe\", \"predicador\", \"fray\", \"d.n\", \"d.\", \"d,n\", \"d;n\", \"p.e\", \"p\", \"dn\", \"fr.\", \"fr\", \"f\", \"regidor\", \"rex.or\", \"alg.l m.or\", \"ldo\", \"licenciado\", \"d\", \"alg.l\", \"alcalde\"]\n",
    "    ranks = [\"my.r\", \"comandan.te\", \"com.te\", \"capt.n\", \"capp[roto]\", \"cap[roto]\", \"capitn\", \"comend.te63\", \"comand.te\", \"alferes\", \"alfer.s mayor\", \"capitan\", \"capitam\", \"cap.n\", \"capit.n\", \"capn\", \"sarg.to may.r\", \"sarg.to\", \"sargento\", \"sarjento mayor\", \"sarjento\", \"sargto mayor\", \"theniente\", \"teniente\", \"thente\"]\n",
    "    ethnicities = [\"ganga\", \"español\", \"espanol\", \"caravali\", \"ingles\", \"yngles\", \"angola\", \"carabalí\", \"carabali\", \"carabaly\", \"congo\", \"conga\", \"mandinga\", \"mina\", \"temo\", \"malagas\", \"arara\", \"manga\"]\n",
    "    status = [\"propiedad\", \"escrava\", \"escravos\", \"esclabo\", \"esclaba\", \"escl.a\", \"escl.o\", \"clavo\", \"clava\", \"escl\", \"escl.\", \"escl.s\", \"clabo\", \"claba\", \"esc.va\", \"esc.ba\", \"esc.vo\", \"escvo\", \"esclavo\", \"esclava\", \"escva\", \"esc.bo\", \"esclabos\", \"esclavos\", \"esc.os\", \"esc.a\", \"esc.o\", \"libre\", \"esc.s\", \"esco\", \"esca\"]\n",
    "    legitimacy = [\"lexma\", \"lexmo\", \"legitima\", \"legitimo\", \"h l\", \"natural\", \"nral\", \"lexitima\", \"lexitimo\", \"nat.l\"]\n",
    "    relationships = [\"hermano\", \"hijo\", \"hija\", \"esposo\", \"esposa\", \"viudo\", \"viuda\", \"padrinos\", \"padrino\", \"padryno\", \"soltera\", \"soltero\", \"madrina\", \"padre\", \"p.p.\", \"p. \", \"p.\"]\n",
    "    origin = [\"naturales\"]    \n",
    "    \n",
    "    vocabs = {\"origin\": origin, \"legitimacy\": legitimacy, \"age\": age, \"occupation\": occupation, \"phenotype\": phenotype, \"titles\": titles, \"ranks\": ranks, \"ethnicities\": ethnicities, \"status\": status, \"relationships\": relationships}\n",
    "    \n",
    "    return vocabs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def build_reciprocal_relationship(people, from_person, to_person, relationship_type):\n",
    "    '''\n",
    "    helper function that adds a reciprocal relationship of a specified type to the records of two people        \n",
    "        people: list of dictionaries, each of which represents one mention of a person in the entry\n",
    "        from_person: unique_id of person that relationship \"comes from\" (i.e. the parent in a \"parent\"-type relationship)\n",
    "        to_person: unique_id of person that relationship \"goes to\" (i.e. the child in a \"parent\"-type relationship)\n",
    "        relationship_type: currently accepts \"parent\", \"godparent\", \"enslaver\", and \"spouse\"\n",
    "    \n",
    "        returns: updated version of people with interpersonal relationship added\n",
    "    '''\n",
    "    \n",
    "    empty_from = False\n",
    "    empty_to = False\n",
    "    no_to = False\n",
    "    no_from = False\n",
    "    \n",
    "    if from_person == None:\n",
    "        no_from = True\n",
    "    if to_person == None:\n",
    "        no_to = True\n",
    "    \n",
    "    for i in range(len(people)):\n",
    "        if people[i]['id'] == from_person:\n",
    "            from_loc = i\n",
    "            if people[i]['relationships'] == None:\n",
    "                empty_from = True\n",
    "        elif people[i]['id'] == to_person:\n",
    "            to_loc = i\n",
    "            if people[i]['relationships'] == None:\n",
    "                empty_to = True\n",
    "    \n",
    "    if relationship_type == \"godparent\":\n",
    "        if not no_from:\n",
    "            if empty_from:\n",
    "                people[from_loc]['relationships'] = [{\"related_person\": to_person, \"relationship_type\": \"godchild\"}]\n",
    "            else:\n",
    "                people[from_loc]['relationships'].append({\"related_person\": to_person, \"relationship_type\": \"godchild\"})\n",
    "            \n",
    "        if not no_to:\n",
    "            if empty_to:\n",
    "                people[to_loc]['relationships'] = [{\"related_person\": from_person, \"relationship_type\": \"godparent\"}]\n",
    "            else:\n",
    "                people[to_loc]['relationships'].append({\"related_person\": from_person, \"relationship_type\": \"godparent\"})\n",
    "    \n",
    "    elif relationship_type == \"parent\":\n",
    "        if not no_from:\n",
    "            if empty_from:\n",
    "                people[from_loc]['relationships'] = [{\"related_person\": to_person, \"relationship_type\": \"child\"}]\n",
    "            else:\n",
    "                people[from_loc]['relationships'].append({\"related_person\": to_person, \"relationship_type\": \"child\"})\n",
    "            \n",
    "        if not no_to:\n",
    "            if empty_to:\n",
    "                people[to_loc]['relationships'] = [{\"related_person\": from_person, \"relationship_type\": \"parent\"}]\n",
    "            else:\n",
    "                people[to_loc]['relationships'].append({\"related_person\": from_person, \"relationship_type\": \"parent\"})\n",
    "                \n",
    "    elif relationship_type == \"grandparent\":\n",
    "        if not no_from:\n",
    "            if empty_from:\n",
    "                people[from_loc]['relationships'] = [{\"related_person\": to_person, \"relationship_type\": \"grandchild\"}]\n",
    "            else:\n",
    "                people[from_loc]['relationships'].append({\"related_person\": to_person, \"relationship_type\": \"grandchild\"})\n",
    "            \n",
    "        if not no_to:\n",
    "            if empty_to:\n",
    "                people[to_loc]['relationships'] = [{\"related_person\": from_person, \"relationship_type\": \"grandparent\"}]\n",
    "            else:\n",
    "                people[to_loc]['relationships'].append({\"related_person\": from_person, \"relationship_type\": \"grandparent\"})\n",
    "    \n",
    "    elif relationship_type == \"enslaver\":\n",
    "        if not no_from:\n",
    "            if empty_from:\n",
    "                people[from_loc]['relationships'] = [{\"related_person\": to_person, \"relationship_type\": \"slave\"}]\n",
    "            else:\n",
    "                people[from_loc]['relationships'].append({\"related_person\": to_person, \"relationship_type\": \"slave\"})\n",
    "            \n",
    "        if not no_to:\n",
    "            if empty_to:\n",
    "                people[to_loc]['relationships'] = [{\"related_person\": from_person, \"relationship_type\": \"enslaver\"}]\n",
    "            else:\n",
    "                people[to_loc]['relationships'].append({\"related_person\": from_person, \"relationship_type\": \"enslaver\"})\n",
    "                \n",
    "    elif relationship_type == \"spouse\":\n",
    "        if not no_from:\n",
    "            if empty_from:\n",
    "                people[from_loc]['relationships'] = [{\"related_person\": to_person, \"relationship_type\": \"spouse\"}]\n",
    "            else:\n",
    "                people[from_loc]['relationships'].append({\"related_person\": to_person, \"relationship_type\": \"spouse\"})\n",
    "            \n",
    "        if not no_to:\n",
    "            if empty_to:\n",
    "                people[to_loc]['relationships'] = [{\"related_person\": from_person, \"relationship_type\": \"spouse\"}]\n",
    "            else:\n",
    "                people[to_loc]['relationships'].append({\"related_person\": from_person, \"relationship_type\": \"spouse\"})\n",
    "            \n",
    "    return people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "print([True]*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def alt_assign_relationships(entry_text, entities, people_df, people, volume_metadata):\n",
    "    '''\n",
    "    matches all labeled relationships to the correct individuals and builds triples\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "        entities: df containing all entities extracted from that entry by an NER model\n",
    "        people_df: entities given the label \"PER\" from a single entry by an NER model with unique ids\n",
    "        people: list of dictionaries, each of which represents one mention of a person in the entry\n",
    "        (as produced by assign_characteristics)\n",
    "        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata        \n",
    "    \n",
    "        returns: updated version of people with interpersonal relationships added\n",
    "    '''\n",
    "    \n",
    "    rel_types = retrieve_controlled_vocabularies()[\"relationships\"]\n",
    "    relationships = copy.deepcopy(entities.loc[entities['pred_label'] == 'REL'])\n",
    "    relationships.reset_index(inplace=True)\n",
    "    characteristics = copy.deepcopy(entities.loc[entities['pred_label'] == 'CHAR'])\n",
    "    characteristics.reset_index(inplace=True)    \n",
    "    cat_char = categorize_characteristics(entities, characteristics)    \n",
    "    entities.reset_index(inplace=True)  \n",
    "    \n",
    "    #############################################################\n",
    "    ### KAI EDIT: ###\n",
    "    #############################################################\n",
    "    #Get the size \n",
    "    entities_shape = entities.shape\n",
    "    #Now define a column vector that is the approriate size, True by default\n",
    "    truths_list = [True] * entities_shape[0] #[0] is the number of rows\n",
    "    #Now add that column to entities\n",
    "    entities['assgnmt_status'] = truths_list\n",
    "    \n",
    "    if determine_principals(entry_text, entities, 1) != None:\n",
    "        principal = determine_principals(entry_text, entities, 1)[0]\n",
    "        for i in range(len(people)):\n",
    "            if people[i][\"name\"] == principal:\n",
    "                principal_id = people[i]['id']                \n",
    "                break\n",
    "    else:\n",
    "        principal = \"Unknown principal\"\n",
    "        for i in range(len(people)):\n",
    "            if people[i][\"name\"] == principal:\n",
    "                principal_id = people[i]['id']                \n",
    "                break\n",
    "    \n",
    "    found_parents = False\n",
    "    found_godparents = False\n",
    "    godparents = []\n",
    "    found_paternal_grandparents = False\n",
    "    found_maternal_grandparents = False\n",
    "    found_enslaver = False\n",
    "    enslaver_id = None\n",
    "             \n",
    "    #build godparent/godchild relationships    \n",
    "    #future improvement: add logic to look for spousal relationship between godparents\n",
    "    if (len(entities) != 0) and (len(relationships) != 0):        \n",
    "        for index in range(len(entities)):\n",
    "            if entities['pred_label'][index] == \"REL\":                \n",
    "                if ((entities['pred_entity'][index].lower() == \"madrina\") or (entities['pred_entity'][index].lower() == \"padrino\") or (entities['pred_entity'][index].lower() == \"padryno\")) and (found_godparents == False):                    \n",
    "                    if (len(entities) > (index + 1)) and (entities['pred_label'][index + 1] == \"PER\"):\n",
    "                        for j in range(len(people_df)):\n",
    "                            if people_df['pred_start'][j] == entities['pred_start'][index + 1]:\n",
    "                                from_person = people_df['unique_id'][j]\n",
    "                        people = build_reciprocal_relationship(people, from_person, principal_id, \"godparent\")                        \n",
    "                        found_godparents = True\n",
    "                        godparent = {\"name\": people_df[\"pred_entity\"][j], \"id\": people_df[\"unique_id\"][j]}\n",
    "                        godparents.append(godparent)\n",
    "                elif ((entities['pred_entity'][index].lower() == \"padrinos\") or (entities['pred_entity'][index].lower() == \"p.p.\")) and (found_godparents == False):\n",
    "                    if (len(entities) > (index + 1)) and (entities['pred_label'][index + 1] == \"PER\"):\n",
    "                        for j in range(len(people_df)):\n",
    "                            if people_df['pred_start'][j] == entities['pred_start'][index + 1]:\n",
    "                                from_person = people_df['unique_id'][j]\n",
    "                        people = build_reciprocal_relationship(people, from_person, principal_id, \"godparent\")\n",
    "                        found_godparents = True\n",
    "                        godparent = {\"name\": people_df[\"pred_entity\"][j], \"id\": people_df[\"unique_id\"][j]}\n",
    "                        godparents.append(godparent)\n",
    "                    if (len(entities) > (index + 2)) and (entities['pred_label'][index + 2] == \"PER\"):\n",
    "                        for j in range(len(people_df)):\n",
    "                            if people_df['pred_start'][j] == entities['pred_start'][index + 2]:\n",
    "                                from_person = people_df['unique_id'][j]\n",
    "                        people = build_reciprocal_relationship(people, from_person, principal_id, \"godparent\")\n",
    "                        found_godparents = True\n",
    "                        godparent = {\"name\": people_df[\"pred_entity\"][j], \"id\": people_df[\"unique_id\"][j]}\n",
    "                        godparents.append(godparent)\n",
    "                elif (\"p.\" in entities['pred_entity'][index].lower()) and (found_godparents == False):\n",
    "                    if (len(entities) > (index + 1)) and not (\"p.\" in entities['pred_entity'][index + 1].lower()):                        \n",
    "                        if (len(entities) > (index + 1)) and (entities['pred_label'][index + 1] == \"PER\"):\n",
    "                            for j in range(len(people_df)):\n",
    "                                if people_df['pred_start'][j] == entities['pred_start'][index + 1]:\n",
    "                                    from_person = people_df['unique_id'][j]\n",
    "                            people = build_reciprocal_relationship(people, from_person, principal_id, \"godparent\")\n",
    "                            found_godparents = True\n",
    "                            godparent = {\"name\": people_df[\"pred_entity\"][j], \"id\": people_df[\"unique_id\"][j]}\n",
    "                            godparents.append(godparent)\n",
    "                    elif (len(entities) > (index + 1)):\n",
    "                        if (len(entities) > (index + 2)) and (entities['pred_label'][index + 2] == \"PER\"):\n",
    "                            for j in range(len(people_df)):\n",
    "                                if people_df['pred_start'][j] == entities['pred_start'][index + 2]:\n",
    "                                    from_person = people_df['unique_id'][j]\n",
    "                            people = build_reciprocal_relationship(people, from_person, principal_id, \"godparent\")\n",
    "                            found_godparents = True\n",
    "                            godparent = {\"name\": people_df[\"pred_entity\"][j], \"id\": people_df[\"unique_id\"][j]}\n",
    "                            godparents.append(godparent)\n",
    "                        if (len(entities) > (index + 3)) and (entities['pred_label'][index + 3] == \"PER\"):\n",
    "                            for j in range(len(people_df)):\n",
    "                                if people_df['pred_start'][j] == entities['pred_start'][index + 3]:\n",
    "                                    from_person = people_df['unique_id'][j]\n",
    "                            people = build_reciprocal_relationship(people, from_person, principal_id, \"godparent\")\n",
    "                            found_godparents = True\n",
    "                            godparent = {\"name\": people_df[\"pred_entity\"][j], \"id\": people_df[\"unique_id\"][j]}\n",
    "                            godparents.append(godparent)\n",
    "                #build grandparents\n",
    "                elif (\"abuelos\" in entities[\"pred_entity\"][index].lower()):                    \n",
    "                    if (\"paternos\" in entities[\"pred_entity\"][index].lower()) and (found_paternal_grandparents == False):                        \n",
    "                        paternal_grandfather = ''\n",
    "                        paternal_grandmother = ''\n",
    "                        if entities[\"pred_label\"][index + 1] == \"PER\":\n",
    "                            for j in range(len(people_df)):\n",
    "                                if people_df['pred_start'][j] == entities['pred_start'][index + 1]:\n",
    "                                    grandparent_id = people_df['unique_id'][j]\n",
    "                                    break\n",
    "                            if determine_sex(entities[\"pred_entity\"][index + 1].split(' ')[0], name_list=\"names.json\") == \"male\":\n",
    "                                paternal_grandfather = grandparent_id\n",
    "                                paternal_grandfather_index = j\n",
    "                            elif determine_sex(entities[\"pred_entity\"][index + 1].split(' ')[0], name_list=\"names.json\") == \"female\":\n",
    "                                paternal_grandmother = grandparent_id\n",
    "                                paternal_grandmother_index = j\n",
    "                            else:\n",
    "                                paternal_grandmother = grandparent_id\n",
    "                                paternal_grandmother_index = j\n",
    "                            if entities[\"pred_label\"][index + 2] == \"PER\":\n",
    "                                for j in range(len(people_df)):\n",
    "                                    if people_df['pred_start'][j] == entities['pred_start'][index + 2]:\n",
    "                                        grandparent_id = people_df['unique_id'][j]\n",
    "                                        break\n",
    "                                if (determine_sex(entities[\"pred_entity\"][index + 2].split(' ')[0], name_list=\"names.json\") == \"male\") and (paternal_grandfather == ''):\n",
    "                                    paternal_grandfather = grandparent_id\n",
    "                                    paternal_grandfather_index = j\n",
    "                                elif (determine_sex(entities[\"pred_entity\"][index + 2].split(' ')[0], name_list=\"names.json\") == \"female\") and (paternal_grandmother == ''):\n",
    "                                    paternal_grandmother = grandparent_id\n",
    "                                    paternal_grandmother_index = j\n",
    "                                elif paternal_grandmother == '':\n",
    "                                    paternal_grandmother = grandparent_id\n",
    "                                    paternal_grandmother_index = j\n",
    "                                else:\n",
    "                                    paternal_grandfather = grandparent_id\n",
    "                                    paternal_grandfather_index = j\n",
    "                        if paternal_grandfather != '':\n",
    "                            found_paternal_grandparents = True\n",
    "                            people = build_reciprocal_relationship(people, paternal_grandfather, principal_id, \"grandparent\")\n",
    "                        if paternal_grandmother != '':\n",
    "                            found_paternal_grandparents = True\n",
    "                            people = build_reciprocal_relationship(people, paternal_grandmother, principal_id, \"grandparent\")\n",
    "                        if (paternal_grandfather != '') and (paternal_grandmother != ''):\n",
    "                            people = build_reciprocal_relationship(people, paternal_grandfather, paternal_grandmother, \"spouse\")\n",
    "                    elif (\"matern\" in entities[\"pred_entity\"][index].lower()) and (found_maternal_grandparents == False):                        \n",
    "                        maternal_grandfather = ''\n",
    "                        maternal_grandmother = ''\n",
    "                        if entities[\"pred_label\"][index + 1] == \"PER\":\n",
    "                            for j in range(len(people_df)):\n",
    "                                if people_df['pred_start'][j] == entities['pred_start'][index + 1]:\n",
    "                                    grandparent_id = people_df['unique_id'][j]\n",
    "                                    break\n",
    "                            if determine_sex(entities[\"pred_entity\"][index + 1].split(' ')[0], name_list=\"names.json\") == \"male\":\n",
    "                                maternal_grandfather = grandparent_id\n",
    "                                maternal_grandfather_index = j\n",
    "                            elif determine_sex(entities[\"pred_entity\"][index + 1].split(' ')[0], name_list=\"names.json\") == \"female\":\n",
    "                                maternal_grandmother = grandparent_id\n",
    "                                maternal_grandmother_index = j\n",
    "                            else:\n",
    "                                maternal_grandmother = grandparent_id\n",
    "                                maternal_grandmother_index = j\n",
    "                            if entities[\"pred_label\"][index + 2] == \"PER\":\n",
    "                                for j in range(len(people_df)):\n",
    "                                    if people_df['pred_start'][j] == entities['pred_start'][index + 2]:\n",
    "                                        grandparent_id = people_df['unique_id'][j]\n",
    "                                        break\n",
    "                                if (determine_sex(entities[\"pred_entity\"][index + 2].split(' ')[0], name_list=\"names.json\") == \"male\") and (maternal_grandfather == ''):\n",
    "                                    maternal_grandfather = grandparent_id\n",
    "                                    maternal_grandfather_index = j\n",
    "                                elif (determine_sex(entities[\"pred_entity\"][index + 2].split(' ')[0], name_list=\"names.json\") == \"female\") and (maternal_grandmother == ''):\n",
    "                                    maternal_grandmother = grandparent_id\n",
    "                                    maternal_grandmother_index = j\n",
    "                                elif maternal_grandmother == '':\n",
    "                                    maternal_grandmother = grandparent_id\n",
    "                                    maternal_grandmother_index = j\n",
    "                                else:\n",
    "                                    maternal_grandfather = grandparent_id\n",
    "                                    maternal_grandfather_index = j\n",
    "                        if maternal_grandfather != '':\n",
    "                            found_maternal_grandparents = True\n",
    "                            people = build_reciprocal_relationship(people, maternal_grandfather, principal_id, \"grandparent\")\n",
    "                        if maternal_grandmother != '':\n",
    "                            found_maternal_grandparents = True\n",
    "                            people = build_reciprocal_relationship(people, maternal_grandmother, principal_id, \"grandparent\")\n",
    "                        if (maternal_grandfather != '') and (maternal_grandmother != ''):                            \n",
    "                            people = build_reciprocal_relationship(people, maternal_grandfather, maternal_grandmother, \"spouse\")\n",
    "                elif (\"matern\" in entities[\"pred_entity\"][index].lower()) and found_paternal_grandparents and (found_maternal_grandparents == False):\n",
    "                    maternal_grandfather = ''\n",
    "                    maternal_grandmother = ''\n",
    "                    if entities[\"pred_label\"][index + 1] == \"PER\":\n",
    "                        for j in range(len(people_df)):\n",
    "                            if people_df['pred_start'][j] == entities['pred_start'][index + 1]:\n",
    "                                grandparent_id = people_df['unique_id'][j]\n",
    "                                break\n",
    "                        if determine_sex(entities[\"pred_entity\"][index + 1].split(' ')[0], name_list=\"names.json\") == \"male\":\n",
    "                            maternal_grandfather = grandparent_id\n",
    "                            maternal_grandfather_index = j\n",
    "                        elif determine_sex(entities[\"pred_entity\"][index + 1].split(' ')[0], name_list=\"names.json\") == \"female\":\n",
    "                            maternal_grandmother = grandparent_id\n",
    "                            maternal_grandmother_index = j\n",
    "                        else:\n",
    "                            maternal_grandmother = grandparent_id\n",
    "                            maternal_grandmother_index = j\n",
    "                        if entities[\"pred_label\"][index + 2] == \"PER\":\n",
    "                            for j in range(len(people_df)):\n",
    "                                if people_df['pred_start'][j] == entities['pred_start'][index + 2]:\n",
    "                                    grandparent_id = people_df['unique_id'][j]\n",
    "                                    break\n",
    "                            if (determine_sex(entities[\"pred_entity\"][index + 2].split(' ')[0], name_list=\"names.json\") == \"male\") and (maternal_grandfather == ''):\n",
    "                                maternal_grandfather = grandparent_id\n",
    "                                maternal_grandfather_index = j\n",
    "                            elif (determine_sex(entities[\"pred_entity\"][index + 2].split(' ')[0], name_list=\"names.json\") == \"female\") and (maternal_grandmother == ''):\n",
    "                                maternal_grandmother = grandparent_id\n",
    "                                maternal_grandmother_index = j\n",
    "                            elif maternal_grandmother == '':\n",
    "                                maternal_grandmother = grandparent_id\n",
    "                                maternal_grandmother_index = j\n",
    "                            else:\n",
    "                                maternal_grandfather = grandparent_id\n",
    "                                maternal_grandfather_index = j\n",
    "                    if maternal_grandfather != '':\n",
    "                        found_maternal_grandparents = True\n",
    "                        people = build_reciprocal_relationship(people, maternal_grandfather, principal_id, \"grandparent\")\n",
    "                    if maternal_grandmother != '':\n",
    "                        found_maternal_grandparents = True\n",
    "                        people = build_reciprocal_relationship(people, maternal_grandmother, principal_id, \"grandparent\")\n",
    "                    if (maternal_grandfather != '') and (maternal_grandmother != ''):\n",
    "                            people = build_reciprocal_relationship(people, maternal_grandfather, maternal_grandmother, \"spouse\")\n",
    "                \n",
    "                #############################################################\n",
    "                ### KAI EDIT: ###\n",
    "                #############################################################\n",
    "                #Make sure that this is aligned correctly...\n",
    "                elif ((found_parents == False) and (found_godparents == False) and (found_paternal_grandparents == False) and (found_maternal_grandparents == False) and (found_enslaver == False)):\n",
    "                    #ie if after all these checks, there are still no relationships found, then we have a case where we have a relationship but no assignment\n",
    "                    #Initial attempt: add a column to entities, just binary whether \n",
    "                    #Note that this relies on setting ALL to FOUND by default, so I don't have to add to the code above each time\n",
    "                    #Thus, we only flip it in the case that no relationships are found\n",
    "                    entities['assgnmt_status'][index] == False\n",
    "                    \n",
    "                \n",
    "                \n",
    "    if len(godparents) == 2:\n",
    "        first_godparent_sex = determine_sex(godparents[0][\"name\"].split(' ')[0], name_list=\"names.json\")\n",
    "        second_godparent_sex = determine_sex(godparents[1][\"name\"].split(' ')[0], name_list=\"names.json\")\n",
    "        #if (first_godparent_sex != second_godparent_sex) or (first_godparent_sex == \"unknown\" and second_godparent_sex == \"unknown\"):\n",
    "            #print(\"found possible godparent couple: \")\n",
    "            #print(godparents[0][\"name\"])\n",
    "            #print(godparents[1][\"name\"])\n",
    "    \n",
    "    for i in range(len(cat_char)):\n",
    "        #build enslaver/enslaved person relationships\n",
    "        if cat_char[\"category\"][i] == \"status\":            \n",
    "            #skip if associated with first mention of principal\n",
    "            char_start = cat_char['pred_start'][i]\n",
    "            if char_start <= 25:\n",
    "                continue            \n",
    "            \n",
    "            #match enslaved couple to owner\n",
    "            if (cat_char[\"pred_entity\"][i].lower()[len(cat_char[\"pred_entity\"][i]) - 1] == 's'):\n",
    "                close_ep = -1\n",
    "                far_ep = -1\n",
    "                ens = -1\n",
    "                for j in range(len(people_df)):\n",
    "                    pers_start = people_df[\"pred_start\"][j]\n",
    "                    poss_diff = char_start - pers_start\n",
    "                    if (ens == -1) and (poss_diff < 0) and (abs(poss_diff) < 25):\n",
    "                        ens = j\n",
    "                    elif (ens != -1) and (poss_diff < 0) and (abs(poss_diff) < abs(char_start - people_df[\"pred_start\"][ens])):\n",
    "                        ens = j\n",
    "                    elif (close_ep == -1) and (poss_diff > 0) and (poss_diff < 50):\n",
    "                        close_ep = j\n",
    "                    elif (close_ep != -1) and (far_ep == -1) and (poss_diff > 0) and (poss_diff < char_start - people_df[\"pred_start\"][close_ep]):\n",
    "                        far_ep = close_ep\n",
    "                        close_ep = j\n",
    "                    elif (close_ep != -1) and (far_ep == -1) and (poss_diff > 0) and (poss_diff < 50):\n",
    "                        far_ep = j\n",
    "                    elif (close_ep != -1) and (far_ep != -1) and (poss_diff > 0) and (poss_diff < char_start - people_df[\"pred_start\"][close_ep]):\n",
    "                        far_ep = close_ep\n",
    "                        close_ep = j\n",
    "                    elif (close_ep != -1) and (far_ep != -1) and (poss_diff > 0) and (poss_diff < char_start - people_df[\"pred_start\"][far_ep]):\n",
    "                        far_ep = j\n",
    "                if (ens != -1) and (close_ep != -1) and (far_ep != -1):\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][ens], people_df[\"unique_id\"][close_ep], \"enslaver\")\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][ens], people_df[\"unique_id\"][far_ep], \"enslaver\")\n",
    "                elif (ens != -1) and (close_ep != -1):\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][ens], people_df[\"unique_id\"][close_ep], \"enslaver\")\n",
    "            #match enslaved person to owner        \n",
    "            elif \"propiedad\" in cat_char[\"pred_entity\"][i].lower():\n",
    "                for j in range(len(entities)):\n",
    "                    if entities[\"pred_start\"][j] == cat_char[\"pred_start\"][i]:\n",
    "                        signal_entity_index = j\n",
    "                        break                \n",
    "                if found_enslaver and (entry_text.rfind(\"misma\", cat_char[\"pred_start\"][i] - 25, cat_char[\"pred_start\"][i]) != -1):\n",
    "                    if (entities[\"pred_label\"][signal_entity_index - 1] == \"PER\") and (cat_char[\"pred_start\"][i] - entities[\"pred_end\"][signal_entity_index - 1] <= 20):\n",
    "                        people = build_reciprocal_relationship(people, enslaver_id, entities[\"unique_id\"][signal_entity_index - 1], \"enslaver\")\n",
    "                        if (entities[\"pred_label\"][signal_entity_index - 2] == \"PER\") and (entities[\"pred_end\"][signal_entity_index - 2] - entities[\"pred_start\"][signal_entity_index - 1] <= 5):\n",
    "                            people = build_reciprocal_relationship(people, enslaver_id, entities[\"unique_id\"][signal_entity_index - 2], \"enslaver\")\n",
    "                elif (entities[\"pred_label\"][signal_entity_index + 1] == \"PER\") and ((entities[\"pred_start\"][signal_entity_index + 1] - cat_char[\"pred_start\"][i]) <= 25):\n",
    "                    for j in range(len(people_df)):\n",
    "                        if people_df['pred_start'][j] == entities['pred_start'][signal_entity_index + 1]:\n",
    "                            found_enslaver = True\n",
    "                            enslaver_id = people_df[\"unique_id\"][j]\n",
    "                            people = build_reciprocal_relationship(people, enslaver_id, principal_id, \"enslaver\")\n",
    "                            break               \n",
    "            else:\n",
    "                ep = -1\n",
    "                ens = -1\n",
    "                for k in range(len(people_df)):\n",
    "                    pers_start = people_df[\"pred_start\"][k]\n",
    "                    poss_diff = char_start - pers_start\n",
    "                    if (ep == -1) and (poss_diff > 0) and (poss_diff < 50):\n",
    "                        ep = k                        \n",
    "                    elif (ens == -1) and (poss_diff < 0) and (abs(poss_diff) < 25):\n",
    "                        ens = k                        \n",
    "                    elif (ep != -1) and (poss_diff > 0) and (poss_diff < char_start - people_df[\"pred_start\"][ep]):\n",
    "                        ep = k\n",
    "                    elif (ens != -1) and (poss_diff < 0) and (abs(poss_diff) < abs(char_start - people_df[\"pred_start\"][ens])):\n",
    "                        ens = k\n",
    "                if (ep != -1) and (ens != -1):\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][ens], people_df[\"unique_id\"][ep], \"enslaver\")\n",
    "        #build parent/child relationships\n",
    "        elif (((cat_char[\"category\"][i] == \"relationships\") and ((cat_char[\"pred_entity\"][i] == \"hijo\") or (cat_char[\"pred_entity\"][i] == \"hija\") or (cat_char[\"pred_entity\"][i] == \"h\") or (cat_char[\"pred_entity\"][i] == \"h.\"))) or (cat_char[\"category\"][i] == \"legitimacy\")) and (found_parents == False):\n",
    "            rel_start = cat_char[\"pred_start\"][i]\n",
    "            close_parent = -1\n",
    "            far_parent = -1\n",
    "            for l in range(len(people_df)):\n",
    "                pers_start = people_df[\"pred_start\"][l]\n",
    "                poss_diff = rel_start - pers_start\n",
    "                if (close_parent == -1) and (poss_diff < 0) and (abs(poss_diff) < 25):\n",
    "                    close_parent = l\n",
    "                elif (close_parent != -1) and (far_parent == -1) and (poss_diff < 0) and (abs(poss_diff) < abs(rel_start - people_df[\"pred_start\"][close_parent])):\n",
    "                    far_parent = close_parent\n",
    "                    close_parent = l\n",
    "                elif (close_parent != -1) and (far_parent == -1) and (poss_diff < 0) and ((pers_start - people_df[\"pred_end\"][close_parent]) < 10):\n",
    "                    far_parent = l\n",
    "                elif (close_parent != -1) and (far_parent != -1) and (poss_diff > 0) and (abs(poss_diff) < abs(rel_start - people_df[\"pred_start\"][close_parent])):\n",
    "                    far_parent = close_parent\n",
    "                    close_parent = l\n",
    "                elif (close_parent != -1) and (far_parent != -1) and (poss_diff > 0) and (abs(poss_diff) < abs(rel_start - people_df[\"pred_start\"][far_parent])):\n",
    "                    far_parent = l\n",
    "            if (close_parent != -1) and (far_parent != -1):\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][close_parent], principal_id, \"parent\")\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][far_parent], principal_id, \"parent\")\n",
    "                if ((cat_char[\"category\"][i] == \"legitimacy\") and ('r' not in cat_char[\"pred_entity\"][i])) or ((cat_char[\"category\"][i] == \"relationships\") and ((cat_char['pred_entity'][i] == 'h') or (cat_char['pred_entity'][i] == 'h.')) and ((entry_text[cat_char[\"pred_end\"][i]] == 'l') or (entry_text[cat_char[\"pred_end\"][i] + 1] == 'l'))):\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][close_parent], people_df[\"unique_id\"][far_parent], \"spouse\")\n",
    "                #future improvement (after normalization) if both parents enslaved and child not free, make sure child's status is enslaved\n",
    "                #future improvement (after normalization) if child is enslaved, make sure reciprocal enslaver/enslaved person relationship exists with mother's enslaver                \n",
    "                found_parents = True\n",
    "            elif (close_parent != -1):\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][close_parent], principal_id, \"parent\")\n",
    "                #future improvement (after normalization) if single parent is mother and she is enslaved and child not free, make sure child's status is enslaved\n",
    "                #future improvement (after normalization) if child is enslaved, make sure reciprocal enslaver/enslaved person relationship exists with mother's enslaver\n",
    "                found_parents = True\n",
    "                \n",
    "    #build parent-child relationships between parents and grandparents\n",
    "    if found_parents and found_paternal_grandparents:        \n",
    "        if (far_parent != -1) and (determine_sex(people_df[\"pred_entity\"][far_parent].split(' ')[0]) == \"male\"):\n",
    "            if (paternal_grandmother != '') and (paternal_grandfather != ''):\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandmother_index], people_df[\"unique_id\"][far_parent], \"parent\")\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandfather_index], people_df[\"unique_id\"][far_parent], \"parent\")\n",
    "            elif paternal_grandmother != '':\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandmother_index], people_df[\"unique_id\"][far_parent], \"parent\")        \n",
    "            else:                \n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandfather_index], people_df[\"unique_id\"][far_parent], \"parent\")\n",
    "        elif (close_parent != -1) and (determine_sex(people_df[\"pred_entity\"][close_parent].split(' ')[0]) == \"male\"):\n",
    "            if (paternal_grandmother != '') and (paternal_grandfather != ''):\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandmother_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandfather_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "            elif paternal_grandmother != '':\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandmother_index], people_df[\"unique_id\"][close_parent], \"parent\")        \n",
    "            else:                \n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandfather_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "    if found_parents and found_maternal_grandparents:\n",
    "        if (close_parent != -1)  and (determine_sex(people_df[\"pred_entity\"][close_parent].split(' ')[0]) == \"female\"):\n",
    "            if (maternal_grandmother != '') and (maternal_grandfather != ''):\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandmother_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandfather_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "            elif maternal_grandmother != '':\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandmother_index], people_df[\"unique_id\"][close_parent], \"parent\")        \n",
    "            else:                \n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandfather_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "        elif (far_parent != -1) and (determine_sex(people_df[\"pred_entity\"][far_parent].split(' ')[0]) == \"female\"):\n",
    "            if (maternal_grandmother != '') and (maternal_grandfather != ''):\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandmother_index], people_df[\"unique_id\"][far_parent], \"parent\")\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandfather_index], people_df[\"unique_id\"][far_parent], \"parent\")\n",
    "            elif maternal_grandmother != '':\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandmother_index], people_df[\"unique_id\"][far_parent], \"parent\")        \n",
    "            else:                \n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandfather_index], people_df[\"unique_id\"][far_parent], \"parent\")\n",
    "        elif (close_parent != -1) and (far_parent == -1):\n",
    "            if (maternal_grandmother != '') and (maternal_grandfather != ''):\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandmother_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandfather_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "            elif maternal_grandmother != '':\n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandmother_index], people_df[\"unique_id\"][close_parent], \"parent\")        \n",
    "            else:                \n",
    "                people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandfather_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "        \n",
    "    return people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def categorize_characteristics(entities_df, characteristics_df):\n",
    "    '''\n",
    "    determines which category each labeled characteristic belongs to        \n",
    "        characteristics_df: entities given the label \"CHAR\" from a single entry by an NER model        \n",
    "    \n",
    "        returns: the same dataframe with an additional column containing a characteristic category\n",
    "    '''\n",
    "    \n",
    "    vocabs = retrieve_controlled_vocabularies()\n",
    "    categories = []\n",
    "    \n",
    "    for index, characteristic in entities_df.iterrows():\n",
    "        #development\n",
    "        #if characteristic[\"pred_entity\"] == \"libre\":\n",
    "            #print(characteristic[\"pred_label\"])\n",
    "        if characteristic[\"pred_label\"] != \"CHAR\":\n",
    "            continue\n",
    "        category = None       \n",
    "        if characteristic[\"pred_entity\"] in [\"Natural\", \"nral\", \"Nat.l\", \"N.l\", \"nat.l\", \"natural\"]:\n",
    "            loc_indices = []\n",
    "            for i, entity in entities_df.iterrows():\n",
    "                if entity[\"pred_label\"] == \"LOC\":\n",
    "                    loc_indices.append(i)\n",
    "            for loc_index in loc_indices:\n",
    "                if ((loc_index - index) == 1):\n",
    "                    category = \"origin\"\n",
    "            if category == None:\n",
    "                category = \"legitimacy\"        \n",
    "        for cat in vocabs:\n",
    "            if (characteristic['pred_entity'] == 'h') or (characteristic['pred_entity'] == \"h.\"):\n",
    "                category = \"relationships\"\n",
    "            elif characteristic[\"pred_entity\"] == \"propiedad\":\n",
    "                category = \"status\"\n",
    "            if category != None:\n",
    "                break\n",
    "            for term in vocabs[cat]:\n",
    "                if term in characteristic['pred_entity'].lower():\n",
    "                        category = cat\n",
    "                        break\n",
    "        #if category == None:\n",
    "            #print(\"Failed to find a category for \" + characteristic['pred_entity'])\n",
    "        categories.append(category)\n",
    "        \n",
    "    characteristics_df[\"category\"] = categories\n",
    "    \n",
    "    return characteristics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#this is currently configured specifically for baptisms/burials\n",
    "\n",
    "def assign_characteristics(entry_text, entities_df, characteristics_df, unique_individuals, volume_metadata):\n",
    "    '''\n",
    "    matches all labeled characteristics to the correct individual(s) and builds triples\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "        characteristics_df: entities given the label \"CHAR\" from a single entry by an NER model\n",
    "        unique_individuals: as determined by id_unique_individuals and/or meta-function of disambig pipeline\n",
    "        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata        \n",
    "    \n",
    "        returns: structured representation (a list of dictionaries)\n",
    "    '''\n",
    "    people = []\n",
    "    ethnicities = retrieve_controlled_vocabularies()[\"ethnicities\"]\n",
    "    categorized_characteristics = categorize_characteristics(entities_df, characteristics_df)\n",
    "    assignments = [None] * len(characteristics_df.index)    \n",
    "    categorized_characteristics.reset_index(inplace=True)\n",
    "    unique_individuals.reset_index(inplace=True)    \n",
    "    \n",
    "    for index in range(len(categorized_characteristics)):\n",
    "        #development\n",
    "        #if categorized_characteristics[\"pred_entity\"][index] == \"libre\":\n",
    "            #print(\"libre\")\n",
    "        if ((categorized_characteristics[\"category\"][index] == \"age\") or (categorized_characteristics[\"category\"][index] == \"legitimacy\")) and (volume_metadata[\"type\"] == \"baptism\"):\n",
    "            principal = determine_principals(entry_text, unique_individuals, 1)\n",
    "            if principal != None:\n",
    "                principal = determine_principals(entry_text, unique_individuals, 1)[0]\n",
    "            else:\n",
    "                principal = \"Unknown principal\"\n",
    "            princ_loc = unique_individuals.index[unique_individuals[\"pred_entity\"] == principal].tolist()\n",
    "            for loc in princ_loc:\n",
    "                if assignments[index] == None:\n",
    "                    assignments[index] = unique_individuals[\"unique_id\"][loc]\n",
    "                else:\n",
    "                    assignments[index] += ';' + unique_individuals[\"unique_id\"][loc]\n",
    "        elif (categorized_characteristics[\"category\"][index] == \"occupation\") or (categorized_characteristics[\"category\"][index] == \"phenotype\") or (categorized_characteristics[\"category\"][index] == \"ethnicities\") or ((categorized_characteristics[\"category\"][index] == \"status\") and (categorized_characteristics[\"pred_entity\"][index].lower()[-1] != 's')):\n",
    "            char_start = categorized_characteristics[\"pred_start\"][index]\n",
    "            lowest_diff = 50\n",
    "            assign = None\n",
    "            for i, person in unique_individuals.iterrows():\n",
    "                person_start = person[\"pred_start\"]\n",
    "                diff = char_start - person_start\n",
    "                if (diff > 0) and (diff < lowest_diff):\n",
    "                    lowest_diff = diff\n",
    "                    assign = i\n",
    "            if assign != None:                \n",
    "                assignments[index] = unique_individuals[\"unique_id\"][assign]\n",
    "        elif categorized_characteristics[\"category\"][index] == \"status\":\n",
    "            char_start = categorized_characteristics[\"pred_start\"][index]\n",
    "            lowest_diff = 30\n",
    "            second_lowest_diff = 50\n",
    "            assign = [None, None]\n",
    "            for i, person in unique_individuals.iterrows():\n",
    "                person_start = person[\"pred_start\"]\n",
    "                diff = char_start - person_start\n",
    "                if (diff > 0) and (diff < lowest_diff):\n",
    "                    lowest_diff = diff\n",
    "                    if assign[0] != None:\n",
    "                        assign[1] = assign[0]\n",
    "                        second_lowest_diff = lowest_diff\n",
    "                    assign[0] = i\n",
    "                elif (diff > 0) and (diff < second_lowest_diff) and (assign[0] != None):\n",
    "                    second_lowest_diff = diff\n",
    "                    assign[1] = i\n",
    "            ids = []\n",
    "            for a in assign:\n",
    "                if a != None:\n",
    "                    ids.append(unique_individuals[\"unique_id\"][a])\n",
    "            if len(ids) == 2:\n",
    "                assignments[index] = ids[0] + ';' + ids[1]\n",
    "            elif len(ids) == 1:\n",
    "                assignments[index] = ids[0]\n",
    "        elif categorized_characteristics[\"category\"][index] == \"origin\":            \n",
    "            for i, entity in entities_df.iterrows():                \n",
    "                if entity[\"pred_start\"] == categorized_characteristics[\"pred_start\"][index]:\n",
    "                    signal_entity_index = i\n",
    "                    break\n",
    "            if (entities_df[\"pred_label\"][signal_entity_index - 1] == \"PER\") and (entities_df[\"pred_label\"][signal_entity_index + 1] == \"LOC\") and (entities_df[\"pred_start\"][signal_entity_index + 1] - entities_df[\"pred_end\"][signal_entity_index - 1] <= 20):\n",
    "                place = entities_df[\"pred_entity\"][signal_entity_index + 1]\n",
    "                multiple = False\n",
    "                if categorized_characteristics[\"pred_entity\"][index] == \"naturales\":\n",
    "                    multiple = True\n",
    "                categorized_characteristics.at[index, \"pred_entity\"] = place\n",
    "                for i, person in unique_individuals.iterrows():\n",
    "                    if person[\"pred_start\"] == entities_df[\"pred_start\"][signal_entity_index - 1]:\n",
    "                        assignments[index] = person[\"unique_id\"]\n",
    "                        break\n",
    "                if multiple and (entities_df[\"pred_label\"][signal_entity_index - 2] == \"PER\") and (entities_df[\"pred_start\"][signal_entity_index - 1] - entities_df[\"pred_end\"][signal_entity_index - 2] <= 10):\n",
    "                    for i, person in unique_individuals.iterrows():\n",
    "                        if person[\"pred_start\"] == entities_df[\"pred_start\"][signal_entity_index - 2]:\n",
    "                            assignments[index] += ';' + person[\"unique_id\"]                            \n",
    "                            break\n",
    "            elif entities_df[\"pred_label\"][signal_entity_index + 1] == \"LOC\":\n",
    "                place = entities_df[\"pred_entity\"][signal_entity_index + 1]                \n",
    "                categorized_characteristics.at[index, \"pred_entity\"] = place\n",
    "                principal = determine_principals(entry_text, unique_individuals, 1)\n",
    "                if principal != None:\n",
    "                    principal = determine_principals(entry_text, unique_individuals, 1)[0]\n",
    "                else:\n",
    "                    principal = \"Unknown principal\"\n",
    "                princ_loc = unique_individuals.index[unique_individuals[\"pred_entity\"] == principal].tolist()\n",
    "                for loc in princ_loc:\n",
    "                    if assignments[index] == None:\n",
    "                        assignments[index] = unique_individuals[\"unique_id\"][loc]\n",
    "                    else:\n",
    "                        assignments[index] += ';' + unique_individuals[\"unique_id\"][loc]\n",
    "\n",
    "            \n",
    "    categorized_characteristics[\"assignment\"] = assignments    \n",
    "    \n",
    "    for i in range(len(unique_individuals.index)):        \n",
    "        \n",
    "        characteristics = {\"origin\": None, \"ethnicities\":[], \"age\":None, \"legitimacy\":None,\"occupation\":[], \"phenotype\":[], \"status\":None, \"titles\":None, \"ranks\":None, \"relationships\":None}\n",
    "        \n",
    "        for eth in ethnicities:\n",
    "            if eth in unique_individuals[\"pred_entity\"][i].lower():                \n",
    "                characteristics[\"ethnicities\"].append(eth[0].upper() + eth[1:])        \n",
    "        \n",
    "        for j in range(len(categorized_characteristics.index)):\n",
    "            if (categorized_characteristics[\"assignment\"][j] == None):\n",
    "                continue\n",
    "            if unique_individuals[\"unique_id\"][i] in categorized_characteristics[\"assignment\"][j]:\n",
    "                if (categorized_characteristics[\"category\"][j] == \"origin\") or (categorized_characteristics[\"category\"][j] == \"age\") or (categorized_characteristics[\"category\"][j] == \"legitimacy\") or (categorized_characteristics[\"category\"][j] == \"status\"):\n",
    "                    characteristics[categorized_characteristics[\"category\"][j]] = categorized_characteristics[\"pred_entity\"][j]\n",
    "                else:\n",
    "                    characteristics[categorized_characteristics[\"category\"][j]].append(categorized_characteristics[\"pred_entity\"][j])\n",
    "        \n",
    "        person_record = {\"id\": unique_individuals[\"unique_id\"][i], \"name\": unique_individuals[\"pred_entity\"][i]}\n",
    "        \n",
    "        for key in characteristics:\n",
    "            if ((key==\"ethnicities\") or (key == \"occupation\") or (key == \"phenotype\")) and (len(characteristics[key]) > 0):\n",
    "                person_record[key] = characteristics[key][0]\n",
    "                if (len(characteristics[key]) > 1):\n",
    "                    for char in range(1,len(characteristics[key])):\n",
    "                        person_record[key] += ';' + characteristics[key][char]\n",
    "            elif (characteristics[key] != None) and (characteristics[key] != []):\n",
    "                person_record[key] = characteristics[key]\n",
    "            else:\n",
    "                person_record[key] = None\n",
    "        \n",
    "        people.append(person_record)\n",
    "    \n",
    "    return people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def id_unique_individuals(entry_text, entities, volume_metadata):\n",
    "    '''\n",
    "    identifies all unique individuals that appear in an entry (i.e. removing all multiple mentions of the same person)\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "        entities: entities of all kinds extracted from that entry by an NER model        \n",
    "        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata\n",
    "        \n",
    "        returns: a list of the unique individuals who appear in an entry AND (temporary?) unique IDs for each individual\n",
    "    '''\n",
    "    event_id = volume_metadata[\"id\"] + '-' + entities.iloc[0]['entry_no']\n",
    "    \n",
    "    people_df = entities.loc[entities['pred_label'] == 'PER']\n",
    "    people_df.reset_index(inplace=True)\n",
    "    people_df = people_df.drop('index',axis=1)\n",
    "    \n",
    "    unique_individuals = people_df['pred_entity'].unique()\n",
    "    unique_individuals = np.vstack([unique_individuals, [None] * len(unique_individuals)])    \n",
    "    \n",
    "    for i in range(len(unique_individuals[0])):        \n",
    "        unique_individuals[1][i] = event_id + '-P' + str(i + 1)        \n",
    "    \n",
    "    return unique_individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the first row of unique_individuals to contain the \"best\" (i.e. most complete/most accurate) name for each disambiguated individual. Once we have the ability to drop non-identical string references, we'll need to add a third row to unique_individuals in which each element is a list/array containing any/all disambiguated name strings since we'll need these to correctly attach characteristic/relationship references. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find_sus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def find_sus(entry_text, entities, sus_df, index):\n",
    "    '''\n",
    "    identifies corner cases: all entries where there are multiple entities that 1) have the same first name appearing \n",
    "        multiple times, 2) have compound names and then a segment of that name appearing, and 3) have a full name with \n",
    "        the first name by itself appearing\n",
    "    Note that this should not be used in tandem with id_unique_individuals, as that function just drops the duplicate names\n",
    "    \n",
    "    params:\n",
    "        entry_text: actual text for comparison\n",
    "        entities: df of entities identified\n",
    "        sus_df: either the empty df body or the df from previously loop iterations\n",
    "        i: current row that the loop is on in DEMO_DF\n",
    "    \n",
    "    returns: df of all the entries that may be corner cases, in the same form demo_df, but with two added id columns\n",
    "    '''\n",
    "    #Set up\n",
    "    people_df = entities.loc[entities['pred_label'] == 'PER']\n",
    "    people_df.reset_index(inplace=True)\n",
    "    people_df = people_df.drop('index',axis=1)\n",
    "    \n",
    "    my_rows = len(people_df.index)\n",
    "    hold = my_rows * [0]\n",
    "    people_df['name_status'] = hold\n",
    "    first_names = []\n",
    "    check_against = []\n",
    "    dups = 0\n",
    "    sus = 0\n",
    "    \n",
    "    #Get a list of all the first names that appear in the entities/people_df\n",
    "    #  This is definitely not the most computationally efficient way to do this\n",
    "    for i in range(my_rows):\n",
    "        #Separate people based on whether it is a first name or a full/compound name\n",
    "        if (\" \" in people_df.iloc[i,1]) or (\"-\" in people_df.iloc[i,1]):\n",
    "            check_against.append(people_df.iloc[i,1])\n",
    "        elif ~(\" \" in people_df.iloc[i,1]): #No spaces thus we are assuming it is a first name\n",
    "            first_names.append(people_df.iloc[i,1])\n",
    "    #Check to see whether they are subsets of full/compound names\n",
    "    if len(first_names)>0 and len(check_against)>0:\n",
    "        for j in range(len(first_names)):\n",
    "            for k in range(len(check_against)):\n",
    "                if first_names[j] in check_against[k]:\n",
    "                    #Mark this entire entry as sus\n",
    "                    sus = 1\n",
    "    #Generally check to see if there are any duplicate entities (same name) in the entry\n",
    "    if people_df['pred_entity'].duplicated().any():\n",
    "        dups = 1;\n",
    "    #Set the status column\n",
    "    if sus and dups:\n",
    "        status = 11 #ie both sus and dups are true\n",
    "    elif sus:\n",
    "        status = 10 #ie sus true, dups false\n",
    "    elif dups:\n",
    "        status = 0.01 #ie sus false, dups true\n",
    "    else:\n",
    "        status = 0\n",
    "    #ie if the entry is suspect or has duplicates, then add it to sus_df\n",
    "    if status>0:\n",
    "        if len(sus_df.index)<1:\n",
    "            data = [{'vol_titl':demo_df.iloc[index,0], 'vol_id':demo_df.iloc[index,1], 'fol_id':demo_df.iloc[index,2],\n",
    "                    'text':demo_df.iloc[index,3],'entry_no':entry_no,'suspect':status}]\n",
    "            sus_df = pd.DataFrame(data)\n",
    "        else:\n",
    "            sus_df = sus_df.append({'vol_titl':demo_df.iloc[index,0], 'vol_id':demo_df.iloc[index,1], 'fol_id':demo_df.iloc[index,2],\n",
    "                    'text':demo_df.iloc[index,3],'entry_no':entry_no,'suspect':status},ignore_index=True)\n",
    "    return sus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split_name_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def split_name_col(people_df):\n",
    "    '''\n",
    "    from the fed in entities, strips DF to only include people, then separates based on if it is a first name or a full name\n",
    "    \n",
    "    \n",
    "    ### Functionality is not fully realized yet, could probably be generalized further, but this entire task may not be necessary\n",
    "    '''\n",
    "    #Set up\n",
    "    my_rows = len(people_df.index)\n",
    "    hold = my_rows * [0]\n",
    "    people_df['name_status'] = hold\n",
    "    \n",
    "    #Separate into two based on first/single and full name status\n",
    "    for i in range(my_rows):\n",
    "        if \"-\" in people_df.iloc[i,1]:\n",
    "            people_df.iloc[i,5] = 2 #2 therefore represents compound name\n",
    "        elif \" \" in people_df.iloc[i,1]:\n",
    "            people_df.iloc[i,5] = 1 #1 therefore represents a full name\n",
    "        else: #Must be a single name\n",
    "            #0 therefore represents a full name\n",
    "            pass\n",
    "    first_n = people_df[people_df.name_status == 0]\n",
    "    full_n = people_df[people_df.name_status == 1]\n",
    "    cmpd_n = people_df[people_df.name_status == 2]\n",
    "    \n",
    "    print(\"DF of first names\")\n",
    "    display(first_n.head())\n",
    "    print(\"DF of full names\")\n",
    "    display(full_n.head())\n",
    "    print(\"DF of compound names\")\n",
    "    display(cmpd_n.head())\n",
    "    print(\"---------------------\")\n",
    "    \n",
    "    return first_n, full_n, cmpd_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### disambiguate\n",
    "1. Doesn't do anything once entities are separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def disambiguate():\n",
    "    '''\n",
    "    goes through the problem cases previously identified and then applies split_name_col to break the entities down into\n",
    "        the ones that may be \n",
    "    '''\n",
    "    people_df = entities.loc[entities['pred_label'] == 'PER']\n",
    "    people_df.reset_index(inplace=True)\n",
    "    people_df = people_df.drop('index',axis=1)\n",
    "    \n",
    "    first_n, full_n, cmpd_n = split_name_col(people_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def determine_principals(entry_text, entities, n_principals):\n",
    "    '''\n",
    "    determines the principal of a single-principal event\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "        entities: entities of all kinds extracted from that entry by an NER model\n",
    "        n_principals: expected number of principals\n",
    "        \n",
    "        returns: the principal(s) of the event in question, or None if no principal can be identified\n",
    "    '''\n",
    "    \n",
    "    entry_text = entry_text.lower()\n",
    "    principals = None\n",
    "    \n",
    "    if n_principals == 1:       \n",
    "        \n",
    "        for index, entity in entities.iterrows():\n",
    "            if (entity['pred_label'] == 'PER') and (entity['pred_start'] <= 20):\n",
    "                principals = [entity['pred_entity']]\n",
    "                \n",
    "        if principals == None:            \n",
    "            prox = entry_text.find('oleos')\n",
    "            if prox == -1:\n",
    "                prox = entry_text.find('óleos')\n",
    "            if prox != -1:\n",
    "                for index, entity in entities.iterrows():\n",
    "                    if (entity['pred_label'] == 'PER') and (abs(entity['pred_start'] - prox) <= 10):\n",
    "                        principals = [entity['pred_entity']]\n",
    "                        \n",
    "        if principals == None:\n",
    "            prox = entry_text.find('nombre')\n",
    "            if prox != -1:\n",
    "                for index, entity in entities.iterrows():\n",
    "                    if (entity['pred_label'] == 'PER') and (abs(entity['pred_start'] - prox) <= 10):\n",
    "                        principals = [entity['pred_entity']]                      \n",
    "        \n",
    "    elif n_principals == 2:\n",
    "        print(\"That number of principals is not supported yet.\")        \n",
    "        #process marriage principals\n",
    "    else:\n",
    "        print(\"Invalid number of principals.\")        \n",
    "    \n",
    "    return principals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def assign_relationships(entry_text, entities, unique_individuals):\n",
    "    '''\n",
    "    Relationship types:\n",
    "        parent/child --> P. and P.s are parents\n",
    "        godparents/godchildren --> P.P and p.s are godparents\n",
    "        slaveholders/enslaved\n",
    "        spouses\n",
    "        grandparents    \n",
    "    Note that this function also calls the determine_principals function to help with rel assignment\n",
    "    Returns: the relationship in forms of triples (subject, REL, relation) i.e. (me, godmother, my_godmother)\n",
    "    '''    \n",
    "    rel_df = entities.loc[entities['pred_label'] == 'REL']\n",
    "    rel_df.reset_index(inplace=True)\n",
    "    rel_df = rel_df.drop('index',axis=1)\n",
    "    #display(rel_df.head()) #Comment this out in final function, this is just for quick verification\n",
    "    \n",
    "    principal = determine_principals(entry_text, unique_individuals, 1)[0]\n",
    "    principal_ID = unique_individuals.loc[unique_individuals.pred_entity==principal,\"unique_id\"].item()\n",
    "    status = retrieve_controlled_vocabularies()[\"status\"]\n",
    "\n",
    "    rel = 0 #Variable telling us later whether or not this entry has any identified relationships\n",
    "    previous = 0 #Variable telling us whether or not the previous REL combined two entities \n",
    "    #(i.e. P. and P. into P.P. and thus can skip the second P. entity)\n",
    "    event_id = volume_metadata[\"id\"] + '-' + entities.iloc[0]['entry_no']\n",
    "    my_relations = []\n",
    "    m,n = entities.shape\n",
    "    for i in range(m):\n",
    "        if (entities.iloc[i,2]=='REL'):\n",
    "            rel = 1 #Relationship present\n",
    "            #We must check to make sure the first entity isn't a REL or it breaks the func due to positional index error\n",
    "            if i==0 or i==(m-1):\n",
    "                print(\"First/last entity is a REL, this functionality is not yet supported.\")\n",
    "            elif entities.iloc[i,1]=='P.P.':\n",
    "                try:\n",
    "                    #This gathers the first name, probably the padrino\n",
    "                    my_ID = unique_individuals.loc[unique_individuals.pred_entity==entities.iloc[i+1,1],\"unique_id\"].item()\n",
    "                    my_triple = (principal_ID,'Padrino',my_ID)\n",
    "                    my_relations.append(my_triple)\n",
    "                    #This should be the second name, probably the madrina\n",
    "                    my_ID = unique_individuals.loc[unique_individuals.pred_entity==entities.iloc[i+2,1],\"unique_id\"].item()\n",
    "                    my_triple = (principal_ID,'Madrina',my_ID)\n",
    "                    my_relations.append(my_triple)\n",
    "                except:\n",
    "                    print(\"Exception: had last entity in DF as a REL and thus out of bounds in current form of function\") \n",
    "            #Checking if we have back-to-back entities in the form of 'P.' followed by 'P.'\n",
    "            elif ( (entities.iloc[i+1,2]=='REL') and ('P' in entities.iloc[i+1,1]) and (entities.iloc[i+2,2]=='PER') ):\n",
    "                previous = 1\n",
    "                my_ID = unique_individuals.loc[unique_individuals.pred_entity==entities.iloc[i+2,1],\"unique_id\"].item()\n",
    "                my_triple = (principal_ID,'Padrino',my_ID)\n",
    "                my_relations.append(my_triple)\n",
    "                my_ID = unique_individuals.loc[unique_individuals.pred_entity==entities.iloc[i+3,1],\"unique_id\"].item()\n",
    "                my_triple = (principal_ID,'Madrina',my_ID)\n",
    "                my_relations.append(my_triple)\n",
    "            #Skipping the second entity from the above case in the next iteration\n",
    "            elif previous:\n",
    "                previous = 0\n",
    "            elif (entities.iloc[i+1,2]=='PER'):\n",
    "                try:\n",
    "                    my_ID = unique_individuals.loc[unique_individuals.pred_entity==entities.iloc[i+1,1],\"unique_id\"].item()\n",
    "                    my_triple = (principal_ID,(entities.iloc[i,1]),my_ID)\n",
    "                    my_relations.append(my_triple)\n",
    "                except:\n",
    "                    print(\"Exception: had last entity in DF as a REL and thus out of bounds in current form of function\")\n",
    "            elif ((entities.iloc[i,1].strip()=='P.') or (entities.iloc[i,1].strip()=='P')) and (entities.iloc[i+1,2]=='PER'): \n",
    "                try:\n",
    "                    my_ID = unique_individuals.loc[unique_individuals.pred_entity==entities.iloc[i+1,1],\"unique_id\"].item()\n",
    "                    my_triple = (principal_ID,\"Padre\",my_ID)\n",
    "                    my_relations.append(my_triple)\n",
    "                except:\n",
    "                    print(\"Exception: had last entity in DF as a REL and thus out of bounds in current form of function\") \n",
    "            else:\n",
    "                print(\"Relationship found, but not between adjacent people\")\n",
    "        elif ((entities.iloc[i,1] in status) and (entities.iloc[i+1,2]=='PER')): #Identify the slave owner\n",
    "            my_ID = unique_individuals.loc[unique_individuals.pred_entity==entities.iloc[i+1,1],\"unique_id\"].item()\n",
    "            my_triple = (principal_ID,\"Esclavista\",my_ID)\n",
    "            my_relations.append(my_triple)\n",
    "    if rel:\n",
    "        print(entry_text) #Uncomment this for verification\n",
    "        print()\n",
    "        print(my_relations)\n",
    "    print(\"------------------------------------------\")\n",
    "    print()\n",
    "    return my_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def normalize_date(date):\n",
    "    '''\n",
    "    converts a string date to its numerical equivalent\n",
    "        date: string identified by model as a date\n",
    "        \n",
    "        returns: date in the format YYYY-MM-DD (ISO 8601)'''\n",
    "    \n",
    "    if date == None:\n",
    "        return \"????\", \"??\", \"??\"\n",
    "    \n",
    "    date = date.replace('.', '')\n",
    "    date = date.replace(',', '')\n",
    "    date = date.replace(';', '')\n",
    "    date = normalize_text(date.lower(), \"synonyms.json\", context = \"date\")\n",
    "    \n",
    "    date = date.replace(',', '')\n",
    "    day = None\n",
    "    month = None\n",
    "    month_pos = None\n",
    "    year = None\n",
    "    years = [\"un\", \"dos\", \"tres\", \"cuatro\", \"cinco\", \"seis\", \"siete\", \"ocho\", \"nueve\"]\n",
    "    decades = [\"diez\", \"veinte\", \"treinta\", \"cuarenta\", \"cincuenta\", \"sesenta\", \"setenta\", \"ochenta\", \"noventa\"]\n",
    "    month_days = [\"primero\", \"dos\", \"tres\", \"cuatro\", \"cinco\", \"seis\", \"siete\", \"ocho\", \"nueve\", \"diez\", \"once\", \"doce\", \"trece\", \"catorce\", \"quince\", \"diez y seis\", \"diez y siete\", \"diez y ocho\", \"diez y nueve\", \"veinte\", \"veinte y uno\", \"veinte y dos\", \"veinte y tres\", \"veinte y cuatro\", \"veinte y cinco\", \"veinte y seis\", \"veinte y siete\", \"veinte y ocho\", \"veinte y nueve\", \"treinta\", \"treinta y uno\"]\n",
    "    months = [\"enero\", \"febrero\", \"marzo\", \"abril\", \"mayo\", \"junio\", \"julio\", \"agosto\", \"septiembre\", \"octubre\", \"noviembre\", \"diciembre\"]\n",
    "    \n",
    "    for word in (date.split(' ')):\n",
    "        if word.lower() in months:\n",
    "            month = '0' * (2 - len(str(months.index(word.lower()) + 1))) + str(months.index(word.lower()) + 1)\n",
    "            month_pos = date.index(word)\n",
    "            \n",
    "    for n in month_days:\n",
    "        if (month_pos != None):\n",
    "            if (n in date) and (date.index(n) < month_pos):            \n",
    "                day = '0' * (2 - len(str(month_days.index(n) + 1))) + str(month_days.index(n) + 1)\n",
    "        else:\n",
    "            if (n in date) and (date.index(n) < 15):            \n",
    "                day = '0' * (2 - len(str(month_days.index(n) + 1))) + str(month_days.index(n) + 1)\n",
    "                \n",
    "    if day == None:\n",
    "        if (month != None):\n",
    "            month_ind = date.split(' ').index(months[int(month) - 1])\n",
    "            for i in range(0, month_ind):\n",
    "                if date.split(' ')[i].isdigit():\n",
    "                    day = date.split(' ')[i]\n",
    "        else:\n",
    "            for i in range(0, 3):\n",
    "                if len(date.split(' ')) <= i:\n",
    "                    break\n",
    "                if date.split(' ')[i].isdigit():\n",
    "                    day = date.split(' ')[i]\n",
    "                    \n",
    "    if (day != None) and (len(day) < 2):\n",
    "        day = '0' + day                \n",
    "     \n",
    "    centuries = [\"seiscientos\", \"setecientos\", \"ochocientos\", \"novecientos\"]\n",
    "    century = None\n",
    "    cent_pos = None\n",
    "    for cent in centuries:\n",
    "        if date.find(cent) != -1:                \n",
    "            if cent_pos == None:\n",
    "                century = cent\n",
    "                cent_pos = date.find(century)\n",
    "            elif date.find(cent) < cent_pos:                    \n",
    "                century = cent\n",
    "                cent_pos = date.find(century)        \n",
    "    if century != None:\n",
    "        year_str = None\n",
    "        dec_str = None\n",
    "        for yr in years:\n",
    "            if date.find(yr, cent_pos + 1) != -1:                    \n",
    "                if year_str == None:\n",
    "                    year_str = yr\n",
    "                else:\n",
    "                    if date.find(yr, cent_pos) < date.find(year_str, cent_pos):\n",
    "                        year_str = yr\n",
    "        for decade in decades:                \n",
    "            if date.find(decade, cent_pos) != -1:                    \n",
    "                dec_str = decade\n",
    "                break\n",
    "        if (year_str != None) and (dec_str != None):\n",
    "            year = 1000 + 100 * (centuries.index(century) + 6) + 10 * (decades.index(dec_str) + 1) + years.index(year_str) + 1\n",
    "            year = str(year)\n",
    "        elif (year_str == None) and (dec_str != None):\n",
    "            year = 1000 + 100 * (centuries.index(century) + 6) + 10 * (decades.index(dec_str) + 1)\n",
    "            year = str(year)\n",
    "        elif (year_str != None) and (dec_str == None):\n",
    "            year = 1000 + 100 * (centuries.index(century) + 6) + years.index(year_str) + 1\n",
    "            year = str(year)\n",
    "        else:\n",
    "            year = str(1000 + 100 * (centuries.index(century) + 6))\n",
    "                \n",
    "    if year == None:\n",
    "        for token in date.split(' '):\n",
    "            if token.isdigit() and (int(token) > 1000) and (int(token) < 2000):\n",
    "                year = token    \n",
    "    \n",
    "    if year == None:\n",
    "        year = \"????\"\n",
    "    if month == None:\n",
    "        month = \"??\"\n",
    "    if day == None:\n",
    "        day = \"??\"\n",
    "    \n",
    "    return year, month, day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def determine_event_date(entry_text, entities, event_type, volume_metadata, event_ref_pos=None):\n",
    "    '''\n",
    "    determines the date of a specific event\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "        entities: entities of all kinds extracted from that entry by an NER model\n",
    "        event_type: this could be either a valid record_type OR a secondary event like a birth\n",
    "        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata\n",
    "        event_ref_pos: optional index for reference to secondary event (to determine most likely date by proximity)\n",
    "        \n",
    "        returns: the date of the event in question, or None if no date can be identified\n",
    "    '''\n",
    "    date = None\n",
    "    date_start = None\n",
    "    \n",
    "    if event_type != volume_metadata[\"type\"]:        \n",
    "        primary_event_date = determine_event_date(entry_text, entities, volume_metadata[\"type\"], volume_metadata)\n",
    "        for index, entity in entities.iterrows():\n",
    "            if (entity['pred_label'] == 'DATE') and (entity['pred_entity'] != primary_event_date) and (date == None):\n",
    "                date = entity['pred_entity']\n",
    "                date_start = entity['pred_start']\n",
    "            elif (entity['pred_label'] == 'DATE') and (entity['pred_entity'] != primary_event_date):\n",
    "                if event_ref_pos == None:\n",
    "                    date = entity['pred_entity']\n",
    "                else:\n",
    "                    if (abs(event_ref_pos - entity['pred_start']) < abs(event_ref_pos - date_start)):\n",
    "                        date = entity['pred_entity']\n",
    "                        date_start = entity['pred_start']\n",
    "    \n",
    "    elif volume_metadata[\"type\"] == \"baptism\":\n",
    "        entry_length = len(entry_text)\n",
    "        \n",
    "        for index, entity in entities.iterrows():\n",
    "            if (entity['pred_label'] == 'DATE') and (entity['pred_start'] <= (entry_length / 3)):\n",
    "                date = entity['pred_entity']        \n",
    "                \n",
    "    else:\n",
    "        print(\"That event type is not supported yet.\")\n",
    "        \n",
    "    year, month, day = normalize_date(date)    \n",
    "    \n",
    "    return year + '-' + month + '-' + day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def determine_event_location(entry_text, entities, event_type, volume_metadata, event_ref_pos=None):\n",
    "    '''\n",
    "    determines the location of a specific event\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "        entities: entities of all kinds extracted from that entry by an NER model\n",
    "        event_type: this could be either a valid record_type OR a secondary event like a birth\n",
    "        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata\n",
    "        event_ref_pos: optional index for reference to secondary event (to determine most likely date by proximity)\n",
    "        \n",
    "        returns: the location of the event in question, or None if no date can be identified\n",
    "    '''\n",
    "    location = None\n",
    "    \n",
    "    if event_type == volume_metadata[\"type\"]:\n",
    "        location = volume_metadata[\"institution\"]    \n",
    "    \n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def identify_cleric(entry_text, entities):\n",
    "    '''\n",
    "    identifies the cleric(s) associated with a sacramental entry\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "        entities: entities of all kinds extracted from that entry by an NER model        \n",
    "        \n",
    "        returns: the associated cleric(s), or None if no date can be identified\n",
    "    '''\n",
    "    clerics = None\n",
    "    \n",
    "    for index, entity in entities.iterrows():\n",
    "            if ((entity['pred_label'] == 'PER') and ((len(entry_text) - entity['pred_end']) <= 10) and (len(entry_text) > 100)):\n",
    "                clerics = entity['pred_entity']\n",
    "            #going to keep this condition for now, but it can create false positives when long, incorrect entities are extracted\n",
    "            #from short and/or garbled entries\n",
    "            elif (entity['pred_entity'] != None) and (len(entry_text) - entity['pred_end'] <= 2) and (entity['pred_label'] == 'PER'):\n",
    "                clerics = entity['pred_entity']                                                 \n",
    "                \n",
    "    if clerics == None:\n",
    "        pvs_label = None\n",
    "        pvs_end = None\n",
    "        for index, entity in entities.iterrows():\n",
    "            if entity['pred_label'] == 'PER' and pvs_label == 'DATE' and (entity['pred_start'] - pvs_end) <= 15:\n",
    "                clerics = entity['pred_entity']                \n",
    "            pvs_label = entity['pred_label']\n",
    "            pvs_end = entity['pred_end']\n",
    "    \n",
    "    if clerics == None:\n",
    "        entry_text = entry_text.lower()\n",
    "        for index, entity in entities.iterrows():\n",
    "            if entity['pred_label'] == 'PER' and entry_text.find(\"cura\", entity['pred_start'] + len(entity['pred_entity'])) != -1 and ((entry_text.find(\"cura\", entity['pred_start'] + len(entity['pred_entity']))) - entity['pred_end']) <= 15:\n",
    "                clerics = entity['pred_entity']                \n",
    "    \n",
    "    return clerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def build_event(entry_text, entities, event_type, principals, volume_metadata, n_event_within_entry, unique_individuals):\n",
    "    '''\n",
    "    builds out relationships related to a baptism or burial event\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "        entities: entities of all kinds extracted from that entry by an NER model\n",
    "        event_type: this could be either a valid record_type OR a secondary event like a birth\n",
    "        principals: the principal(s) of the event, as indicated by determine_principals\n",
    "        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata\n",
    "        unique_individuals: as determined by id_unique_individuals and/or meta-function of disambig pipeline\n",
    "        \n",
    "        n_event_within_entry: event number within entry\n",
    "        \n",
    "        returns: structured representation of these relationships, including (but not necessarily limited to)\n",
    "        the event's principal, the date of the event, the location of the event, and the associated cleric\n",
    "    '''   \n",
    "    event_id = volume_metadata[\"id\"] + '-' + entities.iloc[0]['entry_no'] + '-E' + str(n_event_within_entry)    \n",
    "    #it's possible that this function should also be returning an event iterator,\n",
    "    #but for now I'm planning to do that in build_relationships\n",
    "    \n",
    "    if event_type == \"baptism\":\n",
    "        if principals != None:           \n",
    "            principal = principals[0]\n",
    "        else:\n",
    "            principal = None\n",
    "        date = determine_event_date(entry_text, entities, event_type, volume_metadata)\n",
    "        location = determine_event_location(entry_text, entities, event_type, volume_metadata)\n",
    "        cleric = identify_cleric(entry_text, entities)\n",
    "        \n",
    "        found_principal_id = False\n",
    "        found_cleric_id = False\n",
    "        for index, entity in unique_individuals.iterrows():\n",
    "            if entity['pred_entity'] == principal:\n",
    "                principal = entity['unique_id']\n",
    "                found_principal_id = True\n",
    "                continue\n",
    "            elif entity['pred_entity'] == cleric:\n",
    "                cleric = entity['unique_id']\n",
    "                found_cleric_id = True                \n",
    "        \n",
    "        if (principal != None) and (found_principal_id == False):\n",
    "            principal = None\n",
    "        if (cleric != None) and (found_cleric_id == False):\n",
    "            cleric = None\n",
    "    \n",
    "    elif event_type == \"birth\":\n",
    "        if principals != None:           \n",
    "            principal = principals[0]\n",
    "        else:\n",
    "            principal = None\n",
    "        date = determine_event_date(entry_text, entities, event_type, volume_metadata)\n",
    "        location = determine_event_location(entry_text, entities, event_type, volume_metadata)\n",
    "        cleric = None\n",
    "        \n",
    "        found_principal_id = False        \n",
    "        for index, entity in unique_individuals.iterrows():\n",
    "            if entity['pred_entity'] == principal:\n",
    "                principal = entity['unique_id']\n",
    "                found_principal_id = True\n",
    "                break                \n",
    "        \n",
    "        if (principal != None) and (found_principal_id == False):\n",
    "            principal = None\n",
    "        \n",
    "    else:\n",
    "        print(\"That event type can't be built yet.\")\n",
    "        return\n",
    "    \n",
    "    event_relationships = {\"id\": event_id, \"type\": event_type, \"principal\": principal, \"date\": date, \"location\": location, \"cleric\": cleric}\n",
    "        \n",
    "    return event_relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def drop_obvious_duplicates(people, principals, cleric):\n",
    "    '''\n",
    "    first-pass disambiguation that drops multiple mentions of cleric and principal(s)\n",
    "        people: df containing all entities labeled as people in the entry\n",
    "        principals: as indicated by determine_principals\n",
    "        \n",
    "        returns: people df with obvious duplicates dropped\n",
    "    '''   \n",
    "    found_principal = False\n",
    "    found_cleric = False       \n",
    "    \n",
    "    if len(principals) == 1:\n",
    "        for index, person in people.iterrows():\n",
    "            if (person['pred_entity'] == principals[0]) and (found_principal == False):\n",
    "                found_principal = True\n",
    "            elif person['pred_entity'] == principals[0]:                \n",
    "                people.drop(index, inplace=True)\n",
    "                \n",
    "            if cleric != None:\n",
    "                if (person['pred_entity'] == cleric) and (found_cleric == False):\n",
    "                    found_cleric = True\n",
    "                elif person['pred_entity'] == cleric:                \n",
    "                    people.drop(index, inplace=True)\n",
    "   \n",
    "    people.reset_index(inplace=True)\n",
    "    \n",
    "    return people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def id_obvious_duplicates(people, principals, cleric):\n",
    "    '''\n",
    "    first-pass disambiguation that identifies multiple mentions of cleric and principal(s)\n",
    "        people: df containing all entities labeled as people in the entry with unique ids\n",
    "        principals: as indicated by determine_principals\n",
    "        cleric: as identified by identify_cleric\n",
    "        \n",
    "        returns: dictionary with two keys, each containing list of ids corresponding to each mention of individual in question \n",
    "    '''   \n",
    "   \n",
    "    obv_dups = {\"principal\":[], \"cleric\":[]}\n",
    "    \n",
    "    #clumsy fix for corner case where principal and cleric share first name    \n",
    "    if (len(principals) > 0) and (cleric != None):\n",
    "        for principal in principals:        \n",
    "            for p in principal.split(' '):\n",
    "                if p in cleric:\n",
    "                    return obv_dups   \n",
    "    \n",
    "    if (principals != None) and (len(principals) == 1):\n",
    "        \n",
    "        for index, person in people.iterrows():\n",
    "            \n",
    "            if (person['pred_entity'] == principals[0]) or ((len(person[\"pred_entity\"].split(' ')) == 1) and (person[\"pred_entity\"] in principals[0])):\n",
    "                obv_dups[\"principal\"].append(person[\"unique_id\"])           \n",
    "            \n",
    "            if (person['pred_entity'] == cleric):\n",
    "                    obv_dups[\"cleric\"].append(person[\"unique_id\"])\n",
    "    \n",
    "    return obv_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def assign_unique_ids(people, volume_metadata, entry_number=None):\n",
    "    '''\n",
    "    assigns unique ids to each person in an entry\n",
    "        people: df containing all entities labeled as people in the entry that has received first-pass disambiguation\n",
    "        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata\n",
    "        entry_number: compound id containing folio and folio-specific entry ids\n",
    "        \n",
    "        returns: people df with column containing unique ids appended, next available id\n",
    "    '''\n",
    "    size = len(people.index)    \n",
    "    \n",
    "    if size == 0:\n",
    "        return people, volume_metadata[\"id\"] + '-' + entry_number + \"-P1\"\n",
    "    \n",
    "    unique_ids = []\n",
    "    entry_id = volume_metadata[\"id\"] + '-' + people.iloc[0]['entry_no']\n",
    "    \n",
    "    for i in range(size):\n",
    "        unique_ids.append(entry_id + '-P' + str(i+1))\n",
    "        \n",
    "    people['unique_id'] = unique_ids\n",
    "    next_id = entry_id + '-P' + str(i+2)\n",
    "    \n",
    "    return people, next_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def merge_records(records_to_merge):\n",
    "    '''\n",
    "    merge two or more dictionaries with some (but possibly not all) shared keys\n",
    "        records_to_merge: list containing two or more dictionaries to merge\n",
    "        \n",
    "        returns: single, merged dictionary\n",
    "    '''\n",
    "    #why is this necessary?\n",
    "    if len(records_to_merge) <= 1:\n",
    "        return records_to_merge\n",
    "    \n",
    "    merged_record = records_to_merge[0]\n",
    "    \n",
    "    for i in range(1, len(records_to_merge)):\n",
    "        record = records_to_merge[i]\n",
    "        for key in record:\n",
    "            if record[key] != None:                \n",
    "                if merged_record[key] == None:\n",
    "                    merged_record[key] = record[key]\n",
    "                else:\n",
    "                    if key == 'relationships':\n",
    "                        for rel in record[key]:\n",
    "                            if rel in merged_record[key]:\n",
    "                                continue\n",
    "                            else:\n",
    "                                merged_record[key].append(rel)\n",
    "                    else:\n",
    "                        values = record[key].split(';')\n",
    "                        for value in values:\n",
    "                            if value.lower() in merged_record[key].lower():\n",
    "                                continue\n",
    "                            else:\n",
    "                                merged_record[key] += ';' + value               \n",
    "                \n",
    "    return merged_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def merge_duplicates(people, duplicates):\n",
    "    '''\n",
    "    merge two or more dictionaries with some (but possibly not all) shared keys\n",
    "        people: dataframe in which each row is a person\n",
    "        duplicates: dictionary containing keys \"principal\" and \"cleric\";\n",
    "        the value of each key is a list containing unique ids for each\n",
    "        mention of the appropriate individual\n",
    "        \n",
    "        returns: dataframe with duplicate mentions of each individual type merged\n",
    "    '''\n",
    "    \n",
    "    if (len(duplicates[\"principal\"]) > 1):\n",
    "        dups = []\n",
    "        del_indices = []\n",
    "        for person in people:\n",
    "            if (person['id'] in duplicates[\"principal\"]):\n",
    "                dups.append(person)\n",
    "                del_indices.append(people.index(person))\n",
    "        minus = 0\n",
    "        del_indices.sort()\n",
    "        for index in del_indices:\n",
    "            del people[index - minus]\n",
    "            minus += 1\n",
    "        people.append(merge_records(dups))\n",
    "    \n",
    "    if (len(duplicates[\"cleric\"]) > 1):\n",
    "        dups = []\n",
    "        del_indices = []\n",
    "        for person in people:\n",
    "            if (person['id'] in duplicates[\"cleric\"]):\n",
    "                dups.append(person)\n",
    "                del_indices.append(people.index(person))\n",
    "        minus = 0\n",
    "        del_indices.sort()\n",
    "        for index in del_indices:\n",
    "            del people[index - minus]\n",
    "            minus += 1\n",
    "        people.append(merge_records(dups))\n",
    "    \n",
    "    return people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def build_new_person(people_df, next_id, person_type):\n",
    "    '''\n",
    "    appends a row representing a new person to an existing people df\n",
    "        people_df: df containing all entities labeled as people in the entry with unique ids\n",
    "        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata\n",
    "        person_type: type of person being added, e.g. \"principal\" or \"cleric\"\n",
    "        \n",
    "        returns: updated df with new person added and updated next available id\n",
    "    '''    \n",
    "    \n",
    "    curr_entry = next_id[next_id.find('-') + 1:next_id.find('P') - 1]\n",
    "    person_name = \"Unknown \" + person_type\n",
    "    person_number = next_id[next_id.find('P') + 1:]\n",
    "    person_number = int(person_number)\n",
    "    \n",
    "    people_df = people_df.append({\"entry_no\": curr_entry, \"pred_entity\": person_name, \"pred_label\": \"PER\", \"unique_id\": next_id}, ignore_index=True)\n",
    "    \n",
    "    next_id = next_id[:next_id.find('P') + 1] + str(person_number + 1)\n",
    "    \n",
    "    return people_df, next_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def build_entry_metadata(entry_text, entities, path_to_volume_xml, entry_number=None):\n",
    "    '''\n",
    "    applies rules-based engine for relationship linking to the transcription of a single entry\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity        \n",
    "        entities: entities of all kinds extracted from that entry by an NER model\n",
    "        path_to_volume_xml: path to xml file containing full volume transcription and volume-level metadata\n",
    "        entry_number: entry number, also from spaCy\n",
    "            \n",
    "        returns: three lists containing structured data about the people, places, and events that appear in the entry\n",
    "    '''\n",
    "        \n",
    "    people = []\n",
    "    places = []\n",
    "    events = []\n",
    "    \n",
    "    volume_metadata = retrieve_volume_metadata(path_to_volume_xml)\n",
    "    people_df = copy.deepcopy(entities.loc[entities['pred_label'] == 'PER'])\n",
    "    people_df.reset_index(inplace=True)\n",
    "    people_df, next_id = assign_unique_ids(people_df, volume_metadata, entry_number)\n",
    "    characteristics_df = copy.deepcopy(entities.loc[entities['pred_label'] == 'CHAR'])\n",
    "    characteristics_df.reset_index(inplace=True)\n",
    "    dates_df = copy.deepcopy(entities.loc[entities['pred_label'] == 'DATE'])\n",
    "    dates_df.reset_index(inplace=True)\n",
    "    \n",
    "    if volume_metadata[\"type\"] == \"baptism\":\n",
    "        principal = determine_principals(entry_text, entities, 1)\n",
    "        \n",
    "        if principal == None:            \n",
    "            people_df, next_id = build_new_person(people_df, next_id, \"principal\")\n",
    "            principal = [\"Unknown principal\"]            \n",
    "            \n",
    "        cleric = identify_cleric(entry_text, entities)                 \n",
    "        \n",
    "        events.append(build_event(entry_text, entities, \"baptism\", principal, volume_metadata, 1, people_df))\n",
    "        \n",
    "        if (len(dates_df.index) > 1):\n",
    "            events.append(build_event(entry_text, entities, \"birth\", principal, volume_metadata, 2, people_df))\n",
    "        \n",
    "        characteristics_df = categorize_characteristics(entities, characteristics_df)\n",
    "        people = assign_characteristics(entry_text, entities, characteristics_df, people_df, volume_metadata)       \n",
    "        people = alt_assign_relationships(entry_text, entities, people_df, people, volume_metadata)      \n",
    "        obvious_duplicates = id_obvious_duplicates(people_df, principal, cleric)       \n",
    "        people = merge_duplicates(people, obvious_duplicates)\n",
    "        \n",
    "        #perform more sophisticated disambiguation\n",
    "        \n",
    "        for event in events:\n",
    "            if (event[\"location\"] != None) and (not (event[\"location\"] in places)):\n",
    "                places.append(event[\"location\"])\n",
    "    \n",
    "    elif volume_metadata[\"type\"] == \"marriage\":        \n",
    "        #process marriage record\n",
    "        print(\"That record type is not supported yet.\")\n",
    "        return None\n",
    "    elif volume_metadata[\"type\"] == \"burial\":\n",
    "        #process burial record\n",
    "        print(\"That record type is not supported yet.\")\n",
    "        return None\n",
    "    else:\n",
    "        print(\"That record type is not supported yet.\")\n",
    "        return None    \n",
    "\n",
    "    return people, places, events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 12-ssda-xml-parser.ipynb.\n",
      "Converted 31-collate-xml-entities-spans.ipynb.\n",
      "Converted 33-split-data.ipynb.\n",
      "Converted 41-generic-framework-for-spacy-training.ipynb.\n",
      "Converted 42-initial-model.ipynb.\n",
      "Converted 51-data-preprocessing.ipynb.\n",
      "Converted 52-unstructured-to-markup.ipynb.\n",
      "Converted 53-markup-to-spatial-historian.ipynb.\n",
      "Converted 54-utility-functions.ipynb.\n",
      "Converted 61-prodigy-output-training-demo.ipynb.\n",
      "Converted 62-full-model-application-demo.ipynb.\n",
      "Converted 63-pt-model-training.ipynb.\n",
      "Converted 64-es-model-training.ipynb.\n",
      "Converted 65-all-annotations-model-training.ipynb.\n",
      "Converted 66-es-guatemala-model-training.ipynb.\n",
      "Converted 67-death-and-birth-records-together.ipynb.\n",
      "Converted 70-exhaustive-training.ipynb.\n",
      "Converted 71-relationship-builder.ipynb.\n",
      "Converted 72-full-volume-processor.ipynb.\n",
      "Converted 73-table-output.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
