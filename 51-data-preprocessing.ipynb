{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import srsly\n",
    "from ssda_nlp.xml_parser import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def xml_to_jsonl(path_to_xml_transcription):\n",
    "    xml_transcription = open(path_to_xml_transcription, 'r', encoding=\"utf-8\")\n",
    "    prodigy_input = open(path_to_xml_transcription[:path_to_xml_transcription.find(\".xml\")] + \".jsonl\", 'w', encoding=\"utf-8\")\n",
    "    \n",
    "    in_entry = False\n",
    "    current_entry = ''\n",
    "    \n",
    "    for line in xml_transcription:\n",
    "        if \"<entry\" in line:\n",
    "            in_entry = True            \n",
    "        elif in_entry and (\"</entry>\" in line):\n",
    "            current_entry += line[:line.find(\"</entry>\")]\n",
    "            in_entry = False            \n",
    "            prodigy_input.write(\"{\\\"text\\\":\\\"\" + current_entry + \"\\\"}\\n\")\n",
    "            current_entry = ''\n",
    "        elif in_entry:\n",
    "            while line[0] == ' ':\n",
    "                line = line[1:]\n",
    "            if ((line[len(line) - 1] == '\\n') or (line[len(line) - 1] == '\\r')) and (line[len(line) - 2] == '-'):\n",
    "                current_entry += line[:len(line) - 2]\n",
    "            elif line[len(line) - 1] == '-':\n",
    "                current_entry += line[:len(line) - 1]\n",
    "            elif (line == '\\n') or (line == '\\r'):                \n",
    "                continue\n",
    "            elif (line[len(line) - 1] == '\\n') or (line[len(line) - 1] == '\\r'):\n",
    "                current_entry += line[:len(line) - 1] + ' '\n",
    "            else:                \n",
    "                while line[len(line) - 1] == ' ':\n",
    "                    line = line[:-1]\n",
    "                current_entry += line\n",
    "            \n",
    "    xml_transcription.close()\n",
    "    prodigy_input.close()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def parse_annotation(path_to_annotation):   \n",
    "    annotation = srsly.read_jsonl(path_to_annotation)\n",
    "    \n",
    "    spans = []\n",
    "    texts = []\n",
    "    \n",
    "    for entry in annotation:        \n",
    "        texts.append(entry[\"text\"])\n",
    "        temp = []\n",
    "        if \"spans\" in entry:            \n",
    "            for span in entry[\"spans\"]:\n",
    "                temp.append([span[\"start\"], span[\"end\"], span[\"label\"]])\n",
    "        spans.append(temp)\n",
    "    \n",
    "    #build list of unique entries and list of empty annotation dictionaries for each    \n",
    "    annot_ls = []\n",
    "    \n",
    "    for text in texts:        \n",
    "        annot_ls.append({\"entities\":[]})\n",
    "            \n",
    "    #populate annotation dictionaries\n",
    "    for i in range(len(texts)):\n",
    "        for span in spans[i]:        \n",
    "            annot_ls[i][\"entities\"].append((int(span[0]), int(span[1]), span[2]))       \n",
    "        \n",
    "    #build list of tuples\n",
    "    tuples = []    \n",
    "    for i in range(len(texts)):\n",
    "        tuples.append((texts[i], annot_ls[i]))\n",
    "        \n",
    "    return tuples       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def prodigy_output_to_collated_df(path_to_annotation):\n",
    "    tuples = parse_annotation(path_to_annotation)\n",
    "    entry_nos = []\n",
    "    entry_texts = []\n",
    "    entities = []\n",
    "    starts = []\n",
    "    ends = []\n",
    "    labels = []\n",
    "    entry = 1\n",
    "    for tup in tuples:\n",
    "        for entity in tup[1][\"entities\"]:\n",
    "            entry_nos.append(entry)\n",
    "            entry_texts.append(tup[0])\n",
    "            entities.append(tup[0][entity[0]:entity[1]])\n",
    "            starts.append(entity[0])\n",
    "            ends.append(entity[1])\n",
    "            labels.append(entity[2])\n",
    "        entry += 1\n",
    "    \n",
    "    collated_dict = {\"entry_no\": entry_nos, \"text\": entry_texts, \"entity\": entities, \"start\": starts, \"end\": ends, \"label\": labels}\n",
    "    \n",
    "    collated_df = pd.DataFrame(collated_dict)    \n",
    "    \n",
    "    return collated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 12-ssda-xml-parser.ipynb.\n",
      "Converted 31-collate-xml-entities-spans.ipynb.\n",
      "Converted 33-split-data.ipynb.\n",
      "Converted 41-generic-framework-for-spacy-training.ipynb.\n",
      "Converted 42-initial-model.ipynb.\n",
      "Converted 51-data-preprocessing.ipynb.\n",
      "Converted 52-unstructured-to-markup.ipynb.\n",
      "Converted 53-markup-to-spatial-historian.ipynb.\n",
      "Converted 54-utility-functions.ipynb.\n",
      "Converted 61-prodigy-output-training-demo.ipynb.\n",
      "Converted 62-full-model-application-demo.ipynb.\n",
      "Converted 63-pt-model-training.ipynb.\n",
      "Converted 64-es-model-training.ipynb.\n",
      "Converted 65-all-annotations-model-training.ipynb.\n",
      "Converted 66-es-guatemala-model-training.ipynb.\n",
      "Converted 67-death-and-birth-records-together.ipynb.\n",
      "Converted 71-relationship-builder.ipynb.\n",
      "Converted 72-full-volume-processor.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
