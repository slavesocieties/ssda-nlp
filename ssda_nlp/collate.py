# AUTOGENERATED! DO NOT EDIT! File to edit: 31-collate-xml-entities-spans.ipynb (unless otherwise specified).

__all__ = ['genAltEnts', 'generateSpans', 'parseEntities', 'parseManualEntities', 'parseXML', 'genEntryIDs',
           'collate_frames', 'manual_collate', 'genSpaCyInput', 'frames_to_spacy']

# Cell

import pandas as pd
import numpy as np

# Cell

def parseTranskribusOutput(output_paths):    
    dicts = []
    for path_to_output in output_paths:
        with open(path_to_output, "r") as f:
            text = ''
            for line in f:
                text += line[line.find('"text":') + 9:line.find("}") - 1]
        textDict = {"entry_no": path_to_output[path_to_output.find('.')], "text": text}
        dicts.append(textDict)
    return pd.DataFrame.from_dict(dicts)

def genAltEnts(entity):
    '''
    Function genAltEnts: This function takes an entity and generates all possible forms of it with errant spaces or pound
    symbols mixed in.
        Inputs: entity: String of entity
        Output: list of possible forms of the entity with spaces and pound signs inserted
    '''
    alt_ents = []

    #individual characters replaced by ' '
    for i in range(1, len(entity)):
        alt_ents.append((entity[:i]) + ' ' + entity[i:])

    #individual characters replaced by #
    for j in range(1, len(entity)):
        alt_ents.append((entity[:j]) + '#' + entity[j + 1:])

    #missing last name
    #if (len(entity.split(' ')) > 1):
        #alt_ents.append(entity[:entity.rfind(' ')])

    return alt_ents

# Cell
#search entry for each instance of an entity reference

def generateSpans(entry, entity):
    '''
    Function generateSpans: This function takes individual entries and entities from the merged dataframes and
    returns a list containing a span defining each instance in which that entity appears in that entry
    (spans are returned as nested lists).
        Inputs: entry: String of entry text
                entity: String of entity whose span is to be located within the text
        Output: nested list of spans for the entity for each place it appears within the entry.
    '''
    curr_index = 0
    spans = []
    entity_len = len(entity)

    #Scroll through each entry and find each instance of the entity and append spans if not already present
    while entry.find(entity, curr_index) != -1:
        entity_start = entry.find(entity, curr_index)
        span = [entity_start, entity_start + entity_len]
        curr_index = span[1]
        if span not in spans:
            spans.append(span)

    #If no spans are present
    if spans == []:
        alts = genAltEnts(entity)
        for alt in alts:
            alt_len = len(alt)
            while entry.find(alt, curr_index) != -1:
                entity_start = entry.find(alt, curr_index)
                span = [entity_start, entity_start + alt_len]
                curr_index = span[1]
                if span not in spans:
                    spans.append(span)

    return spans

# Cell
def parseEntities(df_entities):
    '''
    Function parseEntities: takes entity reference dataframe as input, returns each column as a list for ease of processing
    and boils source references down to unique portion
        Inputs: df_entities: pandas DataFrame with columns "Entity Type", "Name", and "Source Associator"
        Outputs: 3 lists: entity types, entities, and parsed source associators
    '''
    types = df_entities["Entity Type"].tolist()
    names = df_entities["Name"].tolist()

    sources = []
    for source_assoc in df_entities["Source Associator"]:
        temp = []
        refs = source_assoc.split(';')
        for ref in refs:
            source = ref[ref.find('-') + 1:]
            while source[0] == '0':
                source = source[1:]
            if (not source in temp):
                temp.append(source)
        sources.append(temp)

    return types, names, sources

# Cell
def parseManualEntities(csv_entities):
    '''
    Function parseManualEntities: takes manually produced entity reference csv as input and produces same outputs
    as those of parseEntities
    Inputs: csv_entities: path to csv containing two columns, one with folio ids and one with a list of names appearing on
    each folio, separated by semicolons
    Outputs: 3 lists: entity types, entities, and parsed source associatos
    '''
    csv = open(csv_entities, 'r', encoding="utf-8")
    folios = []
    people = []

    for line in csv:
        if line[-1] == '\n':
            line = line[:-1]
        if "Folio" in line:
            continue
        folio, temp = line.split(',')
        ppl = temp.split(';')
        folios.append(folio)
        people.append(ppl)

    csv.close()

    names = []
    sources = []

    for x in range(len(folios)):
        for person in people[x]:
            if not (person in names):
                names.append(person)
                temp = [folios[x]]
                sources.append(temp)
            else:
                loc = names.index(person)
                if not (folios[x] in sources[loc]):
                    sources[loc].append(folios[x])

    types = ["PER"] * len(names)

    return types, names, sources

# Cell
def parseXML(df_XML):
    '''
    Function parseXML: takes volume XML dataframe as input and returns each column as a list
        Inputs: df_XML: pandas DataFrame with columns "vol_id", 'vol_titl', 'fol_id', 'entry_no', and 'text'
        Outputs: 5 lists corresonding to each column: volume ids, volume titles, folio ids, entry numbers, and entry texts
    '''
    volume_ids = df_XML["vol_id"].tolist()
    volume_titles = df_XML["vol_titl"].tolist()
    folio_ids = df_XML["fol_id"].tolist()
    entry_numbers = df_XML["entry_no"].tolist()
    entry_texts = df_XML["text"].tolist()

    return volume_ids, volume_titles, folio_ids, entry_numbers, entry_texts

# Cell
def genEntryIDs(folio_ids, entry_ids, texts, entity, entity_folio):
    '''
    Function genEntryIDs: returns of list of IDs for all entries in which an input entity appears in a folio.
        Inputs: Sets of lists, where the index of each list correponds to the information in every other list index (i.e., each list is a column of a dataframe)
                folio_ids: list of folio IDs to be processed
                entry_ids: list of IDs for each entry
                texts: list of entry texts
                entity: entity to be found in the data
                entity_folio: folio in which to look for the entity
        Outputs: 5 lists corresonding to each column: volume ids, volume titles, folio ids, entry numbers, and entry texts
    '''

    #find the indices of the entity folio of interest
    np_fol = np.array(folio_ids)
    folio_indices = np.where(np_fol == entity_folio)[0]

    matches = []

    #Determine if the entity is in the folios if interest and append entry_id if so
    for entries in folio_indices:
        if texts[entries].find(entity) != -1:
            matches.append(entry_ids[entries])

    #Check the to see if maybe the entity has unusual representation with # or ' ' inserted
    if matches == []:
        alts = genAltEnts(entity)
        for alt in alts:
            for entries in folio_indices:
                if (texts[entries].find(alt) != -1) and (entry_ids[entries] not in matches):
                    matches.append(entry_ids[entries])

    return matches

# Cell

def collate_frames(xml_df, ent_df):
    '''
    Function collate_frames: combines XML dataframe and entity dataframe to generate final dataframe with spans
        Inputs: xml_df: pandas DataFrame with columns "vol_id", 'vol_titl', 'fol_id', 'entry_no', and 'text'
                ent_df: pandas DataFrame with columns "Entity Type", "Name", and "Source Associator"
        Outputs: a joined, collated dataframe of xml_df and ent_df
    '''
    #parsing input frames into lists
    vol_ids, vol_titls, fol_ids, entry_nos, entry_texts = parseXML(xml_df)
    ent_types, ent_names, ent_fols = parseEntities(ent_df)

    #building entry ID lists for entities
    ent_entries = [[]] * len(ent_names)
    index = 0

    for entity in ent_names:
        for folio in ent_fols[index]:
            ent_entries[index] = ent_entries[index] + (genEntryIDs(fol_ids, entry_nos, entry_texts, entity, folio))
        index += 1

    #creating collated lists
    out_volume_ids = []
    out_volume_titles = []
    out_folio_ids = []
    out_entry_numbers = []
    out_entry_texts = []
    out_entity_names = []
    out_span_starts = []
    out_span_ends = []
    out_labels = []

    index = 0 #this is clumsy and could probably be implemented better with numpy,
    #but it's to protect against multiple instances of the same entity name

    for entity in ent_names:
        for entry in ent_entries[index]:
            folio_id = entry[:entry.find('-')]
            volume_id = vol_ids[fol_ids.index(folio_id)]
            volume_title = vol_titls[fol_ids.index(folio_id)]
            entry_text = entry_texts[entry_nos.index(entry)]
            label = ent_types[ent_names.index(entity)]

            spans = generateSpans(entry_text, entity)
            for span in spans:
                out_volume_ids.append(volume_id)
                out_volume_titles.append(volume_title)
                out_folio_ids.append(folio_id)
                out_entry_numbers.append(entry)
                out_entry_texts.append(entry_text)
                out_entity_names.append(entity)
                out_span_starts.append(span[0])
                out_span_ends.append(span[1])
                out_labels.append(label)

        index += 1

    #The loop below addresses a rare corner case that could result in tokens being mapped to multiple entities, which spaCy
    #does not allow. This occurs when one entity string in a given entry is a substring of another that appears in the same
    #entry.

    spanned_entries = [] #list of unique entries with entity spans
    spanned_tokens_per_entry = [] #list of indices that appear in spans for each entry
    i = 0
    while i < len(out_entry_numbers):
        if out_entry_numbers[i] in spanned_entries:
            for k in range(out_span_starts[i], out_span_ends[i]):
                if k in spanned_tokens_per_entry[spanned_entries.index(out_entry_numbers[i])]:
                    del out_volume_ids[i]
                    del out_volume_titles[i]
                    del out_folio_ids[i]
                    del out_entry_numbers[i]
                    del out_entry_texts[i]
                    del out_entity_names[i]
                    del out_span_starts[i]
                    del out_span_ends[i]
                    del out_labels[i]
                    i -= 1
                    break
                else:
                    spanned_tokens_per_entry[spanned_entries.index(out_entry_numbers[i])].append(k)
        else:
            spanned_entries.append(out_entry_numbers[i])
            temp = []
            for j in range(out_span_starts[i], out_span_ends[i]):
                temp.append(j)
            spanned_tokens_per_entry.append(temp)
        i += 1

    collated_dict = {"vol_id": out_volume_ids, "vol_titl": out_volume_titles, "fol_id": out_folio_ids, "entry_no": out_entry_numbers, "text": out_entry_texts, "entity": out_entity_names, "start": out_span_starts, "end": out_span_ends, "label": out_labels}

    collated_df = pd.DataFrame(collated_dict)
    return collated_df


# Cell

def manual_collate(xml_df, ent_csv):
    #parsing input frames into lists
    vol_ids, vol_titls, fol_ids, entry_nos, entry_texts = parseXML(xml_df)
    ent_types, ent_names, ent_fols = parseManualEntities(ent_csv)

    #building entry ID lists for entities
    ent_entries = [[]] * len(ent_names)
    index = 0

    for entity in ent_names:
        for folio in ent_fols[index]:
            ent_entries[index] = ent_entries[index] + (genEntryIDs(fol_ids, entry_nos, entry_texts, entity, folio))
        index += 1

    #creating collated lists
    out_volume_ids = []
    out_volume_titles = []
    out_folio_ids = []
    out_entry_numbers = []
    out_entry_texts = []
    out_entity_names = []
    out_span_starts = []
    out_span_ends = []
    out_labels = []

    index = 0 #this is clumsy and could probably be implemented better with numpy,
    #but it's to protect against multiple instances of the same entity name

    for entity in ent_names:
        for entry in ent_entries[index]:
            folio_id = entry[:entry.find('-')]
            volume_id = vol_ids[fol_ids.index(folio_id)]
            volume_title = vol_titls[fol_ids.index(folio_id)]
            entry_text = entry_texts[entry_nos.index(entry)]
            label = ent_types[ent_names.index(entity)]

            spans = generateSpans(entry_text, entity)
            for span in spans:
                out_volume_ids.append(volume_id)
                out_volume_titles.append(volume_title)
                out_folio_ids.append(folio_id)
                out_entry_numbers.append(entry)
                out_entry_texts.append(entry_text)
                out_entity_names.append(entity)
                out_span_starts.append(span[0])
                out_span_ends.append(span[1])
                out_labels.append(label)

        index += 1

    #The loop below addresses a rare corner case that could result in tokens being mapped to multiple entities, which spaCy
    #does not allow. This occurs when one entity string in a given entry is a substring of another that appears in the same
    #entry.

    spanned_entries = [] #list of unique entries with entity spans
    spanned_entities_per_entry = [] #list of unique entity name strings in each entry

    for entry_number in out_entry_numbers:
        if not (entry_number in spanned_entries):
            spanned_entries.append(entry_number)
            spanned_entities_per_entry.append([])

    for i in range(len(out_entity_names)):
        entry_pos = spanned_entries.index(out_entry_numbers[i])
        if not (out_entity_names[i] in spanned_entities_per_entry[entry_pos]):
            spanned_entities_per_entry[entry_pos].append(out_entity_names[i])

    output_pos = 0
    while output_pos < len(out_entity_names):
        entity_entry = spanned_entries.index(out_entry_numbers[output_pos])
        for spanned_entity in spanned_entities_per_entry[entity_entry]:
            if (out_entity_names[output_pos] in spanned_entity) and (out_entity_names[output_pos] != spanned_entity):
                del out_volume_ids[output_pos]
                del out_volume_titles[output_pos]
                del out_folio_ids[output_pos]
                del out_entry_numbers[output_pos]
                del out_entry_texts[output_pos]
                del out_entity_names[output_pos]
                del out_span_starts[output_pos]
                del out_span_ends[output_pos]
                del out_labels[output_pos]
                output_pos -= 1
                break
        output_pos += 1

    collated_dict = {"vol_id": out_volume_ids, "vol_titl": out_volume_titles, "fol_id": out_folio_ids, "entry_no": out_entry_numbers, "text": out_entry_texts, "entity": out_entity_names, "start": out_span_starts, "end": out_span_ends, "label": out_labels}

    collated_df = pd.DataFrame(collated_dict)
    return collated_df

# Cell

def genSpaCyInput(df):
    '''
    Function genSpaCyInput:function takes a dataframe with columns/contents as above and transforms it into a list of tuples, each consisting of raw text
    paired with a dictionary of entities that appear in that text, for input into spaCy.
        Inputs: df: dataframe with columns 'text', 'label', 'start', 'end'
        Outputs: data in SpaCy format
    '''
    text = df["text"].tolist()
    label = df["label"].tolist()
    start = df["start"].tolist()
    end = df["end"].tolist()

    #build list of unique entries and list of empty annotation dictionaries for each
    u_txt = []
    annot_ls = []

    for tx in text:
        if not (tx in u_txt):
            u_txt.append(tx)
            annot_ls.append({"entities":[]})

    #populate annotation dictionaries
    for i in range(len(label)):
        pos = u_txt.index(text[i])
        annot_ls[pos]["entities"].append((int(start[i]), int(end[i]), label[i]))

    #build list of tuples
    tuples = []
    for i in range(len(u_txt)):
        tuples.append((u_txt[i], annot_ls[i]))

    return tuples

# Cell

def frames_to_spacy(xml_df, ent_df):
    df = collate_frames(xml_df, ent_df)
    return genSpaCyInput(df)