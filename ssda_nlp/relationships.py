# AUTOGENERATED! DO NOT EDIT! File to edit: 71-relationship-builder.ipynb (unless otherwise specified).

__all__ = ['retrieve_controlled_vocabularies', 'categorize_characteristics', 'assign_characteristics',
           'assign_relationships', 'id_unique_individuals', 'find_sus', 'split_name_col', 'disambiguate',
           'determine_principals', 'determine_event_date', 'determine_event_location', 'identify_cleric', 'build_event',
           'drop_obvious_duplicates', 'id_obvious_duplicates', 'assign_unique_ids', 'merge_records', 'merge_duplicates',
           'build_entry_metadata']

# Cell
#dependencies

#nlp packages
import spacy
from spacy.util import minibatch, compounding

#manipulation of tables/arrays
import pandas as pd
import numpy as np
import copy

#internal imports
from .collate import *
from .split_data import *
from .modeling import *
from .model_performance_utils import *
from .xml_parser import *
from .unstructured2markup import *
from .utility import *

# Cell
#ideally this function will eventually query the database live, but for now we'll use static snapshots
#of the vocabularies as they currently exist
#should also eventually be enhanced to return equivalents and a requested language

def retrieve_controlled_vocabularies():
    '''
    returns a dictionary containing current version of controlled vocabularies for characteristics
    '''
    age = ["parv", "adul", "niño", "niña", "nino", "nina", "dulto", "dulta"]
    occupation = ["religioso", "ingen.o", "sacristan", "sac.", "sachristan", "cura", "vicario", "eclesiastico", "clerigo", "estudiante"]
    phenotype = ["negro", "negra", "preto", "moreno", "morena", "indio", "india", "pardo", "parda", "mestizo", "mestiza", "mulato", "mulata", "blanco", "blanca", "criollo", "criolla", "branco", "branca"]
    titles = ["don", "doña", "padre"]
    ranks = ["capitan", "capitam"]
    ethnicities = ["ganga", "español", "espanol", "caravali", "ingles", "yngles", "angola", "carabalí", "carabali", "carabaly", "congo", "conga", "mandinga", "mina", "temo", "malagas", "arara", "manga"]
    status = ["clavo", "clava", "escl", "clabo", "claba", "esc.va", "esc.ba", "esc.vo", "escvo", "escva", "esc.bo", "esc.a", "esc.o", "libre", "esc.s", "esco", "esca"]
    legitimacy = ["lexma", "lexmo", "legitima", "legitimo", "h l", "natural", "nral", "lexitima", "lexitimo", "nat.l"]
    relationships = ["hijo", "hija", "esposo", "esposa", "viudo", "viuda", "padrino", "soltera", "soltero"]

    vocabs = {"legitimacy": legitimacy, "age": age, "occupation": occupation, "phenotype": phenotype, "titles": titles, "ranks": ranks, "ethnicities": ethnicities, "status": status, "relationships": relationships}

    return vocabs

# Cell

def categorize_characteristics(characteristics_df):
    '''
    determines which category each labeled characteristic belongs to
        characteristics_df: entities given the label "CHAR" from a single entry by an NER model

        returns: the same dataframe with an additional column containing a characteristic category
    '''

    vocabs = retrieve_controlled_vocabularies()
    categories = []

    for index, characteristic in characteristics_df.iterrows():
        category = None
        for cat in vocabs:
            if (characteristic['pred_entity'] == 'h') or (characteristic['pred_entity'] == "h."):
                category = "relationships"
            if category != None:
                break
            for term in vocabs[cat]:
                if term in characteristic['pred_entity'].lower():
                        category = cat
                        break
        #if category == None:
            #print("Failed to find a category for " + characteristic['pred_entity'])
        categories.append(category)

    characteristics_df["category"] = categories

    return characteristics_df

# Cell

def assign_characteristics(entry_text, characteristics_df, unique_individuals, volume_metadata):
    '''
    matches all labeled characteristics to the correct individual(s) and builds triples
        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity
        characteristics_df: entities given the label "CHAR" from a single entry by an NER model
        unique_individuals: as determined by id_unique_individuals and/or meta-function of disambig pipeline
        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata

        returns: structured representation (a list of dictionaries)
    '''
    people = []
    ethnicities = retrieve_controlled_vocabularies()["ethnicities"]
    categorized_characteristics = categorize_characteristics(characteristics_df)
    assignments = [None] * len(characteristics_df.index)
    categorized_characteristics.reset_index(inplace=True)
    unique_individuals.reset_index(inplace=True)

    for index in range(len(categorized_characteristics)):
        if ((categorized_characteristics["category"][index] == "age") or (categorized_characteristics["category"][index] == "legitimacy")) and (volume_metadata["type"] == "baptism"):
            principal = determine_principals(entry_text, unique_individuals, 1)[0]
            princ_loc = unique_individuals.index[unique_individuals["pred_entity"] == principal].tolist()
            for loc in princ_loc:
                if assignments[index] == None:
                    assignments[index] = unique_individuals["unique_id"][loc]
                else:
                    assignments[index] += ';' + unique_individuals["unique_id"][loc]
        elif (categorized_characteristics["category"][index] == "occupation") or (categorized_characteristics["category"][index] == "phenotype") or (categorized_characteristics["category"][index] == "ethnicities") or ((categorized_characteristics["category"][index] == "status") and (categorized_characteristics["pred_entity"][index].lower()[-1] != 's')):
            char_start = categorized_characteristics["pred_start"][index]
            lowest_diff = 50
            assign = None
            for i, person in unique_individuals.iterrows():
                person_start = person["pred_start"]
                diff = char_start - person_start
                if (diff > 0) and (diff < lowest_diff):
                    lowest_diff = diff
                    assign = i
            if assign != None:
                assignments[index] = unique_individuals["unique_id"][assign]
        elif categorized_characteristics["category"][index] == "status":
            char_start = categorized_characteristics["pred_start"][index]
            lowest_diff = 30
            second_lowest_diff = 50
            assign = [None, None]
            for i, person in unique_individuals.iterrows():
                person_start = person["pred_start"]
                diff = char_start - person_start
                if (diff > 0) and (diff < lowest_diff):
                    lowest_diff = diff
                    if assign[0] != None:
                        assign[1] = assign[0]
                        second_lowest_diff = lowest_diff
                    assign[0] = i
                elif (diff > 0) and (diff < second_lowest_diff) and (assign[0] != None):
                    second_lowest_diff = diff
                    assign[1] = i
            ids = []
            for a in assign:
                if a != None:
                    ids.append(unique_individuals["unique_id"][a])
            if len(ids) == 2:
                assignments[index] = ids[0] + ';' + ids[1]
            elif len(ids) == 1:
                assignments[index] = ids[0]

    categorized_characteristics["assignment"] = assignments

    #display(categorized_characteristics)

    for i in range(len(unique_individuals.index)):

        characteristics = {"ethnicities":[], "age":None, "legitimacy":None,"occupation":[], "phenotype":[], "status":None}

        for eth in ethnicities:
            if eth in unique_individuals["pred_entity"][i].lower():
                characteristics["ethnicities"].append(eth[0].upper() + eth[1:])

        for j in range(len(categorized_characteristics.index)):
            if (categorized_characteristics["assignment"][j] == None):
                continue
            if unique_individuals["unique_id"][i] in categorized_characteristics["assignment"][j]:
                if (categorized_characteristics["category"][j] == "age") or (categorized_characteristics["category"][j] == "legitimacy") or (categorized_characteristics["category"][j] == "status"):
                    characteristics[categorized_characteristics["category"][j]] = categorized_characteristics["pred_entity"][j]
                else:
                    characteristics[categorized_characteristics["category"][j]].append(categorized_characteristics["pred_entity"][j])

        person_record = {"id": unique_individuals["unique_id"][i], "name": unique_individuals["pred_entity"][i]}

        for key in characteristics:
            if ((key=="ethnicities") or (key == "occupation") or (key == "phenotype")) and (len(characteristics[key]) > 0):
                person_record[key] = characteristics[key][0]
                if (len(characteristics[key]) > 1):
                    for char in range(1,len(characteristics[key]) + 1):
                        person_record[key] += ';' + characteristics[key][char]
            elif (characteristics[key] != None) and (characteristics[key] != []):
                person_record[key] = characteristics[key]

        people.append(person_record)

    return people

# Cell

def assign_relationships(entry_text, entities, unique_individuals):
    '''
    Relationship types:
        parent/child --> P. and P.s are parents
        godparents/godchildren --> P.P and p.s are godparents
        slaveholders/enslaved
        spouses
        grandparents
        '''
    #display(entities.head(15))
    rel_df = entities.loc[entities['pred_label'] == 'REL']
    rel_df.reset_index(inplace=True)
    rel_df = rel_df.drop('index',axis=1)
    #display(rel_df.head()) #Comment this out in final function, this is just for quick verification

    rel = 0 #Varibale telling us later whether or not this entry has any identified relationships
    previous = 0 #Variable telling us whether or not the previous REL combined two entities (i.e. P. and P. into P.P. and thus can skip the second P. entity)
    event_id = volume_metadata["id"] + '-' + entities.iloc[0]['entry_no']
    my_relations = []
    m,n = entities.shape
    for i in range(m):
        if (entities.iloc[i,2]=='REL'):
            rel = 1 #Relationship present
            #We must check to make sure the first entity isn't a REL or it breaks the func due to positional index error
            if i==0 or i==(m-1):
                print("First/last entity is a REL, this functionality is not yet supported.")
            #We must treat this differently because we can typically extract both padrinos instead of
            #just a single relationship
            elif entities.iloc[i,1]=='P.P.':
                try:
                    #This gathers the first name, probably the padrino
                    my_triple = (entities.iloc[i-1,1],(entities.iloc[i,1]),(entities.iloc[i+1,1]))
                    my_relations.append(my_triple)
                    #This should be the second name, probably the madrina
                    my_triple = (entities.iloc[i-1,1],(entities.iloc[i,1]),(entities.iloc[i+2,1]))
                    my_relations.append(my_triple)
                except:
                    print("Exception: had last entity in DF as a REL and thus out of bounds in current form of function")
            elif (entities.iloc[i-1,2]=='PER') and (entities.iloc[i+1,2]=='PER'):
                try:
                    my_triple = (entities.iloc[i-1,1],(entities.iloc[i,1]),(entities.iloc[i+1,1]))
                    my_relations.append(my_triple)
                except:
                    print("Exception: had last entity in DF as a REL and thus out of bounds in current form of function")
            #Checking if we have back-to-back entities in the form of 'P.' followed by 'P.'
            elif((entities.iloc[i-1,2]=='PER') and ('P' in entities.iloc[i+1,1])):
                print("Please manually check me at index = ", index)
                previous = 1
                if (entities.iloc[i+2,2]=='PER'):
                    my_triple = (entities.iloc[i-1,1],'P. P.',(entities.iloc[i+2,1]))
                    my_relations.append(my_triple)
            #Skipping the second entity from the above case in the next iteration
            elif previous:
                previous = 0
            else:
                print("Relationship found, but not between adjacent people")
    if rel:
        #print(entry_text) #Uncomment this for verification
        #print()
        print(my_relations)
        #print("------------")
        #print()

    #Currently does not return the relationship triples... need to build out some functionality to deal with that as they're returned

# Cell

def id_unique_individuals(entry_text, entities, volume_metadata):
    '''
    identifies all unique individuals that appear in an entry (i.e. removing all multiple mentions of the same person)
        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity
        entities: entities of all kinds extracted from that entry by an NER model
        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata

        returns: a list of the unique individuals who appear in an entry AND (temporary?) unique IDs for each individual
    '''
    event_id = volume_metadata["id"] + '-' + entities.iloc[0]['entry_no']

    people_df = entities.loc[entities['pred_label'] == 'PER']
    people_df.reset_index(inplace=True)
    people_df = people_df.drop('index',axis=1)
    unique_individuals = people_df['pred_entity'].unique()
    unique_individuals = np.vstack([unique_individuals, [None] * len(unique_individuals)])

    for i in range(len(unique_individuals[0])):
        unique_individuals[1][i] = event_id + '-P' + str(i + 1)

    return unique_individuals

# Cell

def find_sus(entry_text, entities, sus_df, index):
    '''
    identifies corner cases: all entries where there are multiple entities that 1) have the same first name appearing
        multiple times, 2) have compound names and then a segment of that name appearing, and 3) have a full name with
        the first name by itself appearing
    Note that this should not be used in tandem with id_unique_individuals, as that function just drops the duplicate names

    params:
        entry_text: actual text for comparison
        entities: df of entities identified
        sus_df: either the empty df body or the df from previously loop iterations
        i: current row that the loop is on in DEMO_DF

    returns: df of all the entries that may be corner cases, in the same form demo_df, but with two added id columns
    '''
    #Set up
    people_df = entities.loc[entities['pred_label'] == 'PER']
    people_df.reset_index(inplace=True)
    people_df = people_df.drop('index',axis=1)

    my_rows = len(people_df.index)
    hold = my_rows * [0]
    people_df['name_status'] = hold
    first_names = []
    check_against = []
    dups = 0
    sus = 0

    #Get a list of all the first names that appear in the entities/people_df
    #  This is definitely not the most computationally efficient way to do this
    for i in range(my_rows):
        #Separate people based on whether it is a first name or a full/compound name
        if (" " in people_df.iloc[i,1]) or ("-" in people_df.iloc[i,1]):
            check_against.append(people_df.iloc[i,1])
        elif ~(" " in people_df.iloc[i,1]): #No spaces thus we are assuming it is a first name
            first_names.append(people_df.iloc[i,1])
    #Check to see whether they are subsets of full/compound names
    if len(first_names)>0 and len(check_against)>0:
        for j in range(len(first_names)):
            for k in range(len(check_against)):
                if first_names[j] in check_against[k]:
                    #Mark this entire entry as sus
                    sus = 1
    #Generally check to see if there are any duplicate entities (same name) in the entry
    if people_df['pred_entity'].duplicated().any():
        dups = 1;
    #Set the status column
    if sus and dups:
        status = 11 #ie both sus and dups are true
    elif sus:
        status = 10 #ie sus true, dups false
    elif dups:
        status = 0.01 #ie sus false, dups true
    else:
        status = 0
    #ie if the entry is suspect or has duplicates, then add it to sus_df
    if status>0:
        if len(sus_df.index)<1:
            data = [{'vol_titl':demo_df.iloc[index,0], 'vol_id':demo_df.iloc[index,1], 'fol_id':demo_df.iloc[index,2],
                    'text':demo_df.iloc[index,3],'entry_no':entry_no,'suspect':status}]
            sus_df = pd.DataFrame(data)
        else:
            sus_df = sus_df.append({'vol_titl':demo_df.iloc[index,0], 'vol_id':demo_df.iloc[index,1], 'fol_id':demo_df.iloc[index,2],
                    'text':demo_df.iloc[index,3],'entry_no':entry_no,'suspect':status},ignore_index=True)
    return sus_df

# Cell

def split_name_col(people_df):
    '''
    from the fed in entities, strips DF to only include people, then separates based on if it is a first name or a full name


    ### Functionality is not fully realized yet, could probably be generalized further, but this entire task may not be necessary
    '''
    #Set up
    my_rows = len(people_df.index)
    hold = my_rows * [0]
    people_df['name_status'] = hold

    #Separate into two based on first/single and full name status
    for i in range(my_rows):
        if "-" in people_df.iloc[i,1]:
            people_df.iloc[i,5] = 2 #2 therefore represents compound name
        elif " " in people_df.iloc[i,1]:
            people_df.iloc[i,5] = 1 #1 therefore represents a full name
        else: #Must be a single name
            #0 therefore represents a full name
            pass
    first_n = people_df[people_df.name_status == 0]
    full_n = people_df[people_df.name_status == 1]
    cmpd_n = people_df[people_df.name_status == 2]

    print("DF of first names")
    display(first_n.head())
    print("DF of full names")
    display(full_n.head())
    print("DF of compound names")
    display(cmpd_n.head())
    print("---------------------")

    return first_n, full_n, cmpd_n

# Cell

def disambiguate():
    '''
    goes through the problem cases previously identified and then applies split_name_col to break the entities down into
        the ones that may be
    '''
    people_df = entities.loc[entities['pred_label'] == 'PER']
    people_df.reset_index(inplace=True)
    people_df = people_df.drop('index',axis=1)

    first_n, full_n, cmpd_n = split_name_col(people_df)


# Cell

def determine_principals(entry_text, entities, n_principals):
    '''
    determines the principal of a single-principal event
        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity
        entities: entities of all kinds extracted from that entry by an NER model
        n_principals: expected number of principals

        returns: the principal(s) of the event in question, or None if no principal can be identified
    '''

    entry_text = entry_text.lower()
    principals = []

    if n_principals == 1:

        for index, entity in entities.iterrows():
            if (entity['pred_label'] == 'PER') and (entity['pred_start'] <= 20):
                principals.append(entity['pred_entity'])

        if len(principals) == 0:
            prox = entry_text.find('oleos')
            if prox != -1:
                for index, entity in entities.iterrows():
                    if (entity['pred_label'] == 'PER') and (abs(entity['pred_start'] - prox) <= 10):
                        principals.append(entity['pred_entity'])

        if len(principals) == 0:
            prox = entry_text.find('nombre')
            if prox != -1:
                for index, entity in entities.iterrows():
                    if (entity['pred_label'] == 'PER') and (abs(entity['pred_start'] - prox) <= 10):
                        principals.append(entity['pred_entity'])

    elif n_principals == 2:
        print("That number of principals is not supported yet.")
        return None
        #process marriage principals
    else:
        print("Invalid number of principals.")
        return None

    return principals

# Cell

def determine_event_date(entry_text, entities, event_type, volume_metadata, event_ref_pos=None):
    '''
    determines the date of a specific event
        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity
        entities: entities of all kinds extracted from that entry by an NER model
        event_type: this could be either a valid record_type OR a secondary event like a birth
        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata
        event_ref_pos: optional index for reference to secondary event (to determine most likely date by proximity)

        returns: the date of the event in question, or None if no date can be identified
    '''
    date = None
    date_start = None

    if event_type != volume_metadata["type"]:
        primary_event_date = determine_event_date(entry_text, entities, volume_metadata["type"], volume_metadata)
        for index, entity in entities.iterrows():
            if (entity['pred_label'] == 'DATE') and (entity['pred_entity'] != primary_event_date) and (date == None):
                date = entity['pred_entity']
                date_start = entity['pred_start']
            elif (entity['pred_label'] == 'DATE') and (entity['pred_entity'] != primary_event_date):
                if event_ref_pos == None:
                    date = entity['pred_entity']
                else:
                    if (abs(event_ref_pos - entity['pred_start']) < abs(event_ref_pos - date_start)):
                        date = entity['pred_entity']
                        date_start = entity['pred_start']

    elif volume_metadata["type"] == "baptism":
        entry_length = len(entry_text)

        for index, entity in entities.iterrows():
            if (entity['pred_label'] == 'DATE') and (entity['pred_start'] <= (entry_length / 3)):
                date = entity['pred_entity']

    else:
        date = "That event type is not supported yet."

    return date

# Cell

def determine_event_location(entry_text, entities, event_type, volume_metadata, event_ref_pos=None):
    '''
    determines the location of a specific event
        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity
        entities: entities of all kinds extracted from that entry by an NER model
        event_type: this could be either a valid record_type OR a secondary event like a birth
        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata
        event_ref_pos: optional index for reference to secondary event (to determine most likely date by proximity)

        returns: the location of the event in question, or None if no date can be identified
    '''
    location = None

    if event_type == volume_metadata["type"]:
        location = volume_metadata["institution"]

    return location

# Cell

def identify_cleric(entry_text, entities):
    '''
    identifies the cleric(s) associated with a sacramental entry
        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity
        entities: entities of all kinds extracted from that entry by an NER model

        returns: the associated cleric(s), or None if no date can be identified
    '''
    clerics = None

    for index, entity in entities.iterrows():
            if ((entity['pred_label'] == 'PER') and ((len(entry_text) - entity['pred_end']) <= 10) and (len(entry_text) > 100)):
                clerics = entity['pred_entity']
            #going to keep this condition for now, but it can create false positives when long, incorrect entities are extracted
            #from short and/or garbled entries
            elif (entity['pred_entity'] != None) and (len(entry_text) - entity['pred_end'] <= 2) and (entity['pred_label'] == 'PER'):
                clerics = entity['pred_entity']

    if clerics == None:
        pvs_label = None
        pvs_end = None
        for index, entity in entities.iterrows():
            if entity['pred_label'] == 'PER' and pvs_label == 'DATE' and (entity['pred_start'] - pvs_end) <= 15:
                clerics = entity['pred_entity']
            pvs_label = entity['pred_label']
            pvs_end = entity['pred_end']

    if clerics == None:
        entry_text = entry_text.lower()
        for index, entity in entities.iterrows():
            if entity['pred_label'] == 'PER' and entry_text.find("cura", entity['pred_start'] + len(entity['pred_entity'])) != -1 and ((entry_text.find("cura", entity['pred_start'] + len(entity['pred_entity']))) - entity['pred_end']) <= 15:
                clerics = entity['pred_entity']

    return clerics

# Cell

def build_event(entry_text, entities, event_type, principals, volume_metadata, n_event_within_entry, unique_individuals):
    '''
    builds out relationships related to a baptism or burial event
        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity
        entities: entities of all kinds extracted from that entry by an NER model
        event_type: this could be either a valid record_type OR a secondary event like a birth
        principals: the principal(s) of the event, as indicated by determine_principals
        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata
        unique_individuals: as determined by id_unique_individuals and/or meta-function of disambig pipeline

        n_event_within_entry: event number within entry

        returns: structured representation of these relationships, including (but not necessarily limited to)
        the event's principal, the date of the event, the location of the event, and the associated cleric
    '''
    event_id = volume_metadata["id"] + '-' + entities.iloc[0]['entry_no'] + '-E' + str(n_event_within_entry)
    #it's possible that this function should also be returning an event iterator,
    #but for now I'm planning to do that in build_relationships

    if event_type == "baptism":
        if len(principals) == 0:
            principal = None
        else:
            principal = principals[0]
        date = determine_event_date(entry_text, entities, event_type, volume_metadata)
        location = determine_event_location(entry_text, entities, event_type, volume_metadata)
        cleric = identify_cleric(entry_text, entities)

        found_principal_id = False
        found_cleric_id = False
        for index, entity in unique_individuals.iterrows():
            if entity['pred_entity'] == principal:
                principal = entity['unique_id']
                found_principal_id = True
                continue
            elif entity['pred_entity'] == cleric:
                cleric = entity['unique_id']
                found_cleric_id = True

        if (principal != None) and (found_cleric_id == False):
            principal = None
        if (cleric != None) and (found_cleric_id == False):
            cleric = None
    elif event_type == "birth":
        if len(principals) == 0:
            principal = None
        else:
            principal = principals[0]
        date = determine_event_date(entry_text, entities, event_type, volume_metadata)
        location = determine_event_location(entry_text, entities, event_type, volume_metadata)
        cleric = None
    else:
        print("That event type can't be built yet.")
        return

    event_relationships = {"id": event_id, "type": event_type, "principal": principal, "date": date, "location": location, "cleric": cleric}

    return event_relationships

# Cell

def drop_obvious_duplicates(people, principals, cleric):
    '''
    first-pass disambiguation that drops multiple mentions of cleric and principal(s)
        people: df containing all entities labeled as people in the entry
        principals: as indicated by determine_principals

        returns: people df with obvious duplicates dropped
    '''
    found_principal = False
    found_cleric = False

    if len(principals) == 1:
        for index, person in people.iterrows():
            if (person['pred_entity'] == principals[0]) and (found_principal == False):
                found_principal = True
            elif person['pred_entity'] == principals[0]:
                people.drop(index, inplace=True)

            if cleric != None:
                if (person['pred_entity'] == cleric) and (found_cleric == False):
                    found_cleric = True
                elif person['pred_entity'] == cleric:
                    people.drop(index, inplace=True)

    people.reset_index(inplace=True)

    return people

# Cell

def id_obvious_duplicates(people, principals, cleric):
    '''
    first-pass disambiguation that identifies multiple mentions of cleric and principal(s)
        people: df containing all entities labeled as people in the entry with unique ids
        principals: as indicated by determine_principals

        returns: dictionary with two keys, each containing list of ids corresponding to each mention of individual in question
    '''

    obv_dups = {"principal":[], "cleric":[]}

    if len(principals) == 1:

        for index, person in people.iterrows():

            if (person['pred_entity'] == principals[0]):
                obv_dups["principal"].append(person["unique_id"])

            if (person['pred_entity'] == cleric):
                    obv_dups["cleric"].append(person["unique_id"])

    return obv_dups

# Cell

def assign_unique_ids(people, volume_metadata):
    '''
    assigns unique ids to each person in an entry
        people: df containing all entities labeled as people in the entry that has received first-pass disambiguation
        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata

        returns: people df with column containing unique ids appended
    '''
    size = len(people.index)

    if size == 0:
        return people

    unique_ids = []
    entry_id = volume_metadata["id"] + '-' + people.iloc[0]['entry_no']

    for i in range(size):
        unique_ids.append(entry_id + '-P' + str(i+1))

    people['unique_id'] = unique_ids

    return people

# Cell

def merge_records(records_to_merge):
    '''
    merge two or more dictionaries with some (but possibly not all) shared keys
        records_to_merge: list containing two or more dictionaries to merge

        returns: single, merged dictionary
    '''
    merged_record = records_to_merge[0]

    for i in range(1, len(records_to_merge)):
        record = records_to_merge[i]
        for key in record:
            if key in merged_record:
                values = record[key].split(';')
                for value in values:
                    if value in merged_record[key]:
                        continue
                    else:
                        merged_record[key] += ';' + value
            else:
                merged_record[key] = record[key]

    return merged_record

# Cell

def merge_duplicates(people, duplicates):

    if (len(duplicates["principal"]) > 1):
        dups = []
        for person in people:
            if (person['id'] in duplicates["principal"]):
                dups.append(person)
                del people[people.index(person)]
        people.append(merge_records(dups))

    if (len(duplicates["cleric"]) > 1):
        dups = []
        for person in people:
            if (person['id'] in duplicates["cleric"]):
                dups.append(person)
                del people[people.index(person)]
        people.append(merge_records(dups))

    return people

# Cell

def build_entry_metadata(entry_text, entities, path_to_volume_xml):
    '''
    Master function that will combine all helper functions built above
        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity
        entities: entities of all kinds extracted from that entry by an NER model
        path_to_volume_xml: path to xml file containing full volume transcription and volume-level metadata

        returns: paths to three JSON files containg, respectively,
        metadata re people, places, and events that appear in the entry
    '''

    people = []
    places = []
    events = []

    volume_metadata = retrieve_volume_metadata(path_to_volume_xml)
    people_df = copy.deepcopy(entities.loc[entities['pred_label'] == 'PER'])
    people_df.reset_index(inplace=True)
    people_df = assign_unique_ids(people_df, volume_metadata)
    characteristics_df = copy.deepcopy(entities.loc[entities['pred_label'] == 'CHAR'])
    characteristics_df.reset_index(inplace=True)
    dates_df = copy.deepcopy(entities.loc[entities['pred_label'] == 'DATE'])
    dates_df.reset_index(inplace=True)

    if volume_metadata["type"] == "baptism":
        principal = determine_principals(entry_text, entities, 1)
        cleric = identify_cleric(entry_text, entities)
        events.append(build_event(entry_text, entities, "baptism", principal, volume_metadata, 1, people_df))
        if (len(dates_df.index) > 1):
            events.append(build_event(entry_text, entities, "birth", principal, volume_metadata, 2, people_df))

        #assign interpersonal relationships to people

        characteristics_df = categorize_characteristics(characteristics_df)
        people = assign_characteristics(entry_text, characteristics_df, people_df, volume_metadata)

        obvious_duplicates = id_obvious_duplicates(people_df, principal, cleric)
        people = merge_duplicates(people, obvious_duplicates)

        #perform more sophisticated disambiguation

        for event in events:
            if (event["location"] != None) and (not (event["location"] in places)):
                places.append(event["location"])

    elif volume_metadata["type"] == "marriage":
        #process marriage record
        print("That record type is not supported yet.")
        return None
    elif volume_metadata["type"] == "burial":
        #process burial record
        print("That record type is not supported yet.")
        return None
    else:
        print("That record type is not supported yet.")
        return None

    #code that turns pieces defined above into well-formed relationships

    return people, places, events