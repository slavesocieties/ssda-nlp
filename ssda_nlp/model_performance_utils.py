# AUTOGENERATED! DO NOT EDIT! File to edit: 42-initial-model.ipynb (unless otherwise specified).

__all__ = ['model_meta_training', 'get_ner_df', 'get_corrects_df', 'get_fns_df', 'get_fps_df']

# Cell
#export
#data structure imports
import pandas as pd
import numpy as np

#python imports
import random

#modeling imports
from spacy.util import fix_random_seed
from .collate import *
#from ssda.entity_corpus import *
#from ssda.xml_parser import *
from .split_data import *
from .modeling import *

# Cell
def model_meta_training(mdl, train_spacy, valid_df, eval_metric = 'f_score', patience_max = 5, save_dir = None, verbose=False, **kwargs):
    '''
    Function model_meta_training: a model wrapping function which implements early stopping based on model improvement on the validation set.
        Inputs: mdl: Spacy model, blank or pretrained
                train_spacy: training data in Spacy format
                valid_df: dataframe of validation data split
                eval_metric: metric on which the validation performance should be evaluated.  Can be `f_score`, `precision` or `recall`.
                patience_max (default 5): Number of iterations to wait for the model performance to improve
                save_dir (default None): String of data directory.  Use this parameter if you want the best model and its performance to be saved to disk.
                verbose (default False): Boolean indicating whether you want to print the performance after every *iterations* iterations.
                **kwargs: keyword arguments which will be passed directly to `train_model`.  You should include at least the parameter n_iter and set it low
                    to something like 10.
        Returns: mdl: Spacy model, although the original variable you passed in will still be a valid reference.  NOTE THAT THIS MODEL WILL BE OVERTRAINED BY
                      PATIENCE CYCLES.  If you want the "best" model back, you'll need to load it from the saved directory.
                 perf_df: pandas dataframe with the training performance (avg_cycle_loss) and validation performance (precision, recall, f_score) at every
                      n_iter iterations, including the patience cycles.
    '''

    #loop parameters
    keep_training = True
    old_metric_val = -1
    cycle_no = 1
    patience = patience_max

    #output datafarme
    df_cols = ['cycle_no', 'avg_cycle_loss', 'precision', 'recall', 'f_score']
    perf_df = pd.DataFrame(columns = df_cols)

    #training loop
    while keep_training is True:

        # Train for the number of desired iterations
        mdl, loss_df = train_model(mdl, train_spacy, **kwargs)

        # Test based on the validation set to get performance and log
        ent_preds_df, metrics_df, per_ent_metrics = test_model(mdl, valid_df, 'entry_no', 'text')
        cycle_df_0 = pd.DataFrame(np.array([[cycle_no, loss_df['epoch_loss'].mean()]]), columns=df_cols[:2])
        cycle_df = pd.concat([cycle_df_0, metrics_df], axis=1)
        perf_df = pd.concat([perf_df, cycle_df])

        #print if desired
        if verbose: display(cycle_df)

        # If the performance is *strictly* better than previous performance (sometimes the performance is the same), save the model and performance
        if metrics_df.loc[0, eval_metric] > old_metric_val:
            old_metric_val = metrics_df.loc[0, eval_metric]
            if save_dir is not None:
                save_model(mdl, save_dir)

            #modify looping parameters
            cycle_no += 1
            patience = patience_max
        else:
            # If the performance isn't better, wait patience cycles
            patience -= 1
            if verbose: print("Performance hasn't improved for {0} cycles...".format(patience_max - patience))
            cycle_no += 1

            #If you've exhausted the patience, end training
            if patience==0:

                #Stop training loop
                keep_training=False

                #Save performance
                perf_df.reset_index(drop=True, inplace=True)
                if save_dir is not None:
                    perf_df.to_csv(save_dir + '/perf_df.csv', index=False)
                if verbose:
                    print('Done training after {0} meta cycles.'.format(cycle_no-1))

    return mdl, perf_df

# Cell
def get_ner_df(split_df, ent_df, verbose=True):
    '''
    Function get_ner_df: create joined dataframe of the ground truth dataframe and the prediction dataframe (from Spacy)
        Inputs: split_df: dataframe of split data (e.g., train, test, or validation)
                ent_df: dataframe of entities predicted from the data (e.g., from ssda.modeling: test_model)
                verbose (default True): True if you want to print shape information about the dataframes created.
        Returns: dataframe of fully joined results (an outer join so all rows are reported, regardless of whether they have matches)
    '''

    #join original dataframe df with entities recognized df
    ner_df = pd.merge(split_df, ent_df, left_on=['entry_no', 'entity', 'start', 'end'],
                   right_on =['entry_no', 'pred_entity', 'pred_start', 'pred_end'], how='outer')

    #print if verbose
    if verbose: print('Fully joined results size: ', len(ner_df))

    return ner_df

# Cell
def get_corrects_df(ner_df, split_df, verbose=True):
    '''
    Function get_corrects_df: get dataframe of entities which were correctly identified
        Inputs: ner_df: fully joined dataframe of entity ground truth and prediction results (e.g., from `get_ner_df`)
                split_df: dataframe of split data (e.g., train, test, or validation)
                verbose (default True): True if you want to print shape information about the dataframes created.
        Returns: dataframe of correctly identified entities with metadata
    '''
    corrs_df = ner_df.dropna().sort_values(['vol_id', 'entry_no'])
    if verbose: print("Number correctly identified: ", corrs_df.shape[0], " of ", split_df.shape[0])

    return corrs_df

# Cell
def get_fns_df(ner_df, split_df, verbose=True):
    '''
    Function get_fns_df: get dataframe of ground truth entities which were not predicted by the model
        Inputs: ner_df: fully joined dataframe of entity ground truth and prediction results (e.g., from `get_ner_df`)
                split_df: dataframe of split data (e.g., train, test, or validation)
                verbose (default True): True if you want to print shape information about the dataframes created.
        Returns: dataframe of false negatives (ground truth entities which were not predicted)
    '''

    #get predictions that were missed
    fn_df = ner_df[ner_df[['pred_entity', 'pred_start', 'pred_end']].isna().any(axis=1)]

    #cleanup for easy viewing
    fn_df = fn_df.copy().drop(['vol_titl', 'fol_id', 'pred_entity', 'pred_label', 'pred_start', 'pred_end'], axis=1)
    fn_df.sort_values(['vol_id','entry_no'], inplace=True)

    #print if verbose
    if verbose: print("Number not identified: ", fn_df.shape[0], " of ", split_df.shape[0])

    return fn_df

# Cell
def get_fps_df(ner_df, split_df, ent_df, verbose=True):
    '''
    Function get_fps_df: get dataframe of entities predicted by the model that were not in the ground truth dataframe
        Inputs: ner_df: fully joined dataframe of entity ground truth and prediction results (e.g., from `get_ner_df`)
                split_df: dataframe of split data (e.g., train, test, or validation)
                ent_df: dataframe of entities predicted from the data (e.g., from ssda.modeling: test_model)
                verbose (default True): True if you want to print shape information about the dataframes created.
        Returns: dataframe of false negatives (ground truth entities which were not predicted)
    '''

    #get incorrect identifications of entities in the texts
    fp_df = ner_df[ner_df[['entity', 'start', 'end']].isna().any(axis=1)]

    #cleanup for easy viewing
    fp_df = fp_df.copy().drop(['vol_id', 'vol_titl', 'fol_id', 'entity', 'label', 'start', 'end'], axis=1)

    #insert texts and vol_id in the dataframe
    fp_df = pd.merge(fp_df, split_df[['vol_id','entry_no', 'text']], on='entry_no').drop(['text_x'], axis=1)
    fp_df.rename(columns={'text_y':'text'}, inplace=True)

    #reorder columns
    col_order = ['vol_id', 'entry_no', 'text'] + list(fp_df.columns.drop(['vol_id', 'entry_no', 'text']))
    fp_df = fp_df[col_order]
    fp_df.sort_values(['vol_id','entry_no'], inplace=True)

    #I get duplicates because NaNs
    fp_df.drop_duplicates(inplace=True)

    #print info
    if verbose:
        print('False positive entities that were not correctly identified in the texts:')
        print("Number not correctly identified: ", fp_df.shape[0], " of ", ent_df.shape[0])

    return fp_df