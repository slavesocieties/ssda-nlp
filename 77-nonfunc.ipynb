{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3322fe80",
   "metadata": {},
   "source": [
    "# Code Validation / Refactoring\n",
    "## Non-functionalized versions for easier trouble shooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4ccd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_test\n",
    "#dependencies\n",
    "\n",
    "#nlp packages\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "#manipulation of tables/arrays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "\n",
    "import difflib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "#internal imports\n",
    "from ssda_nlp.collate import *\n",
    "from ssda_nlp.split_data import *\n",
    "from ssda_nlp.modeling import *\n",
    "from ssda_nlp.model_performance_utils import *\n",
    "from ssda_nlp.xml_parser import *\n",
    "from ssda_nlp.unstructured2markup import *\n",
    "from ssda_nlp.utility import *\n",
    "from ssda_nlp.relationships import *\n",
    "from ssda_nlp.full_volume import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35941f47",
   "metadata": {},
   "source": [
    "## Making better similar_names logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f88a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def strip_accents(text):\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError: # unicode is a default on python 3 \n",
    "        pass\n",
    "\n",
    "    text = unicodedata.normalize('NFD', text)\\\n",
    "           .encode('ascii', 'ignore')\\\n",
    "           .decode(\"utf-8\")\n",
    "\n",
    "    return str(text)\n",
    "\n",
    "my_string = 'àéêöhello'\n",
    "print(my_string)\n",
    "s = strip_accents(my_string)\n",
    "print(s)\n",
    "\n",
    "my_string = \"á, é, í, ó, ú, ü, ñ, ¿, ¡, ?, #,  , !, $, @, .\"\n",
    "print(my_string)\n",
    "s = strip_accents(my_string)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42354c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_test\n",
    "\n",
    "for person_idx in range(len(entry_people)):\n",
    "    relationships = entry_people[person_idx].get('relationships')\n",
    "    \n",
    "    #are there any people with very similar names that still appear separately\n",
    "    nameTemp = entry_people[person_idx].get('name')\n",
    "    name_list.append(nameTemp)\n",
    "    \n",
    "    #1: Short name that could be a first name\n",
    "    #Make a boolean list to flag which names may be first names (based on length)\n",
    "    possibleFirstNames = []\n",
    "    fullNames = []\n",
    "    for name in name_list:\n",
    "        if name in all_first_names:\n",
    "            possibleFirstNames.append(name)\n",
    "        else:\n",
    "            fullNames.append(name)\n",
    "    #Check to see if names appear within each other (i.e. is a person double counted)\n",
    "    doubleCountedNames = []\n",
    "    for idx in range(len(possibleFirstNames)):\n",
    "        doubleCountedNames= doubleCountedNames + ([name for name in fullNames if possibleFirstNames[idx] in name])\n",
    "    if not len(doubleCountedNames)==0:\n",
    "        print(\"Possible double count (first name appears in a second instance (full name))\")\n",
    "        print(name_list)\n",
    "        similarNames = 1\n",
    "        \n",
    "    #2: Two similarly-sized names, that could be variations (i.e. missing hypens or have #'s for unknown letters')\n",
    "    # This is a bad check that needs to be replaced\n",
    "    # Doesn't even check to see if composition is similar, only length...\n",
    "    for name_idx in range(len(name_list)):\n",
    "        # Why is it even passing here?\n",
    "        if name_idx==0:\n",
    "            pass\n",
    "        else:\n",
    "            for idx in range(len(name_list)):\n",
    "                for idx2 in range(len(name_list)-idx-1):\n",
    "                    idx2 = idx2+idx+1\n",
    "                    if check_lengths(name_list, idx, idx2):\n",
    "                        print(\"Similar Names: similar size\")\n",
    "                        print(name_list)\n",
    "                        similarNames = 1 \n",
    "                        \n",
    "    #3: Strip spaces, \".\", \"#\", convert accented characters to unaccented\n",
    "    for name in name_list:\n",
    "        # Strip any accents\n",
    "        utf_name = strip_accents(name)\n",
    "        # Remove bad characters\n",
    "        condensed_name = utf_name.translate({ord(c): None for c in './\\? !@#$'})\n",
    "        # ^ How to check for similarity when # appears in one name but the actual letter does in the other?\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c30c470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#á, é, í, ó, ú, ü, ñ\n",
    "\n",
    "name_list = [\"Kai Malcolm\", \"Kai\", \"Kai Devon Malcolm\", \"Devon Malcolm\", \n",
    "             \"Dan Jenkins\", \"Daniel Jenkins\", \"Daniel Genkins\", \"Da##iel Genkin#\", \"D#l Ge#ins\", \n",
    "            \"Maria Andoval\", \"María Andoval\", \"Máríá ##o#al\",\n",
    "            \"María Dolores\", \"María Dolores Sanchez\", \"Maria Sanchez\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bade0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3: Strip spaces, \".\", \"#\", convert accented characters to unaccented\n",
    "for name in name_list:\n",
    "    # Strip any accents\n",
    "    utf_name = strip_accents(name)\n",
    "    # Remove bad characters\n",
    "    condensed_name = utf_name.translate({ord(c): None for c in './\\? !@#$'})\n",
    "    # ^ How to check for similarity when # appears in one name but the actual letter does in the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdee75c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "s_1 = 'Mohan Mehta'\n",
    "s_2 = 'Mohan Mehte'\n",
    "print(SequenceMatcher(a=s_1,b=s_2).ratio())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74d58d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8292682926829268\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "s_1 = 'Maria Dolores Sanchez'\n",
    "s_2 = 'Juan Dolores Sanchez'\n",
    "print(SequenceMatcher(a=s_1,b=s_2).ratio())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3af74da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kai Malcolm, Kai, 0.462\n",
      "---------------------\n",
      "Kai, Kai Devon Malcolm, 0.333\n",
      "---------------------\n",
      "Kai Devon Malcolm, Devon Malcolm, 0.889\n",
      "---------------------\n",
      "Devon Malcolm, Dan Jenkins, 0.273\n",
      "---------------------\n",
      "Dan Jenkins, Daniel Jenkins, 0.870\n",
      "---------------------\n",
      "Daniel Jenkins, Daniel Genkins, 0.923\n",
      "---------------------\n",
      "Daniel Genkins, Da##iel Genkin#, 0.917\n",
      "---------------------\n",
      "Da##iel Genkin#, D#l Ge#ins, 0.667\n",
      "---------------------\n",
      "D#l Ge#ins, Maria Andoval, 0.105\n",
      "---------------------\n",
      "Maria Andoval, María Andoval, 1.000\n",
      "---------------------\n",
      "María Andoval, Máríá ##o#al, 0.800\n",
      "---------------------\n",
      "Máríá ##o#al, María Dolores, 0.700\n",
      "---------------------\n",
      "María Dolores, María Dolores Sanchez, 0.774\n",
      "---------------------\n",
      "María Dolores Sanchez, Maria Sanchez, 0.774\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "for name_idx in range(len(name_list)):\n",
    "    try:\n",
    "        name1 = name_list[name_idx]\n",
    "        name2 = name_list[name_idx+1]\n",
    "        # Strip any accents, Remove bad characters\n",
    "        n1 = strip_accents(name1).translate({ord(c): None for c in './\\? !@#$'})\n",
    "        n2 = strip_accents(name2).translate({ord(c): None for c in './\\? !@#$'})\n",
    "        \n",
    "        print(f\"{name_list[name_idx]}, {name_list[name_idx+1]}, {SequenceMatcher(a=n1,b=n2).ratio():0.3f}\")\n",
    "        print(\"---------------------\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b38755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_ex1 = ['María Dolores Sanchez', 'Don Miguel o’Reilly', 'Juan', 'Deliby', 'Don Francisco Sanchez', 'Maria Dolores', 'José Rivas', 'María de la Luz Blanco']\n",
    "entry_ex2 = ['Antonio Guillo', 'Don Miguel o’Reilly', 'Isaac Guillo', 'Sara Ca mel', 'Juan Antonio', 'Antonio Pellice', 'Susana Pellicer']\n",
    "entry_ex3 = ['María Juana Francisca Fish', 'Don Miguel o’Reilly', 'Don Jeremías Fish', 'Eva Fish', 'Maria Juana Francisca', 'Lorenzo Capó', 'Juana Joaquina Gonzalez']\n",
    "#'Teniente de Cura Beneficiado'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c794e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f120cb59",
   "metadata": {},
   "source": [
    "## NB 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6534e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#this is currently configured specifically for baptisms/burials\n",
    "\n",
    "def assign_characteristics(entry_text, entities_df, characteristics_df, unique_individuals, volume_metadata):\n",
    "    '''\n",
    "    matches all labeled characteristics to the correct individual(s) and builds triples\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "        characteristics_df: entities given the label \"CHAR\" from a single entry by an NER model\n",
    "        unique_individuals: as determined by id_unique_individuals and/or meta-function of disambig pipeline\n",
    "        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata        \n",
    "    \n",
    "        returns: structured representation (a list of dictionaries)\n",
    "    '''\n",
    "    people = []\n",
    "    ethnicities = retrieve_controlled_vocabularies()[\"ethnicities\"]\n",
    "    #CATEGORIZE_CHARACTERISTICS#############################################################################\n",
    "    #def categorize_characteristics(entities_df, characteristics_df):\n",
    "    '''\n",
    "    determines which category each labeled characteristic belongs to        \n",
    "        characteristics_df: entities given the label \"CHAR\" from a single entry by an NER model        \n",
    "\n",
    "        returns: the same dataframe with an additional column containing a characteristic category\n",
    "    '''\n",
    "\n",
    "    vocabs = retrieve_controlled_vocabularies()\n",
    "    categories = []\n",
    "    uncategorized_characteristics = pd.DataFrame({\"entry_no\": pd.Series([], dtype=\"str\"), \"pred_entity\": pd.Series([], dtype=\"str\"), \"pred_label\": pd.Series([], dtype=\"str\"), \"pred_start\": pd.Series([], dtype=\"int\"), \"pred_end\": pd.Series([], dtype=\"int\"), \"assigned\": pd.Series([], dtype=\"bool\")}) \n",
    "    entities_df.reset_index(inplace = True, drop = True)    \n",
    "\n",
    "    for index, characteristic in entities_df.iterrows():        \n",
    "        #development\n",
    "        #if characteristic[\"pred_entity\"] == \"libre\":\n",
    "            #print(characteristic[\"pred_label\"])\n",
    "        if characteristic[\"pred_label\"] != \"CHAR\":\n",
    "            continue\n",
    "        category = None       \n",
    "        if characteristic[\"pred_entity\"] in [\"Natural\", \"nral\", \"Nat.l\", \"N.l\", \"nat.l\", \"natural\"]:\n",
    "            loc_indices = []\n",
    "            for i, entity in entities_df.iterrows():\n",
    "                if entity[\"pred_label\"] == \"LOC\":\n",
    "                    loc_indices.append(i)\n",
    "            for loc_index in loc_indices:\n",
    "                if ((loc_index - index) == 1):\n",
    "                    category = \"origin\"\n",
    "            if category == None:\n",
    "                category = \"legitimacy\"        \n",
    "        for cat in vocabs:\n",
    "            if (characteristic['pred_entity'] == 'h') or (characteristic['pred_entity'] == \"h.\"):\n",
    "                category = \"relationships\"\n",
    "            elif characteristic[\"pred_entity\"] == \"propiedad\":\n",
    "                category = \"status\"\n",
    "            if category != None:\n",
    "                break\n",
    "            for term in vocabs[cat]:\n",
    "                if term in characteristic['pred_entity'].lower():\n",
    "                        category = cat\n",
    "                        break\n",
    "\n",
    "        if category == None:            \n",
    "            uncat_char = uncategorized_characteristics.append(entities_df.iloc[index])\n",
    "\n",
    "        categories.append(category)\n",
    "\n",
    "    characteristics_df[\"category\"] = categories\n",
    "    uncat_char[\"category\"] = None\n",
    "    categorized_characteristics = characteristics_df\n",
    "    #return characteristics_df, uncategorized_characteristics\n",
    "    #categorized_characteristics, uncat_char = categorize_characteristics(entities_df, characteristics_df)\n",
    "    #CATEGORIZE_CHARACTERISTICS#############################################################################\n",
    "    assignments = [None] * len(characteristics_df.index)    \n",
    "    categorized_characteristics.reset_index(inplace=True)\n",
    "    unique_individuals.reset_index(inplace=True)    \n",
    "    \n",
    "    for index in range(len(categorized_characteristics)):\n",
    "        #development\n",
    "        #if categorized_characteristics[\"pred_entity\"][index] == \"libre\":\n",
    "            #print(\"libre\")\n",
    "        if ((categorized_characteristics[\"category\"][index] == \"age\") or (categorized_characteristics[\"category\"][index] == \"legitimacy\")) and (volume_metadata[\"type\"] == \"baptism\"):\n",
    "            principal = determine_principals(entry_text, unique_individuals, 1)\n",
    "            if principal != None:\n",
    "                principal = determine_principals(entry_text, unique_individuals, 1)[0]\n",
    "            else:\n",
    "                principal = \"Unknown principal\"\n",
    "            princ_loc = unique_individuals.index[unique_individuals[\"pred_entity\"] == principal].tolist()\n",
    "            for loc in princ_loc:\n",
    "                if assignments[index] == None:\n",
    "                    assignments[index] = unique_individuals[\"unique_id\"][loc]\n",
    "                else:\n",
    "                    assignments[index] += ';' + unique_individuals[\"unique_id\"][loc]\n",
    "        elif (categorized_characteristics[\"category\"][index] == \"occupation\") or (categorized_characteristics[\"category\"][index] == \"phenotype\") or (categorized_characteristics[\"category\"][index] == \"ethnicities\") or ((categorized_characteristics[\"category\"][index] == \"status\") and (categorized_characteristics[\"pred_entity\"][index].lower()[-1] != 's')):\n",
    "            char_start = categorized_characteristics[\"pred_start\"][index]\n",
    "            lowest_diff = 50\n",
    "            assign = None\n",
    "            for i, person in unique_individuals.iterrows():\n",
    "                person_start = person[\"pred_start\"]\n",
    "                diff = char_start - person_start\n",
    "                if (diff > 0) and (diff < lowest_diff):\n",
    "                    lowest_diff = diff\n",
    "                    assign = i\n",
    "            if assign != None:                \n",
    "                assignments[index] = unique_individuals[\"unique_id\"][assign]\n",
    "        elif categorized_characteristics[\"category\"][index] == \"status\":\n",
    "            char_start = categorized_characteristics[\"pred_start\"][index]\n",
    "            lowest_diff = 30\n",
    "            second_lowest_diff = 50\n",
    "            assign = [None, None]\n",
    "            for i, person in unique_individuals.iterrows():\n",
    "                person_start = person[\"pred_start\"]\n",
    "                diff = char_start - person_start\n",
    "                if (diff > 0) and (diff < lowest_diff):\n",
    "                    lowest_diff = diff\n",
    "                    if assign[0] != None:\n",
    "                        assign[1] = assign[0]\n",
    "                        second_lowest_diff = lowest_diff\n",
    "                    assign[0] = i\n",
    "                elif (diff > 0) and (diff < second_lowest_diff) and (assign[0] != None):\n",
    "                    second_lowest_diff = diff\n",
    "                    assign[1] = i\n",
    "            ids = []\n",
    "            for a in assign:\n",
    "                if a != None:\n",
    "                    ids.append(unique_individuals[\"unique_id\"][a])\n",
    "            if len(ids) == 2:\n",
    "                assignments[index] = ids[0] + ';' + ids[1]\n",
    "            elif len(ids) == 1:\n",
    "                assignments[index] = ids[0]\n",
    "        elif categorized_characteristics[\"category\"][index] == \"origin\":            \n",
    "            for i, entity in entities_df.iterrows():                \n",
    "                if entity[\"pred_start\"] == categorized_characteristics[\"pred_start\"][index]:\n",
    "                    signal_entity_index = i\n",
    "                    break            \n",
    "            if (signal_entity_index != 0) and (len(entities_df[\"pred_label\"]) > (signal_entity_index + 1)) and (entities_df[\"pred_label\"][signal_entity_index - 1] == \"PER\") and (entities_df[\"pred_label\"][signal_entity_index + 1] == \"LOC\") and (entities_df[\"pred_start\"][signal_entity_index + 1] - entities_df[\"pred_end\"][signal_entity_index - 1] <= 20):\n",
    "                place = entities_df[\"pred_entity\"][signal_entity_index + 1]\n",
    "                multiple = False\n",
    "                if categorized_characteristics[\"pred_entity\"][index] == \"naturales\":\n",
    "                    multiple = True\n",
    "                categorized_characteristics.at[index, \"pred_entity\"] = place\n",
    "                for i, person in unique_individuals.iterrows():\n",
    "                    if person[\"pred_start\"] == entities_df[\"pred_start\"][signal_entity_index - 1]:\n",
    "                        assignments[index] = person[\"unique_id\"]\n",
    "                        break\n",
    "                if multiple and (entities_df[\"pred_label\"][signal_entity_index - 2] == \"PER\") and (entities_df[\"pred_start\"][signal_entity_index - 1] - entities_df[\"pred_end\"][signal_entity_index - 2] <= 10):\n",
    "                    for i, person in unique_individuals.iterrows():\n",
    "                        if person[\"pred_start\"] == entities_df[\"pred_start\"][signal_entity_index - 2]:\n",
    "                            assignments[index] += ';' + person[\"unique_id\"]                            \n",
    "                            break\n",
    "            elif (len(entities_df[\"pred_label\"]) > (signal_entity_index + 1)) and (entities_df[\"pred_label\"][signal_entity_index + 1] == \"LOC\"):\n",
    "                place = entities_df[\"pred_entity\"][signal_entity_index + 1]                \n",
    "                categorized_characteristics.at[index, \"pred_entity\"] = place\n",
    "                principal = determine_principals(entry_text, unique_individuals, 1)\n",
    "                if principal != None:\n",
    "                    principal = determine_principals(entry_text, unique_individuals, 1)[0]\n",
    "                else:\n",
    "                    principal = \"Unknown principal\"\n",
    "                princ_loc = unique_individuals.index[unique_individuals[\"pred_entity\"] == principal].tolist()\n",
    "                for loc in princ_loc:\n",
    "                    if assignments[index] == None:\n",
    "                        assignments[index] = unique_individuals[\"unique_id\"][loc]\n",
    "                    else:\n",
    "                        assignments[index] += ';' + unique_individuals[\"unique_id\"][loc]\n",
    "\n",
    "            \n",
    "    categorized_characteristics[\"assignment\"] = assignments    \n",
    "    \n",
    "    for i in range(len(unique_individuals.index)):        \n",
    "        \n",
    "        characteristics = {\"origin\": None, \"ethnicities\":[], \"age\":None, \"legitimacy\":None,\"occupation\":[], \"phenotype\":[], \"status\":None, \"titles\":None, \"ranks\":None, \"relationships\":None}\n",
    "        \n",
    "        for eth in ethnicities:\n",
    "            if eth in unique_individuals[\"pred_entity\"][i].lower():                \n",
    "                characteristics[\"ethnicities\"].append(eth[0].upper() + eth[1:])        \n",
    "        \n",
    "        for j in range(len(categorized_characteristics.index)):\n",
    "            if (categorized_characteristics[\"assignment\"][j] == None):\n",
    "                continue\n",
    "            if unique_individuals[\"unique_id\"][i] in categorized_characteristics[\"assignment\"][j]:\n",
    "                if (categorized_characteristics[\"category\"][j] == \"origin\") or (categorized_characteristics[\"category\"][j] == \"age\") or (categorized_characteristics[\"category\"][j] == \"legitimacy\") or (categorized_characteristics[\"category\"][j] == \"status\"):\n",
    "                    characteristics[categorized_characteristics[\"category\"][j]] = categorized_characteristics[\"pred_entity\"][j]\n",
    "                else:\n",
    "                    characteristics[categorized_characteristics[\"category\"][j]].append(categorized_characteristics[\"pred_entity\"][j])\n",
    "        \n",
    "        person_record = {\"id\": unique_individuals[\"unique_id\"][i], \"name\": unique_individuals[\"pred_entity\"][i]}\n",
    "        \n",
    "        for key in characteristics:\n",
    "            if ((key==\"ethnicities\") or (key == \"occupation\") or (key == \"phenotype\")) and (len(characteristics[key]) > 0):\n",
    "                person_record[key] = characteristics[key][0]\n",
    "                if (len(characteristics[key]) > 1):\n",
    "                    for char in range(1,len(characteristics[key])):\n",
    "                        person_record[key] += ';' + characteristics[key][char]\n",
    "            elif (characteristics[key] != None) and (characteristics[key] != []):\n",
    "                person_record[key] = characteristics[key]\n",
    "            else:\n",
    "                person_record[key] = None\n",
    "        \n",
    "        people.append(person_record)\n",
    "    \n",
    "    return people, categorized_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b1c5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def id_unique_individuals(entry_text, entities, volume_metadata):\n",
    "    '''\n",
    "    identifies all unique individuals that appear in an entry (i.e. removing all multiple mentions of the same person)\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "        entities: entities of all kinds extracted from that entry by an NER model        \n",
    "        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata\n",
    "        \n",
    "        returns: a list of the unique individuals who appear in an entry AND (temporary?) unique IDs for each individual\n",
    "    '''\n",
    "    event_id = volume_metadata[\"id\"] + '-' + entities.iloc[0]['entry_no']\n",
    "    \n",
    "    people_df = entities.loc[entities['pred_label'] == 'PER']\n",
    "    people_df.reset_index(inplace=True)\n",
    "    people_df = people_df.drop('index',axis=1)\n",
    "    \n",
    "    unique_individuals = people_df['pred_entity'].unique()\n",
    "    unique_individuals = np.vstack([unique_individuals, [None] * len(unique_individuals)])    \n",
    "    \n",
    "    for i in range(len(unique_individuals[0])):        \n",
    "        unique_individuals[1][i] = event_id + '-P' + str(i + 1)        \n",
    "    \n",
    "    return unique_individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a18cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def build_event(entry_text, entities, event_type, principals, volume_metadata, n_event_within_entry, unique_individuals):\n",
    "    '''\n",
    "    builds out relationships related to a baptism or burial event\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "        entities: entities of all kinds extracted from that entry by an NER model\n",
    "        event_type: this could be either a valid record_type OR a secondary event like a birth\n",
    "        principals: the principal(s) of the event, as indicated by determine_principals\n",
    "        volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata\n",
    "        unique_individuals: as determined by id_unique_individuals and/or meta-function of disambig pipeline\n",
    "        \n",
    "        n_event_within_entry: event number within entry\n",
    "        \n",
    "        returns: structured representation of these relationships, including (but not necessarily limited to)\n",
    "        the event's principal, the date of the event, the location of the event, and the associated cleric\n",
    "    '''   \n",
    "    event_id = volume_metadata[\"id\"] + '-' + entities.iloc[0]['entry_no'] + '-E' + str(n_event_within_entry)    \n",
    "    #it's possible that this function should also be returning an event iterator,\n",
    "    #but for now I'm planning to do that in build_relationships\n",
    "    \n",
    "    if event_type == \"baptism\":\n",
    "        if principals != None:           \n",
    "            principal = principals[0]\n",
    "        else:\n",
    "            principal = None\n",
    "        date = determine_event_date(entry_text, entities, event_type, volume_metadata)\n",
    "        location = determine_event_location(entry_text, entities, event_type, volume_metadata)\n",
    "        cleric = identify_cleric(entry_text, entities)\n",
    "        \n",
    "        found_principal_id = False\n",
    "        found_cleric_id = False\n",
    "        for index, entity in unique_individuals.iterrows():\n",
    "            if entity['pred_entity'] == principal:\n",
    "                principal = entity['unique_id']\n",
    "                found_principal_id = True\n",
    "                continue\n",
    "            elif entity['pred_entity'] == cleric:\n",
    "                cleric = entity['unique_id']\n",
    "                found_cleric_id = True                \n",
    "        \n",
    "        if (principal != None) and (found_principal_id == False):\n",
    "            principal = None\n",
    "        if (cleric != None) and (found_cleric_id == False):\n",
    "            cleric = None\n",
    "    \n",
    "    elif event_type == \"birth\":\n",
    "        if principals != None:           \n",
    "            principal = principals[0]\n",
    "        else:\n",
    "            principal = None\n",
    "        date = determine_event_date(entry_text, entities, event_type, volume_metadata)\n",
    "        location = determine_event_location(entry_text, entities, event_type, volume_metadata)\n",
    "        cleric = None\n",
    "        \n",
    "        found_principal_id = False        \n",
    "        for index, entity in unique_individuals.iterrows():\n",
    "            if entity['pred_entity'] == principal:\n",
    "                principal = entity['unique_id']\n",
    "                found_principal_id = True\n",
    "                break                \n",
    "        \n",
    "        if (principal != None) and (found_principal_id == False):\n",
    "            principal = None\n",
    "        \n",
    "    else:\n",
    "        print(\"That event type can't be built yet.\")\n",
    "        return\n",
    "    \n",
    "    event_relationships = {\"id\": event_id, \"type\": event_type, \"principal\": principal, \"date\": date, \"location\": location, \"cleric\": cleric}\n",
    "        \n",
    "    return event_relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44675ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def drop_obvious_duplicates(people, principals, cleric):\n",
    "    '''\n",
    "    first-pass disambiguation that drops multiple mentions of cleric and principal(s)\n",
    "        people: df containing all entities labeled as people in the entry\n",
    "        principals: as indicated by determine_principals\n",
    "        \n",
    "        returns: people df with obvious duplicates dropped\n",
    "    '''   \n",
    "    found_principal = False\n",
    "    found_cleric = False       \n",
    "    \n",
    "    if len(principals) == 1:\n",
    "        for index, person in people.iterrows():\n",
    "            if (person['pred_entity'] == principals[0]) and (found_principal == False):\n",
    "                found_principal = True\n",
    "            elif person['pred_entity'] == principals[0]:                \n",
    "                people.drop(index, inplace=True)\n",
    "                \n",
    "            if cleric != None:\n",
    "                if (person['pred_entity'] == cleric) and (found_cleric == False):\n",
    "                    found_cleric = True\n",
    "                elif person['pred_entity'] == cleric:                \n",
    "                    people.drop(index, inplace=True)\n",
    "   \n",
    "    people.reset_index(inplace=True)\n",
    "    \n",
    "    return people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af0361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb3257d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da609bba",
   "metadata": {},
   "source": [
    "## NB 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82b1897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_test\n",
    "\n",
    "path_to_transcription, path_to_model = \"transcriptions\\\\15834.xml\", \"models/15834\"\n",
    "\n",
    "#def process_volume(path_to_transcription, path_to_model):\n",
    "#retrieve volume metadata and controlled vocabularies\n",
    "volume_metadata = retrieve_volume_metadata(path_to_transcription)\n",
    "images = xml_v2_to_json(path_to_transcription)\n",
    "vocabularies = retrieve_controlled_vocabularies()\n",
    "\n",
    "if volume_metadata[\"country\"] == \"Brazil\":\n",
    "    lang = \"pt\"\n",
    "    language = \"portuguese\"\n",
    "else:\n",
    "    lang = \"es\"\n",
    "    language = \"spanish\"\n",
    "\n",
    "#load and apply trained model\n",
    "\n",
    "trained_model = load_model(path_to_model, language=lang, verbose='True')\n",
    "\n",
    "entry_df = parse_xml_v2(path_to_transcription)\n",
    "\n",
    "ent_preds_df, metrics_df, per_ent_metrics = test_model(trained_model, entry_df, \"entry_no\", \"text\", score_model=False)\n",
    "print(\"Entities extracted.\")\n",
    "\n",
    "#development\n",
    "#pd.set_option(\"display.max_rows\", 101)\n",
    "#display(ent_preds_df.head(100))\n",
    "\n",
    "#iterate through each entry and build relationships\n",
    "\n",
    "people = []\n",
    "places = []\n",
    "events = []\n",
    "\n",
    "entitiesRunning = pd.DataFrame()\n",
    "noCategoryRunning = pd.DataFrame()\n",
    "\n",
    "validation_dict_ALL = []\n",
    "\n",
    "#file path could be passed as parameter, as could language (eventually)\n",
    "with open(\"names.json\", encoding=\"utf-8\") as infile:\n",
    "    name_file = json.load(infile)\n",
    "\n",
    "names = name_file[\"names\"]\n",
    "all_first_names = []\n",
    "for name in names:\n",
    "    all_first_names.append(name[\"name\"])        \n",
    "\n",
    "for i in range(len(entry_df.index)):\n",
    "\n",
    "    entry_no = entry_df['entry_no'][i]\n",
    "    entry_text = entry_df['text'][i]    \n",
    "\n",
    "    entities = copy.deepcopy(ent_preds_df[ent_preds_df['entry_no'] == entry_no])\n",
    "\n",
    "    entities[\"assigned\"] = True\n",
    "    \n",
    "    #BUILD ENTRY METADATA#################################################################################################################\n",
    "    #entry_people, entry_places, entry_events, entities, characteristics_df, categorized_characteristics, uncategorized_characteristics = build_entry_metadata(entry_text, entities, path_to_transcription, entry_no)             \n",
    "    #def build_entry_metadata(entry_text, entities, path_to_volume_xml, entry_number=None):\n",
    "    '''\n",
    "    applies rules-based engine for relationship linking to the transcription of a single entry\n",
    "        entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity        \n",
    "        entities: entities of all kinds extracted from that entry by an NER model\n",
    "        path_to_volume_xml: path to xml file containing full volume transcription and volume-level metadata\n",
    "        entry_number: entry number, also from spaCy\n",
    "\n",
    "        returns: three lists containing structured data about the people, places, and events that appear in the entry\n",
    "    '''\n",
    "\n",
    "    people = []\n",
    "    places = []\n",
    "    events = []\n",
    "\n",
    "    volume_metadata = retrieve_volume_metadata(path_to_volume_xml)\n",
    "    people_df = copy.deepcopy(entities.loc[entities['pred_label'] == 'PER'])\n",
    "    people_df.reset_index(inplace=True)\n",
    "    people_df, next_id = assign_unique_ids(people_df, volume_metadata, entry_number)\n",
    "    characteristics_df = copy.deepcopy(entities.loc[entities['pred_label'] == 'CHAR'])\n",
    "    characteristics_df.reset_index(inplace=True)\n",
    "    dates_df = copy.deepcopy(entities.loc[entities['pred_label'] == 'DATE'])\n",
    "    dates_df.reset_index(inplace=True)\n",
    "\n",
    "    if volume_metadata[\"type\"] == \"baptism\":\n",
    "        principal = determine_principals(entry_text, entities, 1)\n",
    "\n",
    "        if principal == None:            \n",
    "            people_df, next_id = build_new_person(people_df, next_id, \"principal\")\n",
    "            principal = [\"Unknown principal\"]            \n",
    "\n",
    "        cleric = identify_cleric(entry_text, entities)                 \n",
    "\n",
    "        events.append(build_event(entry_text, entities, \"baptism\", principal, volume_metadata, 1, people_df))\n",
    "    \n",
    "        print(\"Kai disabled the birth build events.  ctrl+f this message to change this\")\n",
    "        #if (len(dates_df.index) > 1):\n",
    "        #    events.append(build_event(entry_text, entities, \"birth\", principal, volume_metadata, 2, people_df))\n",
    "\n",
    "        characteristics_df, uncategorized_characteristics = categorize_characteristics(entities, characteristics_df)\n",
    "        people, categorized_characteristics = assign_characteristics(entry_text, entities, characteristics_df, people_df, volume_metadata)       \n",
    "        \n",
    "        #ALT_ASSIGN_RELATIONSHIPS#################################################################################################################\n",
    "        #def alt_assign_relationships(entry_text, entities, people_df, people, volume_metadata):\n",
    "        '''\n",
    "        matches all labeled relationships to the correct individuals and builds triples\n",
    "            entry_text: the full text of a single entry, ported directly from spaCy to ensure congruity\n",
    "            entities: df containing all entities extracted from that entry by an NER model\n",
    "            people_df: entities given the label \"PER\" from a single entry by an NER model with unique ids\n",
    "            people: list of dictionaries, each of which represents one mention of a person in the entry\n",
    "            (as produced by assign_characteristics)\n",
    "            volume_metadata: metadata for the volume that the entry comes from, built by retrieve_volume_metadata        \n",
    "\n",
    "            returns: updated version of people with interpersonal relationships added\n",
    "        '''\n",
    "\n",
    "        rel_types = retrieve_controlled_vocabularies()[\"relationships\"]\n",
    "        relationships = copy.deepcopy(entities.loc[entities['pred_label'] == 'REL'])\n",
    "        relationships.reset_index(inplace=True)\n",
    "        characteristics = copy.deepcopy(entities.loc[entities['pred_label'] == 'CHAR'])\n",
    "        characteristics.reset_index(inplace=True)    \n",
    "        cat_char, uncat_char = categorize_characteristics(entities, characteristics)    \n",
    "        entities.reset_index(inplace=True)  \n",
    "\n",
    "        #if determine_principals(entry_text, entities, 1) != None:\n",
    "        if not (determine_principals(entry_text, entities, 1) == None):\n",
    "            principal = determine_principals(entry_text, entities, 1)[0]\n",
    "            for i in range(len(people)):\n",
    "                if people[i][\"name\"] == principal:\n",
    "                    principal_id = people[i]['id']                \n",
    "                    break\n",
    "        else:\n",
    "            principal = \"Unknown principal\"\n",
    "            for i in range(len(people)):\n",
    "                if people[i][\"name\"] == principal:\n",
    "                    principal_id = people[i]['id']                \n",
    "                    break\n",
    "\n",
    "        found_parents = False\n",
    "        found_godparents = False\n",
    "        godparents = []\n",
    "        found_paternal_grandparents = False\n",
    "        found_maternal_grandparents = False\n",
    "        found_enslaver = False\n",
    "        enslaver_id = None\n",
    "\n",
    "        #build godparent/godchild relationships    \n",
    "        #future improvement: add logic to look for spousal relationship between godparents\n",
    "        if (len(entities) != 0) and (len(relationships) != 0):        \n",
    "            for index in range(len(entities)):\n",
    "                if entities['pred_label'][index] == \"REL\":                \n",
    "                    if ((entities['pred_entity'][index].lower() == \"madrina\") or (entities['pred_entity'][index].lower() == \"padrino\") or (entities['pred_entity'][index].lower() == \"padryno\")) and (found_godparents == False):                    \n",
    "                        if (len(entities) > (index + 1)) and (entities['pred_label'][index + 1] == \"PER\"):\n",
    "                            for j in range(len(people_df)):\n",
    "                                if people_df['pred_start'][j] == entities['pred_start'][index + 1]:\n",
    "                                    from_person = people_df['unique_id'][j]\n",
    "                            people = build_reciprocal_relationship(people, from_person, principal_id, \"godparent\")                        \n",
    "                            found_godparents = True\n",
    "                            godparent = {\"name\": people_df[\"pred_entity\"][j], \"id\": people_df[\"unique_id\"][j]}\n",
    "                            godparents.append(godparent)\n",
    "                    elif ((entities['pred_entity'][index].lower() == \"padrinos\") or (entities['pred_entity'][index].lower() == \"p.p.\")) and (found_godparents == False):\n",
    "                        if (len(entities) > (index + 1)) and (entities['pred_label'][index + 1] == \"PER\"):\n",
    "                            for j in range(len(people_df)):\n",
    "                                if people_df['pred_start'][j] == entities['pred_start'][index + 1]:\n",
    "                                    from_person = people_df['unique_id'][j]\n",
    "                            people = build_reciprocal_relationship(people, from_person, principal_id, \"godparent\")\n",
    "                            found_godparents = True\n",
    "                            godparent = {\"name\": people_df[\"pred_entity\"][j], \"id\": people_df[\"unique_id\"][j]}\n",
    "                            godparents.append(godparent)\n",
    "                        if (len(entities) > (index + 2)) and (entities['pred_label'][index + 2] == \"PER\"):\n",
    "                            for j in range(len(people_df)):\n",
    "                                if people_df['pred_start'][j] == entities['pred_start'][index + 2]:\n",
    "                                    from_person = people_df['unique_id'][j]\n",
    "                            people = build_reciprocal_relationship(people, from_person, principal_id, \"godparent\")\n",
    "                            found_godparents = True\n",
    "                            godparent = {\"name\": people_df[\"pred_entity\"][j], \"id\": people_df[\"unique_id\"][j]}\n",
    "                            godparents.append(godparent)\n",
    "                    elif (\"p.\" in entities['pred_entity'][index].lower()) and (found_godparents == False):\n",
    "                        if (len(entities) > (index + 1)) and not (\"p.\" in entities['pred_entity'][index + 1].lower()):                        \n",
    "                            if (len(entities) > (index + 1)) and (entities['pred_label'][index + 1] == \"PER\"):\n",
    "                                for j in range(len(people_df)):\n",
    "                                    if people_df['pred_start'][j] == entities['pred_start'][index + 1]:\n",
    "                                        from_person = people_df['unique_id'][j]\n",
    "                                people = build_reciprocal_relationship(people, from_person, principal_id, \"godparent\")\n",
    "                                found_godparents = True\n",
    "                                godparent = {\"name\": people_df[\"pred_entity\"][j], \"id\": people_df[\"unique_id\"][j]}\n",
    "                                godparents.append(godparent)\n",
    "                        elif (len(entities) > (index + 1)):\n",
    "                            if (len(entities) > (index + 2)) and (entities['pred_label'][index + 2] == \"PER\"):\n",
    "                                for j in range(len(people_df)):\n",
    "                                    if people_df['pred_start'][j] == entities['pred_start'][index + 2]:\n",
    "                                        from_person = people_df['unique_id'][j]\n",
    "                                people = build_reciprocal_relationship(people, from_person, principal_id, \"godparent\")\n",
    "                                found_godparents = True\n",
    "                                godparent = {\"name\": people_df[\"pred_entity\"][j], \"id\": people_df[\"unique_id\"][j]}\n",
    "                                godparents.append(godparent)\n",
    "                            if (len(entities) > (index + 3)) and (entities['pred_label'][index + 3] == \"PER\"):\n",
    "                                for j in range(len(people_df)):\n",
    "                                    if people_df['pred_start'][j] == entities['pred_start'][index + 3]:\n",
    "                                        from_person = people_df['unique_id'][j]\n",
    "                                people = build_reciprocal_relationship(people, from_person, principal_id, \"godparent\")\n",
    "                                found_godparents = True\n",
    "                                godparent = {\"name\": people_df[\"pred_entity\"][j], \"id\": people_df[\"unique_id\"][j]}\n",
    "                                godparents.append(godparent)\n",
    "                    #build grandparents\n",
    "                    elif (\"abuelos\" in entities[\"pred_entity\"][index].lower()):                    \n",
    "                        if (\"paternos\" in entities[\"pred_entity\"][index].lower()) and (found_paternal_grandparents == False):                        \n",
    "                            paternal_grandfather = ''\n",
    "                            paternal_grandmother = ''\n",
    "                            if entities[\"pred_label\"][index + 1] == \"PER\":\n",
    "                                for j in range(len(people_df)):\n",
    "                                    if people_df['pred_start'][j] == entities['pred_start'][index + 1]:\n",
    "                                        grandparent_id = people_df['unique_id'][j]\n",
    "                                        break\n",
    "                                if determine_sex(entities[\"pred_entity\"][index + 1].split(' ')[0], name_list=\"names.json\") == \"male\":\n",
    "                                    paternal_grandfather = grandparent_id\n",
    "                                    paternal_grandfather_index = j\n",
    "                                elif determine_sex(entities[\"pred_entity\"][index + 1].split(' ')[0], name_list=\"names.json\") == \"female\":\n",
    "                                    paternal_grandmother = grandparent_id\n",
    "                                    paternal_grandmother_index = j\n",
    "                                else:\n",
    "                                    paternal_grandmother = grandparent_id\n",
    "                                    paternal_grandmother_index = j\n",
    "                                if entities[\"pred_label\"][index + 2] == \"PER\":\n",
    "                                    for j in range(len(people_df)):\n",
    "                                        if people_df['pred_start'][j] == entities['pred_start'][index + 2]:\n",
    "                                            grandparent_id = people_df['unique_id'][j]\n",
    "                                            break\n",
    "                                    if (determine_sex(entities[\"pred_entity\"][index + 2].split(' ')[0], name_list=\"names.json\") == \"male\") and (paternal_grandfather == ''):\n",
    "                                        paternal_grandfather = grandparent_id\n",
    "                                        paternal_grandfather_index = j\n",
    "                                    elif (determine_sex(entities[\"pred_entity\"][index + 2].split(' ')[0], name_list=\"names.json\") == \"female\") and (paternal_grandmother == ''):\n",
    "                                        paternal_grandmother = grandparent_id\n",
    "                                        paternal_grandmother_index = j\n",
    "                                    elif paternal_grandmother == '':\n",
    "                                        paternal_grandmother = grandparent_id\n",
    "                                        paternal_grandmother_index = j\n",
    "                                    else:\n",
    "                                        paternal_grandfather = grandparent_id\n",
    "                                        paternal_grandfather_index = j\n",
    "                            if paternal_grandfather != '':\n",
    "                                found_paternal_grandparents = True\n",
    "                                people = build_reciprocal_relationship(people, paternal_grandfather, principal_id, \"grandparent\")\n",
    "                            if paternal_grandmother != '':\n",
    "                                found_paternal_grandparents = True\n",
    "                                people = build_reciprocal_relationship(people, paternal_grandmother, principal_id, \"grandparent\")\n",
    "                            if (paternal_grandfather != '') and (paternal_grandmother != ''):\n",
    "                                people = build_reciprocal_relationship(people, paternal_grandfather, paternal_grandmother, \"spouse\")\n",
    "                        elif (\"matern\" in entities[\"pred_entity\"][index].lower()) and (found_maternal_grandparents == False):                        \n",
    "                            maternal_grandfather = ''\n",
    "                            maternal_grandmother = ''\n",
    "                            if entities[\"pred_label\"][index + 1] == \"PER\":\n",
    "                                for j in range(len(people_df)):\n",
    "                                    if people_df['pred_start'][j] == entities['pred_start'][index + 1]:\n",
    "                                        grandparent_id = people_df['unique_id'][j]\n",
    "                                        break\n",
    "                                if determine_sex(entities[\"pred_entity\"][index + 1].split(' ')[0], name_list=\"names.json\") == \"male\":\n",
    "                                    maternal_grandfather = grandparent_id\n",
    "                                    maternal_grandfather_index = j\n",
    "                                elif determine_sex(entities[\"pred_entity\"][index + 1].split(' ')[0], name_list=\"names.json\") == \"female\":\n",
    "                                    maternal_grandmother = grandparent_id\n",
    "                                    maternal_grandmother_index = j\n",
    "                                else:\n",
    "                                    maternal_grandmother = grandparent_id\n",
    "                                    maternal_grandmother_index = j\n",
    "                                if entities[\"pred_label\"][index + 2] == \"PER\":\n",
    "                                    for j in range(len(people_df)):\n",
    "                                        if people_df['pred_start'][j] == entities['pred_start'][index + 2]:\n",
    "                                            grandparent_id = people_df['unique_id'][j]\n",
    "                                            break\n",
    "                                    if (determine_sex(entities[\"pred_entity\"][index + 2].split(' ')[0], name_list=\"names.json\") == \"male\") and (maternal_grandfather == ''):\n",
    "                                        maternal_grandfather = grandparent_id\n",
    "                                        maternal_grandfather_index = j\n",
    "                                    elif (determine_sex(entities[\"pred_entity\"][index + 2].split(' ')[0], name_list=\"names.json\") == \"female\") and (maternal_grandmother == ''):\n",
    "                                        maternal_grandmother = grandparent_id\n",
    "                                        maternal_grandmother_index = j\n",
    "                                    elif maternal_grandmother == '':\n",
    "                                        maternal_grandmother = grandparent_id\n",
    "                                        maternal_grandmother_index = j\n",
    "                                    else:\n",
    "                                        maternal_grandfather = grandparent_id\n",
    "                                        maternal_grandfather_index = j\n",
    "                            if maternal_grandfather != '':\n",
    "                                found_maternal_grandparents = True\n",
    "                                people = build_reciprocal_relationship(people, maternal_grandfather, principal_id, \"grandparent\")\n",
    "                            if maternal_grandmother != '':\n",
    "                                found_maternal_grandparents = True\n",
    "                                people = build_reciprocal_relationship(people, maternal_grandmother, principal_id, \"grandparent\")\n",
    "                            if (maternal_grandfather != '') and (maternal_grandmother != ''):                            \n",
    "                                people = build_reciprocal_relationship(people, maternal_grandfather, maternal_grandmother, \"spouse\")\n",
    "                    elif (\"matern\" in entities[\"pred_entity\"][index].lower()) and found_paternal_grandparents and (found_maternal_grandparents == False):\n",
    "                        maternal_grandfather = ''\n",
    "                        maternal_grandmother = ''\n",
    "                        if entities[\"pred_label\"][index + 1] == \"PER\":\n",
    "                            for j in range(len(people_df)):\n",
    "                                if people_df['pred_start'][j] == entities['pred_start'][index + 1]:\n",
    "                                    grandparent_id = people_df['unique_id'][j]\n",
    "                                    break\n",
    "                            if determine_sex(entities[\"pred_entity\"][index + 1].split(' ')[0], name_list=\"names.json\") == \"male\":\n",
    "                                maternal_grandfather = grandparent_id\n",
    "                                maternal_grandfather_index = j\n",
    "                            elif determine_sex(entities[\"pred_entity\"][index + 1].split(' ')[0], name_list=\"names.json\") == \"female\":\n",
    "                                maternal_grandmother = grandparent_id\n",
    "                                maternal_grandmother_index = j\n",
    "                            else:\n",
    "                                maternal_grandmother = grandparent_id\n",
    "                                maternal_grandmother_index = j\n",
    "                            if entities[\"pred_label\"][index + 2] == \"PER\":\n",
    "                                for j in range(len(people_df)):\n",
    "                                    if people_df['pred_start'][j] == entities['pred_start'][index + 2]:\n",
    "                                        grandparent_id = people_df['unique_id'][j]\n",
    "                                        break\n",
    "                                if (determine_sex(entities[\"pred_entity\"][index + 2].split(' ')[0], name_list=\"names.json\") == \"male\") and (maternal_grandfather == ''):\n",
    "                                    maternal_grandfather = grandparent_id\n",
    "                                    maternal_grandfather_index = j\n",
    "                                elif (determine_sex(entities[\"pred_entity\"][index + 2].split(' ')[0], name_list=\"names.json\") == \"female\") and (maternal_grandmother == ''):\n",
    "                                    maternal_grandmother = grandparent_id\n",
    "                                    maternal_grandmother_index = j\n",
    "                                elif maternal_grandmother == '':\n",
    "                                    maternal_grandmother = grandparent_id\n",
    "                                    maternal_grandmother_index = j\n",
    "                                else:\n",
    "                                    maternal_grandfather = grandparent_id\n",
    "                                    maternal_grandfather_index = j\n",
    "                        if maternal_grandfather != '':\n",
    "                            found_maternal_grandparents = True\n",
    "                            people = build_reciprocal_relationship(people, maternal_grandfather, principal_id, \"grandparent\")\n",
    "                        if maternal_grandmother != '':\n",
    "                            found_maternal_grandparents = True\n",
    "                            people = build_reciprocal_relationship(people, maternal_grandmother, principal_id, \"grandparent\")\n",
    "                        if (maternal_grandfather != '') and (maternal_grandmother != ''):\n",
    "                                people = build_reciprocal_relationship(people, maternal_grandfather, maternal_grandmother, \"spouse\")\n",
    "\n",
    "                    elif ((found_parents == False) and (found_godparents == False) and (found_paternal_grandparents == False) and (found_maternal_grandparents == False) and (found_enslaver == False)):\n",
    "                        #ie if after all these checks, there are still no relationships found, then we have a case where we have a relationship but no assignment\n",
    "                        #Note that this relies on setting ALL to FOUND by default, so I don't have to add to the code above each time\n",
    "                        #Thus, we only flip it in the case that no relationships are found\n",
    "\n",
    "                        entities.loc[index, \"assigned\"] = False\n",
    "                        #print(\"Failed to find a category for relationship: \" + entities[\"pred_entity\"][index])\n",
    "\n",
    "\n",
    "\n",
    "        if len(godparents) == 2:\n",
    "            first_godparent_sex = determine_sex(godparents[0][\"name\"].split(' ')[0], name_list=\"names.json\")\n",
    "            second_godparent_sex = determine_sex(godparents[1][\"name\"].split(' ')[0], name_list=\"names.json\")\n",
    "            #if (first_godparent_sex != second_godparent_sex) or (first_godparent_sex == \"unknown\" and second_godparent_sex == \"unknown\"):\n",
    "                #print(\"found possible godparent couple: \")\n",
    "                #print(godparents[0][\"name\"])\n",
    "                #print(godparents[1][\"name\"])\n",
    "\n",
    "        for i in range(len(cat_char)):\n",
    "            #build enslaver/enslaved person relationships\n",
    "            if cat_char[\"category\"][i] == \"status\":            \n",
    "                #skip if associated with first mention of principal\n",
    "                char_start = cat_char['pred_start'][i]\n",
    "                if char_start <= 25:\n",
    "                    continue            \n",
    "\n",
    "                #match enslaved couple to owner\n",
    "                if (cat_char[\"pred_entity\"][i].lower()[len(cat_char[\"pred_entity\"][i]) - 1] == 's'):\n",
    "                    close_ep = -1\n",
    "                    far_ep = -1\n",
    "                    ens = -1\n",
    "                    for j in range(len(people_df)):\n",
    "                        pers_start = people_df[\"pred_start\"][j]\n",
    "                        poss_diff = char_start - pers_start\n",
    "                        if (ens == -1) and (poss_diff < 0) and (abs(poss_diff) < 25):\n",
    "                            ens = j\n",
    "                        elif (ens != -1) and (poss_diff < 0) and (abs(poss_diff) < abs(char_start - people_df[\"pred_start\"][ens])):\n",
    "                            ens = j\n",
    "                        elif (close_ep == -1) and (poss_diff > 0) and (poss_diff < 50):\n",
    "                            close_ep = j\n",
    "                        elif (close_ep != -1) and (far_ep == -1) and (poss_diff > 0) and (poss_diff < char_start - people_df[\"pred_start\"][close_ep]):\n",
    "                            far_ep = close_ep\n",
    "                            close_ep = j\n",
    "                        elif (close_ep != -1) and (far_ep == -1) and (poss_diff > 0) and (poss_diff < 50):\n",
    "                            far_ep = j\n",
    "                        elif (close_ep != -1) and (far_ep != -1) and (poss_diff > 0) and (poss_diff < char_start - people_df[\"pred_start\"][close_ep]):\n",
    "                            far_ep = close_ep\n",
    "                            close_ep = j\n",
    "                        elif (close_ep != -1) and (far_ep != -1) and (poss_diff > 0) and (poss_diff < char_start - people_df[\"pred_start\"][far_ep]):\n",
    "                            far_ep = j\n",
    "                    if (ens != -1) and (close_ep != -1) and (far_ep != -1):\n",
    "                        people = build_reciprocal_relationship(people, people_df[\"unique_id\"][ens], people_df[\"unique_id\"][close_ep], \"enslaver\")\n",
    "                        people = build_reciprocal_relationship(people, people_df[\"unique_id\"][ens], people_df[\"unique_id\"][far_ep], \"enslaver\")\n",
    "                    elif (ens != -1) and (close_ep != -1):\n",
    "                        people = build_reciprocal_relationship(people, people_df[\"unique_id\"][ens], people_df[\"unique_id\"][close_ep], \"enslaver\")\n",
    "                #match enslaved person to owner        \n",
    "                elif \"propiedad\" in cat_char[\"pred_entity\"][i].lower():\n",
    "                    for j in range(len(entities)):\n",
    "                        if entities[\"pred_start\"][j] == cat_char[\"pred_start\"][i]:\n",
    "                            signal_entity_index = j\n",
    "                            break                \n",
    "                    if found_enslaver and (entry_text.rfind(\"misma\", cat_char[\"pred_start\"][i] - 25, cat_char[\"pred_start\"][i]) != -1):\n",
    "                        if (entities[\"pred_label\"][signal_entity_index - 1] == \"PER\") and (cat_char[\"pred_start\"][i] - entities[\"pred_end\"][signal_entity_index - 1] <= 20):\n",
    "                            people = build_reciprocal_relationship(people, enslaver_id, entities[\"unique_id\"][signal_entity_index - 1], \"enslaver\")\n",
    "                            if (entities[\"pred_label\"][signal_entity_index - 2] == \"PER\") and (entities[\"pred_end\"][signal_entity_index - 2] - entities[\"pred_start\"][signal_entity_index - 1] <= 5):\n",
    "                                people = build_reciprocal_relationship(people, enslaver_id, entities[\"unique_id\"][signal_entity_index - 2], \"enslaver\")\n",
    "                    elif (entities[\"pred_label\"][signal_entity_index + 1] == \"PER\") and ((entities[\"pred_start\"][signal_entity_index + 1] - cat_char[\"pred_start\"][i]) <= 25):\n",
    "                        for j in range(len(people_df)):\n",
    "                            if people_df['pred_start'][j] == entities['pred_start'][signal_entity_index + 1]:\n",
    "                                found_enslaver = True\n",
    "                                enslaver_id = people_df[\"unique_id\"][j]\n",
    "                                people = build_reciprocal_relationship(people, enslaver_id, principal_id, \"enslaver\")\n",
    "                                break               \n",
    "                else:\n",
    "                    ep = -1\n",
    "                    ens = -1\n",
    "                    for k in range(len(people_df)):\n",
    "                        pers_start = people_df[\"pred_start\"][k]\n",
    "                        poss_diff = char_start - pers_start\n",
    "                        if (ep == -1) and (poss_diff > 0) and (poss_diff < 50):\n",
    "                            ep = k                        \n",
    "                        elif (ens == -1) and (poss_diff < 0) and (abs(poss_diff) < 25):\n",
    "                            ens = k                        \n",
    "                        elif (ep != -1) and (poss_diff > 0) and (poss_diff < char_start - people_df[\"pred_start\"][ep]):\n",
    "                            ep = k\n",
    "                        elif (ens != -1) and (poss_diff < 0) and (abs(poss_diff) < abs(char_start - people_df[\"pred_start\"][ens])):\n",
    "                            ens = k\n",
    "                    if (ep != -1) and (ens != -1):\n",
    "                        people = build_reciprocal_relationship(people, people_df[\"unique_id\"][ens], people_df[\"unique_id\"][ep], \"enslaver\")\n",
    "            #build parent/child relationships\n",
    "            elif (((cat_char[\"category\"][i] == \"relationships\") and ((cat_char[\"pred_entity\"][i] == \"hijo\") or (cat_char[\"pred_entity\"][i] == \"hija\") or (cat_char[\"pred_entity\"][i] == \"h\") or (cat_char[\"pred_entity\"][i] == \"h.\"))) or (cat_char[\"category\"][i] == \"legitimacy\")) and (found_parents == False):\n",
    "                rel_start = cat_char[\"pred_start\"][i]\n",
    "                close_parent = -1\n",
    "                far_parent = -1\n",
    "                for l in range(len(people_df)):\n",
    "                    pers_start = people_df[\"pred_start\"][l]\n",
    "                    poss_diff = rel_start - pers_start\n",
    "                    if (close_parent == -1) and (poss_diff < 0) and (abs(poss_diff) < 25):\n",
    "                        close_parent = l\n",
    "                    elif (close_parent != -1) and (far_parent == -1) and (poss_diff < 0) and (abs(poss_diff) < abs(rel_start - people_df[\"pred_start\"][close_parent])):\n",
    "                        far_parent = close_parent\n",
    "                        close_parent = l\n",
    "                    elif (close_parent != -1) and (far_parent == -1) and (poss_diff < 0) and ((pers_start - people_df[\"pred_end\"][close_parent]) < 10):\n",
    "                        far_parent = l\n",
    "                    elif (close_parent != -1) and (far_parent != -1) and (poss_diff > 0) and (abs(poss_diff) < abs(rel_start - people_df[\"pred_start\"][close_parent])):\n",
    "                        far_parent = close_parent\n",
    "                        close_parent = l\n",
    "                    elif (close_parent != -1) and (far_parent != -1) and (poss_diff > 0) and (abs(poss_diff) < abs(rel_start - people_df[\"pred_start\"][far_parent])):\n",
    "                        far_parent = l\n",
    "                if (close_parent != -1) and (far_parent != -1):\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][close_parent], principal_id, \"parent\")\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][far_parent], principal_id, \"parent\")\n",
    "                    if ((cat_char[\"category\"][i] == \"legitimacy\") and ('r' not in cat_char[\"pred_entity\"][i])) or ((cat_char[\"category\"][i] == \"relationships\") and ((cat_char['pred_entity'][i] == 'h') or (cat_char['pred_entity'][i] == 'h.')) and ((entry_text[cat_char[\"pred_end\"][i]] == 'l') or (entry_text[cat_char[\"pred_end\"][i] + 1] == 'l'))):\n",
    "                        people = build_reciprocal_relationship(people, people_df[\"unique_id\"][close_parent], people_df[\"unique_id\"][far_parent], \"spouse\")\n",
    "                    #future improvement (after normalization) if both parents enslaved and child not free, make sure child's status is enslaved\n",
    "                    #future improvement (after normalization) if child is enslaved, make sure reciprocal enslaver/enslaved person relationship exists with mother's enslaver                \n",
    "                    found_parents = True\n",
    "                elif (close_parent != -1):\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][close_parent], principal_id, \"parent\")\n",
    "                    #future improvement (after normalization) if single parent is mother and she is enslaved and child not free, make sure child's status is enslaved\n",
    "                    #future improvement (after normalization) if child is enslaved, make sure reciprocal enslaver/enslaved person relationship exists with mother's enslaver\n",
    "                    found_parents = True\n",
    "\n",
    "        #build parent-child relationships between parents and grandparents\n",
    "        if found_parents and found_paternal_grandparents:        \n",
    "            if (far_parent != -1) and (determine_sex(people_df[\"pred_entity\"][far_parent].split(' ')[0]) == \"male\"):\n",
    "                if (paternal_grandmother != '') and (paternal_grandfather != ''):\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandmother_index], people_df[\"unique_id\"][far_parent], \"parent\")\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandfather_index], people_df[\"unique_id\"][far_parent], \"parent\")\n",
    "                elif paternal_grandmother != '':\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandmother_index], people_df[\"unique_id\"][far_parent], \"parent\")        \n",
    "                else:                \n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandfather_index], people_df[\"unique_id\"][far_parent], \"parent\")\n",
    "            elif (close_parent != -1) and (determine_sex(people_df[\"pred_entity\"][close_parent].split(' ')[0]) == \"male\"):\n",
    "                if (paternal_grandmother != '') and (paternal_grandfather != ''):\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandmother_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandfather_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "                elif paternal_grandmother != '':\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandmother_index], people_df[\"unique_id\"][close_parent], \"parent\")        \n",
    "                else:                \n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][paternal_grandfather_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "        if found_parents and found_maternal_grandparents:\n",
    "            if (close_parent != -1)  and (determine_sex(people_df[\"pred_entity\"][close_parent].split(' ')[0]) == \"female\"):\n",
    "                if (maternal_grandmother != '') and (maternal_grandfather != ''):\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandmother_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandfather_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "                elif maternal_grandmother != '':\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandmother_index], people_df[\"unique_id\"][close_parent], \"parent\")        \n",
    "                else:                \n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandfather_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "            elif (far_parent != -1) and (determine_sex(people_df[\"pred_entity\"][far_parent].split(' ')[0]) == \"female\"):\n",
    "                if (maternal_grandmother != '') and (maternal_grandfather != ''):\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandmother_index], people_df[\"unique_id\"][far_parent], \"parent\")\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandfather_index], people_df[\"unique_id\"][far_parent], \"parent\")\n",
    "                elif maternal_grandmother != '':\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandmother_index], people_df[\"unique_id\"][far_parent], \"parent\")        \n",
    "                else:                \n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandfather_index], people_df[\"unique_id\"][far_parent], \"parent\")\n",
    "            elif (close_parent != -1) and (far_parent == -1):\n",
    "                if (maternal_grandmother != '') and (maternal_grandfather != ''):\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandmother_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandfather_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "                elif maternal_grandmother != '':\n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandmother_index], people_df[\"unique_id\"][close_parent], \"parent\")        \n",
    "                else:                \n",
    "                    people = build_reciprocal_relationship(people, people_df[\"unique_id\"][maternal_grandfather_index], people_df[\"unique_id\"][close_parent], \"parent\")\n",
    "\n",
    "        #return people, entities\n",
    "        #ALT_ASSIGN_RELATIONSHIPS#################################################################################################################\n",
    "        \n",
    "        ############## THIS IS WHERE THE OBVIOUS DUPLICATES MERGE HAPPENS\n",
    "        obvious_duplicates = id_obvious_duplicates(people_df, principal, cleric)       \n",
    "        people = merge_duplicates(people, obvious_duplicates)\n",
    "        #############\n",
    "\n",
    "        #perform more sophisticated disambiguation\n",
    "\n",
    "        for event in events:\n",
    "            if (event[\"location\"] != None) and (not (event[\"location\"] in places)):\n",
    "                places.append(event[\"location\"])\n",
    "\n",
    "    elif volume_metadata[\"type\"] == \"marriage\":        \n",
    "        #process marriage record\n",
    "        print(\"That record type is not supported yet.\")\n",
    "        return None\n",
    "    elif volume_metadata[\"type\"] == \"burial\":\n",
    "        #process burial record\n",
    "        print(\"That record type is not supported yet.\")\n",
    "        return None\n",
    "    else:\n",
    "        print(\"That record type is not supported yet.\")\n",
    "        return None    \n",
    "    #return people, places, events, entities, characteristics_df, categorized_characteristics, uncategorized_characteristics\n",
    "    #BUILD ENTRY METADATA#################################################################################################################\n",
    "    \n",
    "    if uncategorized_characteristics.shape[0] > 0:\n",
    "        noCategoryRunning = noCategoryRunning.append(uncategorized_characteristics)\n",
    "\n",
    "    #FIND ENTITIES THAT ARE UNASSIGNED OR UNCATEGORIZED\n",
    "    entity_index = 0\n",
    "    for ent_data in entities.itertuples():\n",
    "        for char_data in characteristics_df.itertuples():\n",
    "            char_index = 0\n",
    "            #characteristic is not categorized:\n",
    "            if (char_data.category == None) and (ent_data.pred_start == char_data.pred_start) and (ent_data.pred_entity == char_data.pred_entity):\n",
    "                continue #Already dealth with\n",
    "            #characteristic is categorized but not assigned\n",
    "            elif (ent_data.pred_label == char_data.pred_label) and (ent_data.pred_start == char_data.pred_start) and (ent_data.pred_entity == char_data.pred_entity):\n",
    "                if (char_data.assignment == None):\n",
    "                    entities.at[entity_index, \"assigned\"] = False\n",
    "            char_index += 1\n",
    "        entity_index += 1\n",
    "\n",
    "    entitiesRunning = entitiesRunning.append(entities)  \n",
    "\n",
    "    verbosity = 0\n",
    "\n",
    "    entry_validation_dict = validate_entry(entities, entry_people, entry_places, entry_events, uncategorized_characteristics, all_first_names, isVerbose=verbosity)\n",
    "    validation_dict_ALL.append(entry_validation_dict)\n",
    "\n",
    "    people += entry_people\n",
    "    places += entry_places\n",
    "    events += entry_events\n",
    "\n",
    "noCategoryRunning.reset_index(drop = True, inplace = True)\n",
    "noCategoryRunning[\"assigned\"] = False\n",
    "print(\"Relationships linked.\")\n",
    "\n",
    "#disambiguate locations and assign unique ids\n",
    "\n",
    "unique_places = []\n",
    "for place in places:\n",
    "    if (place != None) and (place not in unique_places):\n",
    "        unique_places.append(place)\n",
    "\n",
    "for person in people:        \n",
    "    if (person[\"origin\"] != None) and (person[\"origin\"] not in unique_places):\n",
    "        unique_places.append(person[\"origin\"])\n",
    "\n",
    "places = []\n",
    "curr_place = 1\n",
    "for unique_place in unique_places:\n",
    "    place_record = {\"id\":volume_metadata[\"id\"] + '-L' + str(curr_place), \"location\":unique_place}\n",
    "    places.append(place_record)\n",
    "    curr_place += 1\n",
    "\n",
    "#incorporate location ids into event metadata and person records\n",
    "\n",
    "for event in events:\n",
    "    location = event[\"location\"]\n",
    "    loc_id = \"unknown\"\n",
    "    if location != None:\n",
    "        for place in places:\n",
    "            if place[\"location\"] == location:\n",
    "                loc_id = place[\"id\"]\n",
    "    if (loc_id == \"unknown\") and (location != None):\n",
    "        print(\"Failed to find location ID for \" + location)\n",
    "        event[\"location\"] = None\n",
    "    else:\n",
    "        event[\"location\"] = loc_id\n",
    "\n",
    "    if event[\"location\"] == \"unknown\":\n",
    "        event[\"location\"] = None\n",
    "\n",
    "for person in people:\n",
    "    if person[\"origin\"] == None:\n",
    "        continue\n",
    "\n",
    "    for place in places:\n",
    "        if place[\"location\"] == person[\"origin\"]:\n",
    "            person[\"origin\"] = place[\"id\"]\n",
    "            break\n",
    "\n",
    "#bracket missing or incomplete event dates\n",
    "\n",
    "incomplete_dates = []\n",
    "last_year = None\n",
    "last_month = None\n",
    "last_day = None\n",
    "\n",
    "for e in range(len(events)):\n",
    "    curr_year = events[e][\"date\"][:4]\n",
    "    curr_month = events[e][\"date\"][5:7]\n",
    "    curr_day = events[e][\"date\"][8:]\n",
    "\n",
    "    #fix incompletely extracted years\n",
    "    if (curr_year != \"????\") and (last_year != None) and (abs(int(curr_year) - int(last_year)) > 1):\n",
    "        if (curr_year[3] == last_year[3]):\n",
    "            curr_year = last_year                \n",
    "        elif (curr_month == \"01\") and (last_month == \"12\"):\n",
    "            curr_year = str(int(last_year) + 1)                \n",
    "        else:\n",
    "            curr_year = last_year\n",
    "        events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day\n",
    "\n",
    "    if (curr_year == \"????\") or (curr_month == \"??\") or (curr_day == \"??\"):\n",
    "        #logic to assign dates for birth events based on associated baptism\n",
    "        if events[e][\"type\"] == \"birth\":\n",
    "            if (events[e][\"id\"][:events[e][\"id\"].find('E')] == events[e - 1][\"id\"][:events[e - 1][\"id\"].find('E')]) and (events[e - 1][\"type\"] == \"baptism\") and ('?' not in events[e - 1][\"date\"]):\n",
    "                    if (curr_month != \"??\") and (curr_day != \"??\"):\n",
    "                        if (curr_month == \"12\") and (last_month == \"01\"):\n",
    "                            curr_year = str(int(last_year) - 1)                                \n",
    "                        elif (30 * int(last_month) + int(last_day) - 30 * int(curr_month) - int(curr_day)) < 21:\n",
    "                            curr_year = last_year\n",
    "                        events[e][\"date\"] = curr_year + '-' + events[e][\"date\"][5:7] + '-' + events[e][\"date\"][8:]\n",
    "                    elif curr_month != \"??\":\n",
    "                        if (curr_month == \"12\"):\n",
    "                            curr_day = \"01\"\n",
    "                            curr_year = str(int(last_year) - 1)\n",
    "                            events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-01-01'\n",
    "                        elif (curr_month == last_month):\n",
    "                            curr_day = \"01\"\n",
    "                            curr_year = last_year\n",
    "                            events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-' + last_month + '-' + last_day\n",
    "                        elif int(curr_month) == (int(last_month) - 1):\n",
    "                            curr_day = \"01\"\n",
    "                            curr_year = last_year\n",
    "                            events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-' + last_month + '-01'                            \n",
    "                    elif curr_day != \"??\":\n",
    "                        if curr_day <= last_day:\n",
    "                            curr_year = last_year\n",
    "                            curr_month = last_month                                \n",
    "                        else:\n",
    "                            if last_month == \"01\":\n",
    "                                curr_month = \"12\"\n",
    "                                curr_year = str(int(last_year) - 1)\n",
    "                            else:\n",
    "                                curr_month = str(int(last_month) - 1)                                    \n",
    "                                if len(curr_month) < 2:\n",
    "                                    curr_month = '0' + curr_month\n",
    "                                curr_year = last_year\n",
    "                        events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day\n",
    "                    else:\n",
    "                        if (last_month == '01') and (int(last_day) < 21):\n",
    "                            curr_year = str(int(last_year) - 1)\n",
    "                            curr_month = \"12\"\n",
    "                            curr_day = str(int(last_day) + 9)                               \n",
    "                        elif int(last_day) < 21:\n",
    "                            curr_year = last_year\n",
    "                            curr_month = str(int(last_month) - 1)\n",
    "                            if len(curr_month) < 2:\n",
    "                                curr_month = '0' + curr_month\n",
    "                            curr_day = str(int(last_day) + 9)\n",
    "                        else:\n",
    "                            curr_year = last_year\n",
    "                            curr_month = last_month\n",
    "                            curr_day = str(int(last_day) - 20)\n",
    "                            if len(curr_day) < 2:\n",
    "                                curr_day = '0' + curr_day\n",
    "                        events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-' + last_month + '-' + last_day\n",
    "\n",
    "        if (curr_year == \"????\") or (curr_month == \"??\") or (curr_day == \"??\"):\n",
    "            incomplete_dates.append(e)\n",
    "    elif last_year == None:\n",
    "        for date in incomplete_dates:\n",
    "            events[date][\"date\"] = complete_date(events[date][\"date\"], None, curr_year + '-' + curr_month + '-' + curr_day)\n",
    "\n",
    "        incomplete_dates = []\n",
    "        last_year = curr_year\n",
    "        last_month = curr_month\n",
    "        last_day = curr_day\n",
    "    elif (compare_dates(int(curr_year), int(curr_month), int(curr_day), int(last_year), int(last_month), int(last_day)) == '>') or (compare_dates(int(curr_year), int(curr_month), int(curr_day), int(last_year), int(last_month), int(last_day)) == '='):\n",
    "        for date in incomplete_dates:\n",
    "            events[date][\"date\"] = complete_date(events[date][\"date\"], last_year + '-' + last_month + '-' + last_day, curr_year + '-' + curr_month + '-' + curr_day)\n",
    "\n",
    "        incomplete_dates = []\n",
    "        last_year = curr_year\n",
    "        last_month = curr_month\n",
    "        last_day = curr_day                    \n",
    "\n",
    "if last_year != None:\n",
    "    for date in incomplete_dates:\n",
    "        events[date][\"date\"] = complete_date(events[date][\"date\"], last_year + '-' + last_month + '-' + last_day, None)\n",
    "\n",
    "#merging any date brackets with equal endpoints\n",
    "for event in events:\n",
    "    interval = event[\"date\"].split('/')\n",
    "    if (len(interval) == 2) and (interval[0] == interval[1]):\n",
    "        event[\"date\"] == interval[0]            \n",
    "\n",
    "print(\"Events configured.\")    \n",
    "\n",
    "for person in people:        \n",
    "    #strip titles and/or ranks from names\n",
    "    if person[\"name\"] != None:\n",
    "        name_parts = person[\"name\"].split(' ')\n",
    "\n",
    "        if len(name_parts) >= 2:\n",
    "            while ((name_parts[0].lower() + ' ' + name_parts[1].lower()) in vocabularies[\"titles\"]) or ((name_parts[0].lower() + ' ' + name_parts[1].lower()) in vocabularies[\"ranks\"]):\n",
    "                if len(name_parts) == 2:\n",
    "                    person[\"name\"] = None\n",
    "                else:\n",
    "                    person[\"name\"] = name_parts[2]\n",
    "                    for i in range(3, len(name_parts)):\n",
    "                        person[\"name\"] += ' ' + name_parts[i]\n",
    "\n",
    "                if (name_parts[0].lower() + ' ' + name_parts[1].lower()) in vocabularies[\"titles\"]:\n",
    "                    if person[\"titles\"] != None:\n",
    "                        person[\"titles\"] += ';' + name_parts[0] + ' ' + name_parts[1]\n",
    "                    else:\n",
    "                        person[\"titles\"] = name_parts[0] + ' ' + name_parts[1]\n",
    "                else:\n",
    "                    if person[\"ranks\"] != None:\n",
    "                        person[\"ranks\"] += ';' + name_parts[0] + ' ' + name_parts[1]\n",
    "                    else:\n",
    "                        person[\"ranks\"] = name_parts[0] + ' ' + name_parts[1]\n",
    "\n",
    "                if person[\"name\"] == None:\n",
    "                    break\n",
    "                name_parts = person[\"name\"].split(' ')\n",
    "                if len(name_parts) < 2:\n",
    "                    break\n",
    "\n",
    "        if person[\"name\"] != None:\n",
    "            while (name_parts[0].lower() in vocabularies[\"titles\"]) or (name_parts[0].lower() in vocabularies[\"ranks\"]):\n",
    "                if len(name_parts) == 1:\n",
    "                    person[\"name\"] = None\n",
    "                else:\n",
    "                    person[\"name\"] = name_parts[1]\n",
    "                    for i in range(2, len(name_parts)):\n",
    "                        person[\"name\"] += ' ' + name_parts[i]\n",
    "\n",
    "                if name_parts[0].lower() in vocabularies[\"titles\"]:\n",
    "                    if person[\"titles\"] != None:\n",
    "                        person[\"titles\"] += ';' + name_parts[0]\n",
    "                    else:\n",
    "                        person[\"titles\"] = name_parts[0]\n",
    "                else:\n",
    "                    if person[\"ranks\"] != None:\n",
    "                        person[\"ranks\"] += ';' + name_parts[0]\n",
    "                    else:\n",
    "                        person[\"ranks\"] = name_parts[0]\n",
    "\n",
    "                if person[\"name\"] == None:\n",
    "                    break\n",
    "                name_parts = person[\"name\"].split(' ')\n",
    "\n",
    "#normalize names and all characteristics\n",
    "names = []\n",
    "name_counts = []\n",
    "ethnonym_vocab = retrieve_json_vocab(\"synonyms.json\", \"ethnonyms\")\n",
    "phenotype_vocab = retrieve_json_vocab(\"synonyms.json\", \"phenotypes\", language=\"spanish\")\n",
    "\n",
    "for person in people:\n",
    "    #normalize characteristics and translate to English\n",
    "    for key in person:\n",
    "        if person[key] == None:\n",
    "            continue\n",
    "        if key == \"name\":\n",
    "            person[key] = normalize_text(person[key], \"synonyms.json\", context=\"name\")\n",
    "            #check extracted name for ethnonyms and/or attributed phenotypes        \n",
    "            if (person[\"name\"] != None) and (person[\"name\"] != normalize_text(person[\"name\"], \"synonyms.json\", context=\"ethnonym\")):\n",
    "                for token in person[\"name\"].split(' '):\n",
    "                    eth_norm = normalize_text(token, \"synonyms.json\", context=\"ethnonym\")\n",
    "                    if token != eth_norm:\n",
    "                        if (person[\"ethnicities\"] == None) or (not (eth_norm in person[\"ethnicities\"])):\n",
    "                            if person[\"ethnicities\"] == None:\n",
    "                                person[\"ethnicities\"] = eth_norm\n",
    "                            else:\n",
    "                                person[\"ethnicities\"] = person[\"ethnicities\"] + ';' + eth_norm\n",
    "                person[\"name\"] = normalize_text(person[\"name\"], \"synonyms.json\", context=\"ethnonym\")\n",
    "            else:\n",
    "                for ethnonym in ethnonym_vocab:\n",
    "                    if ethnonym in person[\"name\"]:\n",
    "                        if person[\"ethnicities\"] == None:\n",
    "                            person[\"ethnicities\"] = ethnonym\n",
    "                        else:\n",
    "                            person[\"ethnicities\"] = person[\"ethnicities\"] + ';' + ethnonym\n",
    "            for phenotype in phenotype_vocab:\n",
    "                if phenotype in normalize_text(person[key], \"synonyms.json\", context=\"characteristic\"):                    \n",
    "                    if person[\"phenotype\"] == None:\n",
    "                        person[\"phenotype\"] = phenotype\n",
    "                    else:\n",
    "                        person[\"phenotype\"] = person[\"phenotype\"] + ';' + phenotype\n",
    "                    if phenotype[-1] == 's':\n",
    "                        for token in person[\"name\"].split(' '):\n",
    "                            if normalize_text(token, \"synonyms.json\", context=\"characteristic\") == phenotype:\n",
    "                                person[\"name\"] = person[\"name\"].replace(' ' + token, '')\n",
    "        elif key == \"ethnicities\":                \n",
    "            if person[key].find(';') == -1:\n",
    "                person[key] = normalize_text(person[key], \"synonyms.json\", context=\"ethnonym\")                    \n",
    "            else:\n",
    "                char_comp = person[key].split(';')\n",
    "                person[key] = \"\"\n",
    "                #strip out duplicate characteristics\n",
    "                for char in char_comp:\n",
    "                    char = normalize_text(char, \"synonyms.json\", context=\"ethnonym\")                       \n",
    "\n",
    "                    if not (char in person[key]):\n",
    "                        if person[key] == \"\":\n",
    "                            person[key] = char\n",
    "                        else:\n",
    "                            person[key] = person[key] + ';' + char\n",
    "        elif (key != \"id\") and (key != \"relationships\"):\n",
    "            if person[key].find(';') == -1:\n",
    "                person[key] = normalize_text(person[key], \"synonyms.json\", context=\"characteristic\")\n",
    "                person[key] = translate_characteristic(person[key], \"synonyms.json\", language)\n",
    "            else:\n",
    "                char_comp = person[key].split(';')\n",
    "                person[key] = \"\"\n",
    "                #strip out duplicate characteristics\n",
    "                for char in char_comp:\n",
    "                    char = normalize_text(char, \"synonyms.json\", context=\"characteristic\")                        \n",
    "                    char = translate_characteristic(char, \"synonyms.json\", language)                        \n",
    "                    if not (char in person[key]):\n",
    "                        if person[key] == \"\":\n",
    "                            person[key] = char\n",
    "                        else:\n",
    "                            person[key] = person[key] + ';' + char           \n",
    "\n",
    "    #future improvement: find additional references for plural characteristics\n",
    "\n",
    "    #count name frequency\n",
    "    if person[\"name\"] != None:\n",
    "        if person[\"name\"] in names:\n",
    "            name_counts[names.index(person['name'])] += 1\n",
    "        else:\n",
    "            names.append(person[\"name\"])\n",
    "            name_counts.append(1)   \n",
    "\n",
    "#disambiguate and merge people across the volume\n",
    "redundant_records = []\n",
    "merged_records = []    \n",
    "for i in range(len(name_counts)):\n",
    "    if (name_counts[i] > .1 * len(images)) and (len(names[i].split(' ')) > 1) and (names[i] != \"Unknown principal\"):\n",
    "        records_to_merge = []            \n",
    "        for j in range(len(people)):\n",
    "            if people[j][\"name\"] == names[i]:\n",
    "                redundant_records.append(people[j])\n",
    "                records_to_merge.append(people[j])                    \n",
    "        merged_records.append(merge_records(records_to_merge))            \n",
    "people = [person for person in people if person not in redundant_records]\n",
    "for person in merged_records:\n",
    "    people.append(person)    \n",
    "\n",
    "print(\"People records enhanced and disambiguated.\")\n",
    "\n",
    "#reduce compound person IDs to single ID, add references field\n",
    "people, events = compact_references(people, events)\n",
    "\n",
    "print(\"Single ID generated for each individual.\")\n",
    "\n",
    "#convert dictionaries into JSON    \n",
    "with open(\"volume_records\\\\\" + volume_metadata[\"id\"] + \".json\", \"w\") as outfile:\n",
    "    outfile.write('{\\n\\\"volume\\\": \\n')\n",
    "    json.dump(volume_metadata, outfile)\n",
    "    outfile.write(',')\n",
    "    outfile.write('\\n\\\"images\\\": [\\n')\n",
    "    first_img = True\n",
    "    for image in images:\n",
    "        if first_img:\n",
    "            first_img = False\n",
    "        else:\n",
    "            outfile.write(\",\\n\")\n",
    "        json.dump(image, outfile)\n",
    "    outfile.write(\"\\n],\\n\")\n",
    "    outfile.write('\\n\\\"people\\\": [\\n')\n",
    "    first_person = True\n",
    "    for person in people:\n",
    "        if first_person:\n",
    "            first_person = False\n",
    "        else:\n",
    "            outfile.write(\",\\n\")            \n",
    "        json.dump(person, outfile)            \n",
    "    outfile.write(\"\\n],\\n\")\n",
    "    outfile.write(\"\\\"places\\\": [\\n\")\n",
    "    first_place = True\n",
    "    for place in places:\n",
    "        if first_place:\n",
    "            first_place = False\n",
    "        else:\n",
    "            outfile.write(\",\\n\")\n",
    "        json.dump(place, outfile)\n",
    "    outfile.write(\"\\n],\\n\")\n",
    "    outfile.write(\"\\\"events\\\": [\\n\")\n",
    "    first_event = True\n",
    "    for event in events:\n",
    "        if first_event:\n",
    "            first_event = False\n",
    "        else:\n",
    "            outfile.write(\",\\n\")\n",
    "        json.dump(event, outfile)\n",
    "    outfile.write(\"\\n]\\n\")\n",
    "    outfile.write('}')\n",
    "\n",
    "#dump validation dictionaries\n",
    "with open(\"validation\\\\\" + volume_metadata[\"id\"] + \".json\", \"w\") as outfile:\n",
    "    outfile.write('{\\n\\\"entries\\\": [\\n')\n",
    "    first_entry = True\n",
    "    for entry in validation_dict_ALL:\n",
    "        if first_entry:\n",
    "            first_entry = False\n",
    "        else:\n",
    "            outfile.write(\",\\n\")\n",
    "        json.dump(entry, outfile)\n",
    "    outfile.write(\"\\n]\\n\")\n",
    "    outfile.write('}')\n",
    "\n",
    "print(\"JSON built, processing completed.\")\n",
    "\n",
    "people, places, events, json_path, entities, noCategory, validation_list = people, places, events, volume_metadata[\"id\"] + \"_ppe.json\", entitiesRunning, noCategoryRunning, validation_dict_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f580618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d3c2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbb398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b2f069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad59d0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e000eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26984376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335347d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_test\n",
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
