{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp full_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#dependencies\n",
    "\n",
    "#nlp packages\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "#manipulation of tables/arrays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "\n",
    "#internal imports\n",
    "from ssda_nlp.collate import *\n",
    "from ssda_nlp.split_data import *\n",
    "from ssda_nlp.modeling import *\n",
    "from ssda_nlp.model_performance_utils import *\n",
    "from ssda_nlp.xml_parser import *\n",
    "from ssda_nlp.unstructured2markup import *\n",
    "from ssda_nlp.utility import *\n",
    "from ssda_nlp.relationships import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def process_volume(path_to_transcription, path_to_model):\n",
    "    '''\n",
    "    runs the transcription of a single volume (formatted according to SSDA markup 2.0 specs) through the ML entity extraction\n",
    "    and rules-based relationship linking pipelines, then formats resulting data for export into SQL\n",
    "        path_to_transcription: path to an XML file containing the transcription of a single volume\n",
    "        path_to_model: path to a spaCy model trained to extract entities from the proper type of volume\n",
    "    \n",
    "        returns: final people, place, and event dictionaries as well as the\n",
    "        path to a JSON file containing volume metadata as well as people, place, and event records\n",
    "    '''\n",
    "    \n",
    "    #retrieve volume metadata and controlled vocabularies\n",
    "    \n",
    "    volume_metadata = retrieve_volume_metadata(path_to_transcription)\n",
    "    images = xml_v2_to_json(path_to_transcription)\n",
    "    vocabularies = retrieve_controlled_vocabularies()\n",
    "    \n",
    "    if volume_metadata[\"country\"] == \"Brazil\":\n",
    "        lang = \"pt\"\n",
    "    else:\n",
    "        lang = \"es\"\n",
    "        \n",
    "    #load and apply trained model\n",
    "    \n",
    "    trained_model = load_model(path_to_model, language=lang, verbose='True')\n",
    "    \n",
    "    entry_df = parse_xml_v2(path_to_transcription)\n",
    "    \n",
    "    ent_preds_df, metrics_df, per_ent_metrics = test_model(trained_model, entry_df, \"entry_no\", \"text\", score_model=False)\n",
    "    print(\"Entities extracted.\")\n",
    "    \n",
    "    #iterate through each entry and build relationships\n",
    "    \n",
    "    people = []\n",
    "    places = []\n",
    "    events = []\n",
    "    \n",
    "    for i in range(len(entry_df.index)):\n",
    "        \n",
    "        entry_no = entry_df['entry_no'][i]\n",
    "        entry_text = entry_df['text'][i]    \n",
    "    \n",
    "        entities = ent_preds_df.loc[ent_preds_df['entry_no'] == entry_no]      \n",
    "    \n",
    "        entry_people, entry_places, entry_events = build_entry_metadata(entry_text, entities, path_to_transcription, entry_no)\n",
    "        \n",
    "        people += entry_people\n",
    "        places += entry_places\n",
    "        events += entry_events\n",
    "        \n",
    "    print(\"Relationships linked.\")\n",
    "    \n",
    "    #disambiguate locations and assign unique ids\n",
    "    \n",
    "    unique_places = []\n",
    "    for place in places:\n",
    "        if (place != None) and (place not in unique_places):\n",
    "            unique_places.append(place)\n",
    "    \n",
    "    places = []\n",
    "    curr_place = 1\n",
    "    for unique_place in unique_places:\n",
    "        place_record = {\"id\":volume_metadata[\"id\"] + '-L' + str(curr_place), \"location\":unique_place}\n",
    "        places.append(place_record)\n",
    "        curr_place += 1\n",
    "        \n",
    "    #incorporate location ids into event metadata\n",
    "    \n",
    "    for event in events:\n",
    "        location = event[\"location\"]\n",
    "        loc_id = \"unknown\"\n",
    "        if location != None:\n",
    "            for place in places:\n",
    "                if place[\"location\"] == location:\n",
    "                    loc_id = place[\"id\"]\n",
    "        if (loc_id == \"unknown\") and (location != None):\n",
    "            print(\"Failed to find location ID for \" + location)\n",
    "            event[\"location\"] = None\n",
    "        else:\n",
    "            event[\"location\"] = loc_id\n",
    "            \n",
    "    names = []\n",
    "    name_counts = []\n",
    "    for person in people:        \n",
    "        #strip titles and/or ranks from names\n",
    "        if person[\"name\"] != None:\n",
    "            name_parts = person[\"name\"].split(' ')\n",
    "\n",
    "            if len(name_parts) >= 2:\n",
    "                while ((name_parts[0].lower() + ' ' + name_parts[1].lower()) in vocabularies[\"titles\"]) or ((name_parts[0].lower() + ' ' + name_parts[1].lower()) in vocabularies[\"ranks\"]):\n",
    "                    if len(name_parts) == 2:\n",
    "                        person[\"name\"] = None\n",
    "                    else:\n",
    "                        person[\"name\"] = name_parts[2]\n",
    "                        for i in range(3, len(name_parts)):\n",
    "                            person[\"name\"] += ' ' + name_parts[i]\n",
    "\n",
    "                    if (name_parts[0].lower() + ' ' + name_parts[1].lower()) in vocabularies[\"titles\"]:\n",
    "                        if person[\"titles\"] != None:\n",
    "                            person[\"titles\"] += ';' + name_parts[0] + ' ' + name_parts[1]\n",
    "                        else:\n",
    "                            person[\"titles\"] = name_parts[0] + ' ' + name_parts[1]\n",
    "                    else:\n",
    "                        if person[\"ranks\"] != None:\n",
    "                            person[\"ranks\"] += ';' + name_parts[0] + ' ' + name_parts[1]\n",
    "                        else:\n",
    "                            person[\"ranks\"] = name_parts[0] + ' ' + name_parts[1]\n",
    "\n",
    "                    if person[\"name\"] == None:\n",
    "                        break\n",
    "                    name_parts = person[\"name\"].split(' ')\n",
    "                    if len(name_parts) < 2:\n",
    "                        break\n",
    "\n",
    "            if person[\"name\"] != None:\n",
    "                while (name_parts[0].lower() in vocabularies[\"titles\"]) or (name_parts[0].lower() in vocabularies[\"ranks\"]):\n",
    "                    if len(name_parts) == 1:\n",
    "                        person[\"name\"] = None\n",
    "                    else:\n",
    "                        person[\"name\"] = name_parts[1]\n",
    "                        for i in range(2, len(name_parts)):\n",
    "                            person[\"name\"] += ' ' + name_parts[i]\n",
    "\n",
    "                    if name_parts[0].lower() in vocabularies[\"titles\"]:\n",
    "                        if person[\"titles\"] != None:\n",
    "                            person[\"titles\"] += ';' + name_parts[0]\n",
    "                        else:\n",
    "                            person[\"titles\"] = name_parts[0]\n",
    "                    else:\n",
    "                        if person[\"ranks\"] != None:\n",
    "                            person[\"ranks\"] += ';' + name_parts[0]\n",
    "                        else:\n",
    "                            person[\"ranks\"] = name_parts[0]\n",
    "\n",
    "                    if person[\"name\"] == None:\n",
    "                        break\n",
    "                    name_parts = person[\"name\"].split(' ')\n",
    "                    \n",
    "            #expand abbreviations in remaining parts of name\n",
    "            \n",
    "            #count name frequency\n",
    "            if person[\"name\"] in names:\n",
    "                name_counts[names.index(person['name'])] += 1\n",
    "            else:\n",
    "                names.append(person[\"name\"])\n",
    "                name_counts.append(1)\n",
    "        \n",
    "            \n",
    "    #disambiguate and merge people across the volume\n",
    "    redundant_records = []\n",
    "    merged_records = []    \n",
    "    for i in range(len(name_counts)):\n",
    "        if (name_counts[i] > .1 * len(images)) and (len(names[i].split(' ')) > 1) and (names[i] != \"Unknown principal\"):\n",
    "            records_to_merge = []            \n",
    "            for j in range(len(people)):\n",
    "                if people[j][\"name\"] == names[i]:\n",
    "                    redundant_records.append(people[j])\n",
    "                    records_to_merge.append(people[j])                    \n",
    "            merged_records.append(merge_records(records_to_merge))\n",
    "            print(\"Records for \" + names[i] + \" merged.\")   \n",
    "    people = [person for person in people if person not in redundant_records]\n",
    "    for person in merged_records:\n",
    "        people.append(person)    \n",
    "    \n",
    "    print(\"People records enhanced and disambiguated.\")\n",
    "    \n",
    "    #convert dictionaries into JSON\n",
    "    json_path = volume_metadata[\"id\"] + \"_ppe.json\"\n",
    "    with open(\"volume_records\\\\\" + volume_metadata[\"id\"] + \"_ppe.json\", \"w\") as outfile:\n",
    "        outfile.write('{\\n\\\"volume\\\": \\n')\n",
    "        json.dump(volume_metadata, outfile)\n",
    "        outfile.write(',')\n",
    "        outfile.write('\\n\\\"images\\\": [\\n')\n",
    "        first_img = True\n",
    "        for image in images:\n",
    "            if first_img:\n",
    "                first_img = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")\n",
    "            json.dump(image, outfile)\n",
    "        outfile.write(\"\\n],\\n\")\n",
    "        outfile.write('\\n\\\"people\\\": [\\n')\n",
    "        first_person = True\n",
    "        for person in people:\n",
    "            if first_person:\n",
    "                first_person = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")            \n",
    "            json.dump(person, outfile)            \n",
    "        outfile.write(\"\\n],\\n\")\n",
    "        outfile.write(\"\\\"places\\\": [\\n\")\n",
    "        first_place = True\n",
    "        for place in places:\n",
    "            if first_place:\n",
    "                first_place = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")\n",
    "            json.dump(place, outfile)\n",
    "        outfile.write(\"\\n],\\n\")\n",
    "        outfile.write(\"\\\"events\\\": [\\n\")\n",
    "        first_event = True\n",
    "        for event in events:\n",
    "            if first_event:\n",
    "                first_event = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")\n",
    "            json.dump(event, outfile)\n",
    "        outfile.write(\"\\n]\\n\")\n",
    "        outfile.write('}')\n",
    "            \n",
    "    print(\"JSON built, processing completed.\")\n",
    "            \n",
    "    return people, places, events, volume_metadata[\"id\"] + \"_ppe.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'models/mat_baut_1'\n",
      "Entities extracted.\n",
      "Relationships linked.\n",
      "Records for Thomas de Orvera merged.\n",
      "Records for Joseph Lopez de Cuella merged.\n",
      "Records for Lorenzo Noriega y Marroquin merged.\n",
      "Records for Juan de Justis merged.\n",
      "Records for Juo de Justis merged.\n",
      "Records for Lorenzo Noriega y Marro merged.\n",
      "Records for Lorenzo Contreras merged.\n",
      "Records for Juan Jph Solana merged.\n",
      "People records enhanced and disambiguated.\n",
      "JSON built, processing completed.\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "people, places, events, json_path = process_volume(\"transcriptions\\\\15834.xml\", \"models/mat_baut_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 12-ssda-xml-parser.ipynb.\n",
      "Converted 31-collate-xml-entities-spans.ipynb.\n",
      "Converted 33-split-data.ipynb.\n",
      "Converted 41-generic-framework-for-spacy-training.ipynb.\n",
      "Converted 42-initial-model.ipynb.\n",
      "Converted 51-data-preprocessing.ipynb.\n",
      "Converted 52-unstructured-to-markup.ipynb.\n",
      "Converted 53-markup-to-spatial-historian.ipynb.\n",
      "Converted 54-utility-functions.ipynb.\n",
      "Converted 61-prodigy-output-training-demo.ipynb.\n",
      "Converted 62-full-model-application-demo.ipynb.\n",
      "Converted 63-pt-model-training.ipynb.\n",
      "Converted 64-es-model-training.ipynb.\n",
      "Converted 65-all-annotations-model-training.ipynb.\n",
      "Converted 66-es-guatemala-model-training.ipynb.\n",
      "Converted 67-death-and-birth-records-together.ipynb.\n",
      "Converted 71-relationship-builder.ipynb.\n",
      "Converted 72-full-volume-processor.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
