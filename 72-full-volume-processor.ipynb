{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp full_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#dependencies\n",
    "\n",
    "#nlp packages\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "#manipulation of tables/arrays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "\n",
    "#internal imports\n",
    "from ssda_nlp.collate import *\n",
    "from ssda_nlp.split_data import *\n",
    "from ssda_nlp.modeling import *\n",
    "from ssda_nlp.model_performance_utils import *\n",
    "from ssda_nlp.xml_parser import *\n",
    "from ssda_nlp.unstructured2markup import *\n",
    "from ssda_nlp.utility import *\n",
    "from ssda_nlp.relationships import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def process_volume(path_to_transcription, path_to_model):\n",
    "    '''\n",
    "    runs the transcription of a single volume (formatted according to SSDA markup 2.0 specs) through the ML entity extraction\n",
    "    and rules-based relationship linking pipelines, then formats resulting data for export into SQL\n",
    "        path_to_transcription: path to an XML file containing the transcription of a single volume\n",
    "        path_to_model: path to a spaCy model trained to extract entities from the proper type of volume\n",
    "    \n",
    "        returns: final people, place, and event dictionaries as well as the\n",
    "        path to a JSON file containing volume metadata as well as people, place, and event records\n",
    "    '''\n",
    "    \n",
    "    #retrieve volume metadata and controlled vocabularies\n",
    "    \n",
    "    volume_metadata = retrieve_volume_metadata(path_to_transcription)\n",
    "    images = xml_v2_to_json(path_to_transcription)\n",
    "    vocabularies = retrieve_controlled_vocabularies()\n",
    "    \n",
    "    if volume_metadata[\"country\"] == \"Brazil\":\n",
    "        lang = \"pt\"\n",
    "        language = \"portuguese\"\n",
    "    else:\n",
    "        lang = \"es\"\n",
    "        language = \"spanish\"\n",
    "        \n",
    "    #load and apply trained model\n",
    "    \n",
    "    trained_model = load_model(path_to_model, language=lang, verbose='True')\n",
    "    \n",
    "    entry_df = parse_xml_v2(path_to_transcription)\n",
    "    \n",
    "    ent_preds_df, metrics_df, per_ent_metrics = test_model(trained_model, entry_df, \"entry_no\", \"text\", score_model=False)\n",
    "    print(\"Entities extracted.\")\n",
    "    \n",
    "    #iterate through each entry and build relationships\n",
    "    \n",
    "    people = []\n",
    "    places = []\n",
    "    events = []\n",
    "    \n",
    "    for i in range(len(entry_df.index)):\n",
    "        \n",
    "        entry_no = entry_df['entry_no'][i]\n",
    "        entry_text = entry_df['text'][i]    \n",
    "    \n",
    "        entities = ent_preds_df.loc[ent_preds_df['entry_no'] == entry_no]      \n",
    "    \n",
    "        entry_people, entry_places, entry_events = build_entry_metadata(entry_text, entities, path_to_transcription, entry_no)\n",
    "        \n",
    "        people += entry_people\n",
    "        places += entry_places\n",
    "        events += entry_events\n",
    "        \n",
    "    print(\"Relationships linked.\")\n",
    "    \n",
    "    #disambiguate locations and assign unique ids\n",
    "    \n",
    "    unique_places = []\n",
    "    for place in places:\n",
    "        if (place != None) and (place not in unique_places):\n",
    "            unique_places.append(place)\n",
    "    \n",
    "    places = []\n",
    "    curr_place = 1\n",
    "    for unique_place in unique_places:\n",
    "        place_record = {\"id\":volume_metadata[\"id\"] + '-L' + str(curr_place), \"location\":unique_place}\n",
    "        places.append(place_record)\n",
    "        curr_place += 1\n",
    "        \n",
    "    #incorporate location ids into event metadata\n",
    "    \n",
    "    for event in events:\n",
    "        location = event[\"location\"]\n",
    "        loc_id = \"unknown\"\n",
    "        if location != None:\n",
    "            for place in places:\n",
    "                if place[\"location\"] == location:\n",
    "                    loc_id = place[\"id\"]\n",
    "        if (loc_id == \"unknown\") and (location != None):\n",
    "            print(\"Failed to find location ID for \" + location)\n",
    "            event[\"location\"] = None\n",
    "        else:\n",
    "            event[\"location\"] = loc_id\n",
    "            \n",
    "        if event[\"location\"] == \"unknown\":\n",
    "            event[\"location\"] = None\n",
    "            \n",
    "    #bracket missing or incomplete event dates\n",
    "    \n",
    "    incomplete_dates = []\n",
    "    last_year = None\n",
    "    last_month = None\n",
    "    last_day = None\n",
    "    \n",
    "    for e in range(len(events)):\n",
    "        curr_year = events[e][\"date\"][:4]\n",
    "        curr_month = events[e][\"date\"][5:7]\n",
    "        curr_day = events[e][\"date\"][8:]\n",
    "        \n",
    "        #fix incompletely extracted years\n",
    "        if (curr_year != \"????\") and (last_year != None) and (abs(int(curr_year) - int(last_year)) > 1):\n",
    "            if (curr_year[3] == last_year[3]):\n",
    "                curr_year = last_year                \n",
    "            elif (curr_month == \"01\") and (last_month == \"12\"):\n",
    "                curr_year = str(int(last_year) + 1)                \n",
    "            else:\n",
    "                curr_year = last_year\n",
    "            events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day\n",
    "        \n",
    "        if (curr_year == \"????\") or (curr_month == \"??\") or (curr_day == \"??\"):\n",
    "            #logic to assign dates for birth events based on associated baptism\n",
    "            if events[e][\"type\"] == \"birth\":\n",
    "                if (events[e][\"id\"][:events[e][\"id\"].find('E')] == events[e - 1][\"id\"][:events[e - 1][\"id\"].find('E')]) and (events[e - 1][\"type\"] == \"baptism\") and ('?' not in events[e - 1][\"date\"]):\n",
    "                        if (curr_month != \"??\") and (curr_day != \"??\"):\n",
    "                            if (curr_month == \"12\") and (last_month == \"01\"):\n",
    "                                curr_year = str(int(last_year) - 1)                                \n",
    "                            elif (30 * int(last_month) + int(last_day) - 30 * int(curr_month) - int(curr_day)) < 21:\n",
    "                                curr_year = last_year\n",
    "                            events[e][\"date\"] = curr_year + '-' + events[e][\"date\"][5:7] + '-' + events[e][\"date\"][8:]\n",
    "                        elif curr_month != \"??\":\n",
    "                            if (curr_month == \"12\"):\n",
    "                                curr_day = \"01\"\n",
    "                                curr_year = str(int(last_year) - 1)\n",
    "                                events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-01-01'\n",
    "                            elif (curr_month == last_month):\n",
    "                                curr_day = \"01\"\n",
    "                                curr_year = last_year\n",
    "                                events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-' + last_month + '-' + last_day\n",
    "                            elif int(curr_month) == (int(last_month) - 1):\n",
    "                                curr_day = \"01\"\n",
    "                                curr_year = last_year\n",
    "                                events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-' + last_month + '-01'                            \n",
    "                        elif curr_day != \"??\":\n",
    "                            if curr_day <= last_day:\n",
    "                                curr_year = last_year\n",
    "                                curr_month = last_month                                \n",
    "                            else:\n",
    "                                if last_month == \"01\":\n",
    "                                    curr_month = \"12\"\n",
    "                                    curr_year = str(int(last_year) - 1)\n",
    "                                else:\n",
    "                                    curr_month = str(int(last_month) - 1)                                    \n",
    "                                    if len(curr_month) < 2:\n",
    "                                        curr_month = '0' + curr_month\n",
    "                                    curr_year = last_year\n",
    "                            events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day\n",
    "                        else:\n",
    "                            if (last_month == '01') and (int(last_day) < 21):\n",
    "                                curr_year = str(int(last_year) - 1)\n",
    "                                curr_month = \"12\"\n",
    "                                curr_day = str(int(last_day) + 9)                               \n",
    "                            elif int(last_day) < 21:\n",
    "                                curr_year = last_year\n",
    "                                curr_month = str(int(last_month) - 1)\n",
    "                                if len(curr_month) < 2:\n",
    "                                    curr_month = '0' + curr_month\n",
    "                                curr_day = str(int(last_day) + 9)\n",
    "                            else:\n",
    "                                curr_year = last_year\n",
    "                                curr_month = last_month\n",
    "                                curr_day = str(int(last_day) - 20)\n",
    "                                if len(curr_day) < 2:\n",
    "                                    curr_day = '0' + curr_day\n",
    "                            events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-' + last_month + '-' + last_day\n",
    "                            \n",
    "            if (curr_year == \"????\") or (curr_month == \"??\") or (curr_day == \"??\"):\n",
    "                incomplete_dates.append(e)\n",
    "        elif last_year == None:\n",
    "            for date in incomplete_dates:\n",
    "                events[date][\"date\"] = complete_date(events[date][\"date\"], None, curr_year + '-' + curr_month + '-' + curr_day)\n",
    "            \n",
    "            incomplete_dates = []\n",
    "            last_year = curr_year\n",
    "            last_month = curr_month\n",
    "            last_day = curr_day\n",
    "        elif (compare_dates(int(curr_year), int(curr_month), int(curr_day), int(last_year), int(last_month), int(last_day)) == '>') or (compare_dates(int(curr_year), int(curr_month), int(curr_day), int(last_year), int(last_month), int(last_day)) == '='):\n",
    "            for date in incomplete_dates:\n",
    "                events[date][\"date\"] = complete_date(events[date][\"date\"], last_year + '-' + last_month + '-' + last_day, curr_year + '-' + curr_month + '-' + curr_day)\n",
    "            \n",
    "            incomplete_dates = []\n",
    "            last_year = curr_year\n",
    "            last_month = curr_month\n",
    "            last_day = curr_day                    \n",
    "    \n",
    "    if last_year != None:\n",
    "        for date in incomplete_dates:\n",
    "            events[date][\"date\"] = complete_date(events[date][\"date\"], last_year + '-' + last_month + '-' + last_day, None)\n",
    "        \n",
    "    #merging any date brackets with equal endpoints\n",
    "    for event in events:\n",
    "        interval = event[\"date\"].split('/')\n",
    "        if (len(interval) == 2) and (interval[0] == interval[1]):\n",
    "            event[\"date\"] == interval[0]            \n",
    "        \n",
    "    print(\"Events configured.\")    \n",
    "    \n",
    "    for person in people:        \n",
    "        #strip titles and/or ranks from names\n",
    "        if person[\"name\"] != None:\n",
    "            name_parts = person[\"name\"].split(' ')\n",
    "\n",
    "            if len(name_parts) >= 2:\n",
    "                while ((name_parts[0].lower() + ' ' + name_parts[1].lower()) in vocabularies[\"titles\"]) or ((name_parts[0].lower() + ' ' + name_parts[1].lower()) in vocabularies[\"ranks\"]):\n",
    "                    if len(name_parts) == 2:\n",
    "                        person[\"name\"] = None\n",
    "                    else:\n",
    "                        person[\"name\"] = name_parts[2]\n",
    "                        for i in range(3, len(name_parts)):\n",
    "                            person[\"name\"] += ' ' + name_parts[i]\n",
    "\n",
    "                    if (name_parts[0].lower() + ' ' + name_parts[1].lower()) in vocabularies[\"titles\"]:\n",
    "                        if person[\"titles\"] != None:\n",
    "                            person[\"titles\"] += ';' + name_parts[0] + ' ' + name_parts[1]\n",
    "                        else:\n",
    "                            person[\"titles\"] = name_parts[0] + ' ' + name_parts[1]\n",
    "                    else:\n",
    "                        if person[\"ranks\"] != None:\n",
    "                            person[\"ranks\"] += ';' + name_parts[0] + ' ' + name_parts[1]\n",
    "                        else:\n",
    "                            person[\"ranks\"] = name_parts[0] + ' ' + name_parts[1]\n",
    "\n",
    "                    if person[\"name\"] == None:\n",
    "                        break\n",
    "                    name_parts = person[\"name\"].split(' ')\n",
    "                    if len(name_parts) < 2:\n",
    "                        break\n",
    "\n",
    "            if person[\"name\"] != None:\n",
    "                while (name_parts[0].lower() in vocabularies[\"titles\"]) or (name_parts[0].lower() in vocabularies[\"ranks\"]):\n",
    "                    if len(name_parts) == 1:\n",
    "                        person[\"name\"] = None\n",
    "                    else:\n",
    "                        person[\"name\"] = name_parts[1]\n",
    "                        for i in range(2, len(name_parts)):\n",
    "                            person[\"name\"] += ' ' + name_parts[i]\n",
    "\n",
    "                    if name_parts[0].lower() in vocabularies[\"titles\"]:\n",
    "                        if person[\"titles\"] != None:\n",
    "                            person[\"titles\"] += ';' + name_parts[0]\n",
    "                        else:\n",
    "                            person[\"titles\"] = name_parts[0]\n",
    "                    else:\n",
    "                        if person[\"ranks\"] != None:\n",
    "                            person[\"ranks\"] += ';' + name_parts[0]\n",
    "                        else:\n",
    "                            person[\"ranks\"] = name_parts[0]\n",
    "\n",
    "                    if person[\"name\"] == None:\n",
    "                        break\n",
    "                    name_parts = person[\"name\"].split(' ')\n",
    "                    \n",
    "    #normalize names and all characteristics\n",
    "    names = []\n",
    "    name_counts = []\n",
    "    ethnonym_vocab = retrieve_json_vocab(\"synonyms.json\", \"ethnonyms\")\n",
    "    phenotype_vocab = retrieve_json_vocab(\"synonyms.json\", \"phenotypes\", language=\"spanish\")\n",
    "    \n",
    "    for person in people:\n",
    "        #normalize characteristics and translate to English\n",
    "        for key in person:\n",
    "            if person[key] == None:\n",
    "                continue\n",
    "            if key == \"name\":\n",
    "                person[key] = normalize_text(person[key], \"synonyms.json\", context=\"name\")\n",
    "                #check extracted name for ethnonyms and/or attributed phenotypes        \n",
    "                if (person[\"name\"] != None) and (person[\"name\"] != normalize_text(person[\"name\"], \"synonyms.json\", context=\"ethnonym\")):\n",
    "                    for token in person[\"name\"].split(' '):\n",
    "                        eth_norm = normalize_text(token, \"synonyms.json\", context=\"ethnonym\")\n",
    "                        if token != eth_norm:\n",
    "                            if (person[\"ethnicities\"] == None) or (not (eth_norm in person[\"ethnicities\"])):\n",
    "                                if person[\"ethnicities\"] == None:\n",
    "                                    person[\"ethnicities\"] = eth_norm\n",
    "                                else:\n",
    "                                    person[\"ethnicities\"] = person[\"ethnicities\"] + ';' + eth_norm\n",
    "                    person[\"name\"] = normalize_text(person[\"name\"], \"synonyms.json\", context=\"ethnonym\")\n",
    "                else:\n",
    "                    for ethnonym in ethnonym_vocab:\n",
    "                        if ethnonym in person[\"name\"]:\n",
    "                            if person[\"ethnicities\"] == None:\n",
    "                                person[\"ethnicities\"] = ethnonym\n",
    "                            else:\n",
    "                                person[\"ethnicities\"] = person[\"ethnicities\"] + ';' + ethnonym\n",
    "                for phenotype in phenotype_vocab:\n",
    "                    if phenotype in normalize_text(person[key], \"synonyms.json\", context=\"characteristic\"):                    \n",
    "                        if person[\"phenotype\"] == None:\n",
    "                            person[\"phenotype\"] = phenotype\n",
    "                        else:\n",
    "                            person[\"phenotype\"] = person[\"phenotype\"] + ';' + phenotype\n",
    "                        if phenotype[-1] == 's':\n",
    "                            for token in person[\"name\"].split(' '):\n",
    "                                if normalize_text(token, \"synonyms.json\", context=\"characteristic\") == phenotype:\n",
    "                                    person[\"name\"] = person[\"name\"].replace(' ' + token, '')\n",
    "            elif key == \"ethnicities\":                \n",
    "                if person[key].find(';') == -1:\n",
    "                    person[key] = normalize_text(person[key], \"synonyms.json\", context=\"ethnonym\")                    \n",
    "                else:\n",
    "                    char_comp = person[key].split(';')\n",
    "                    person[key] = \"\"\n",
    "                    #strip out duplicate characteristics\n",
    "                    for char in char_comp:\n",
    "                        char = normalize_text(char, \"synonyms.json\", context=\"ethnonym\")                       \n",
    "                                          \n",
    "                        if not (char in person[key]):\n",
    "                            if person[key] == \"\":\n",
    "                                person[key] = char\n",
    "                            else:\n",
    "                                person[key] = person[key] + ';' + char\n",
    "            elif (key != \"id\") and (key != \"relationships\"):\n",
    "                if person[key].find(';') == -1:\n",
    "                    person[key] = normalize_text(person[key], \"synonyms.json\", context=\"characteristic\")\n",
    "                    person[key] = translate_characteristic(person[key], \"synonyms.json\", language)\n",
    "                else:\n",
    "                    char_comp = person[key].split(';')\n",
    "                    person[key] = \"\"\n",
    "                    #strip out duplicate characteristics\n",
    "                    for char in char_comp:\n",
    "                        char = normalize_text(char, \"synonyms.json\", context=\"characteristic\")                        \n",
    "                        char = translate_characteristic(char, \"synonyms.json\", language)                        \n",
    "                        if not (char in person[key]):\n",
    "                            if person[key] == \"\":\n",
    "                                person[key] = char\n",
    "                            else:\n",
    "                                person[key] = person[key] + ';' + char           \n",
    "        \n",
    "        #future improvement: find additional references for plural characteristics\n",
    "        \n",
    "        #count name frequency\n",
    "        if person[\"name\"] != None:\n",
    "            if person[\"name\"] in names:\n",
    "                name_counts[names.index(person['name'])] += 1\n",
    "            else:\n",
    "                names.append(person[\"name\"])\n",
    "                name_counts.append(1)   \n",
    "    \n",
    "    #disambiguate and merge people across the volume\n",
    "    redundant_records = []\n",
    "    merged_records = []    \n",
    "    for i in range(len(name_counts)):\n",
    "        if (name_counts[i] > .1 * len(images)) and (len(names[i].split(' ')) > 1) and (names[i] != \"Unknown principal\"):\n",
    "            records_to_merge = []            \n",
    "            for j in range(len(people)):\n",
    "                if people[j][\"name\"] == names[i]:\n",
    "                    redundant_records.append(people[j])\n",
    "                    records_to_merge.append(people[j])                    \n",
    "            merged_records.append(merge_records(records_to_merge))            \n",
    "    people = [person for person in people if person not in redundant_records]\n",
    "    for person in merged_records:\n",
    "        people.append(person)    \n",
    "    \n",
    "    print(\"People records enhanced and disambiguated.\")\n",
    "    \n",
    "    #convert dictionaries into JSON\n",
    "    json_path = volume_metadata[\"id\"] + \"_ppe.json\"\n",
    "    with open(\"volume_records\\\\\" + volume_metadata[\"id\"] + \"_ppe.json\", \"w\") as outfile:\n",
    "        outfile.write('{\\n\\\"volume\\\": \\n')\n",
    "        json.dump(volume_metadata, outfile)\n",
    "        outfile.write(',')\n",
    "        outfile.write('\\n\\\"images\\\": [\\n')\n",
    "        first_img = True\n",
    "        for image in images:\n",
    "            if first_img:\n",
    "                first_img = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")\n",
    "            json.dump(image, outfile)\n",
    "        outfile.write(\"\\n],\\n\")\n",
    "        outfile.write('\\n\\\"people\\\": [\\n')\n",
    "        first_person = True\n",
    "        for person in people:\n",
    "            if first_person:\n",
    "                first_person = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")            \n",
    "            json.dump(person, outfile)            \n",
    "        outfile.write(\"\\n],\\n\")\n",
    "        outfile.write(\"\\\"places\\\": [\\n\")\n",
    "        first_place = True\n",
    "        for place in places:\n",
    "            if first_place:\n",
    "                first_place = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")\n",
    "            json.dump(place, outfile)\n",
    "        outfile.write(\"\\n],\\n\")\n",
    "        outfile.write(\"\\\"events\\\": [\\n\")\n",
    "        first_event = True\n",
    "        for event in events:\n",
    "            if first_event:\n",
    "                first_event = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")\n",
    "            json.dump(event, outfile)\n",
    "        outfile.write(\"\\n]\\n\")\n",
    "        outfile.write('}')\n",
    "            \n",
    "    print(\"JSON built, processing completed.\")\n",
    "            \n",
    "    return people, places, events, volume_metadata[\"id\"] + \"_ppe.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'models/mat_baut_1'\n",
      "Entities extracted.\n",
      "Relationships linked.\n",
      "Events configured.\n",
      "People records enhanced and disambiguated.\n",
      "JSON built, processing completed.\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "people, places, events, json_path = process_volume(\"transcriptions\\\\166470.xml\", \"models/mat_baut_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 12-ssda-xml-parser.ipynb.\n",
      "Converted 31-collate-xml-entities-spans.ipynb.\n",
      "Converted 33-split-data.ipynb.\n",
      "Converted 41-generic-framework-for-spacy-training.ipynb.\n",
      "Converted 42-initial-model.ipynb.\n",
      "Converted 51-data-preprocessing.ipynb.\n",
      "Converted 52-unstructured-to-markup.ipynb.\n",
      "Converted 53-markup-to-spatial-historian.ipynb.\n",
      "Converted 54-utility-functions.ipynb.\n",
      "Converted 61-prodigy-output-training-demo.ipynb.\n",
      "Converted 62-full-model-application-demo.ipynb.\n",
      "Converted 63-pt-model-training.ipynb.\n",
      "Converted 64-es-model-training.ipynb.\n",
      "Converted 65-all-annotations-model-training.ipynb.\n",
      "Converted 66-es-guatemala-model-training.ipynb.\n",
      "Converted 67-death-and-birth-records-together.ipynb.\n",
      "Converted 71-relationship-builder.ipynb.\n",
      "Converted 72-full-volume-processor.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
