{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp full_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#dependencies\n",
    "\n",
    "#nlp packages\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "#manipulation of tables/arrays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "\n",
    "#internal imports\n",
    "from ssda_nlp.collate import *\n",
    "from ssda_nlp.split_data import *\n",
    "from ssda_nlp.modeling import *\n",
    "from ssda_nlp.model_performance_utils import *\n",
    "from ssda_nlp.xml_parser import *\n",
    "from ssda_nlp.unstructured2markup import *\n",
    "from ssda_nlp.utility import *\n",
    "from ssda_nlp.relationships import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def validate_entry(entry_entities, entry_people, entry_places, entry_events, entry_type = \"baptism\"):\n",
    "    '''\n",
    "    analyzes data extracted from a single ecclesiastical entry to assess accuracy of automated entity and relationship extraction\n",
    "        entry_type: baptism, marriage, or burial\n",
    "        entry_entities: df containing all entities extracted from entry (with assignment status)\n",
    "        entry_people: list of dictionaries, each containing people who appear in entry\n",
    "        entry_places: list of dictionaries, each containing places that appear in entry\n",
    "        entry_events: list of dictionaries, each containing events that appear in entry\n",
    "        \n",
    "        returns: dictionary in which key-value pairs encode various details regarding performance of automation\n",
    "    '''\n",
    "    \n",
    "    validation_dict = {}\n",
    "    \n",
    "    if entry_type == \"baptism\":\n",
    "        pass\n",
    "        #is there a baptism event?\n",
    "        for bapt_idx in range(len(entry_events)):\n",
    "            isBaptism = entry_events[bapt_idx].get('type')=='baptism' #Verify that this is indeed the string\n",
    "            if isBaptism:\n",
    "                break\n",
    "        \n",
    "        #Still have access to bapt_idx\n",
    "        #Functionalize the rest of the code below so that it works with list of nested dicts\n",
    "        \n",
    "        #if so:\n",
    "        if isBaptism:\n",
    "            #does baptism event have a complete date?\n",
    "            event_date = entry_events.get('date')\n",
    "            isDateComplete = ( ('?' in event_date) ) # Unfinished date: '????-??-22'\n",
    "            #does baptism event have a location?\n",
    "            event_location = entry_events.get('location')\n",
    "            hasLocation = ( (len(entry_places)>0) )#and ( entry_events.get('location') == entry_places) )\n",
    "            #^Implement an actual check to verify... not sure if this is good enough by itself\n",
    "            \n",
    "        #is there an identified cleric?\n",
    "        hasCleric = len(entry_events.get('cleric'))==15 #This is also a pseudo check, find out what a missing cleric looks like\n",
    "        #is there an identified principal?\n",
    "        hasPrincipal = len(entry_events.get('principal'))==15 #This is also a pseudo check, find out what a missing principal looks like\n",
    "        #if so:\n",
    "        if hasCleric and hasPrincipal:\n",
    "            #are principal's godparent(s) identified?\n",
    "            pass ##########################################################################################################\n",
    "            hasGodparents = 0 ##########################################################################################################\n",
    "        \n",
    "        isInfant = 0 ##########################################################################################################\n",
    "        #if so, and if principal is an infant:\n",
    "        if hasGodparents and isInfant:\n",
    "            pass\n",
    "            #are principal's parent(s) identified?\n",
    "            hasParents = 0 ##########################################################################################################\n",
    "            #is there a birth event?\n",
    "            isBirthEvent = 0 ##########################################################################################################\n",
    "            #if so:\n",
    "            if isBirthEvent:\n",
    "                pass\n",
    "                #does birth event have a complete date?\n",
    "                #does birth event have a location?\n",
    "                \n",
    "        isEnslaved = 0 ##########################################################################################################\n",
    "        #if so, and if principal is enslaved:\n",
    "            #is principal's enslaver identified?\n",
    "            \n",
    "        #other questions re people:\n",
    "        #are there any entities labeled PER who do not have an explicit role in the baptism (e.g. principal, cleric, parent, enslaver, etc.)\n",
    "        #are there likely couples (e.g. parents, godparents) not explicitly flagged as such\n",
    "        #are there any people with very similar names that still appear separately\n",
    "        \n",
    "        #questions re characteristics:\n",
    "        #are there characteristics that were not categorized\n",
    "        #are there characteristics or relationships that were not assigned\n",
    "        #for ethnicities that were assigned, confirm that they were *not* assigned to slaveholders or clerics            \n",
    "            \n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    return validation_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def process_volume(path_to_transcription, path_to_model):\n",
    "    '''\n",
    "    runs the transcription of a single volume (formatted according to SSDA markup 2.0 specs) through the ML entity extraction\n",
    "    and rules-based relationship linking pipelines, then formats resulting data for export into SQL\n",
    "        path_to_transcription: path to an XML file containing the transcription of a single volume\n",
    "        path_to_model: path to a spaCy model trained to extract entities from the proper type of volume\n",
    "    \n",
    "        returns: final people, place, and event dictionaries as well as the\n",
    "        path to a JSON file containing volume metadata as well as people, place, and event records\n",
    "    '''\n",
    "    \n",
    "    #retrieve volume metadata and controlled vocabularies\n",
    "    \n",
    "    volume_metadata = retrieve_volume_metadata(path_to_transcription)\n",
    "    images = xml_v2_to_json(path_to_transcription)\n",
    "    vocabularies = retrieve_controlled_vocabularies()\n",
    "    \n",
    "    if volume_metadata[\"country\"] == \"Brazil\":\n",
    "        lang = \"pt\"\n",
    "        language = \"portuguese\"\n",
    "    else:\n",
    "        lang = \"es\"\n",
    "        language = \"spanish\"\n",
    "        \n",
    "    #load and apply trained model\n",
    "    \n",
    "    trained_model = load_model(path_to_model, language=lang, verbose='True')\n",
    "    \n",
    "    entry_df = parse_xml_v2(path_to_transcription)\n",
    "    \n",
    "    ent_preds_df, metrics_df, per_ent_metrics = test_model(trained_model, entry_df, \"entry_no\", \"text\", score_model=False)\n",
    "    print(\"Entities extracted.\")\n",
    "    \n",
    "    #development\n",
    "    #pd.set_option(\"display.max_rows\", 101)\n",
    "    #display(ent_preds_df.head(100))\n",
    "    \n",
    "    #iterate through each entry and build relationships\n",
    "    \n",
    "    people = []\n",
    "    places = []\n",
    "    events = []\n",
    "    \n",
    "    entitiesRunning = pd.DataFrame()\n",
    "    noCategoryRunning = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(entry_df.index)):\n",
    "        \n",
    "        entry_no = entry_df['entry_no'][i]\n",
    "        entry_text = entry_df['text'][i]    \n",
    "    \n",
    "        entities = copy.deepcopy(ent_preds_df[ent_preds_df['entry_no'] == entry_no])\n",
    "        \n",
    "        entities[\"assigned\"] = True\n",
    "        \n",
    "        entry_people, entry_places, entry_events, entities, characteristics_df, categorized_characteristics, uncategorized_characteristics = build_entry_metadata(entry_text, entities, path_to_transcription, entry_no)             \n",
    "        \n",
    "        if uncategorized_characteristics.shape[0] > 0:\n",
    "            noCategoryRunning = noCategoryRunning.append(uncategorized_characteristics)\n",
    "        \n",
    "        #FIND ENTITIES THAT ARE UNASSIGNED OR UNCATEGORIZED\n",
    "        entity_index = 0\n",
    "        for ent_data in entities.itertuples():\n",
    "            for char_data in characteristics_df.itertuples():\n",
    "                char_index = 0\n",
    "                #characteristic is not categorized:\n",
    "                if (char_data.category == None) and (ent_data.pred_start == char_data.pred_start) and (ent_data.pred_entity == char_data.pred_entity):\n",
    "                    continue #Already dealth with\n",
    "                #characteristic is categorized but not assigned\n",
    "                elif (ent_data.pred_label == char_data.pred_label) and (ent_data.pred_start == char_data.pred_start) and (ent_data.pred_entity == char_data.pred_entity):\n",
    "                    if (char_data.assignment == None):\n",
    "                        entities.at[entity_index, \"assigned\"] = False\n",
    "                char_index += 1\n",
    "            entity_index += 1\n",
    "\n",
    "        entitiesRunning = entitiesRunning.append(entities)  \n",
    "        \n",
    "        if i<3:\n",
    "            validation_dict = validate_entry(entities, entry_people, entry_places, entry_events)\n",
    "            #Do I need to pass in uncategorized_characteristics, \n",
    "            \n",
    "            print(\"Entites df:\")\n",
    "            display(entities.head(30))\n",
    "            print(\"People:\")\n",
    "            display(entry_people)\n",
    "            print(type(entry_people))\n",
    "            print(\"Places:\")\n",
    "            display(entry_places)\n",
    "            print(type(entry_places))\n",
    "            print(\"Events:\")\n",
    "            display(entry_events)\n",
    "            print(type(entry_events))\n",
    "            print(\"------------------------------------------------------------------------------------------------\")\n",
    "                \n",
    "        people += entry_people\n",
    "        places += entry_places\n",
    "        events += entry_events\n",
    "    \n",
    "    noCategoryRunning.reset_index(drop = True, inplace = True)\n",
    "    noCategoryRunning[\"assigned\"] = False\n",
    "    print(\"Relationships linked.\")\n",
    "    \n",
    "    #disambiguate locations and assign unique ids\n",
    "    \n",
    "    unique_places = []\n",
    "    for place in places:\n",
    "        if (place != None) and (place not in unique_places):\n",
    "            unique_places.append(place)\n",
    "            \n",
    "    for person in people:        \n",
    "        if (person[\"origin\"] != None) and (person[\"origin\"] not in unique_places):\n",
    "            unique_places.append(person[\"origin\"])\n",
    "    \n",
    "    places = []\n",
    "    curr_place = 1\n",
    "    for unique_place in unique_places:\n",
    "        place_record = {\"id\":volume_metadata[\"id\"] + '-L' + str(curr_place), \"location\":unique_place}\n",
    "        places.append(place_record)\n",
    "        curr_place += 1\n",
    "        \n",
    "    #incorporate location ids into event metadata and person records\n",
    "    \n",
    "    for event in events:\n",
    "        location = event[\"location\"]\n",
    "        loc_id = \"unknown\"\n",
    "        if location != None:\n",
    "            for place in places:\n",
    "                if place[\"location\"] == location:\n",
    "                    loc_id = place[\"id\"]\n",
    "        if (loc_id == \"unknown\") and (location != None):\n",
    "            print(\"Failed to find location ID for \" + location)\n",
    "            event[\"location\"] = None\n",
    "        else:\n",
    "            event[\"location\"] = loc_id\n",
    "            \n",
    "        if event[\"location\"] == \"unknown\":\n",
    "            event[\"location\"] = None\n",
    "            \n",
    "    for person in people:\n",
    "        if person[\"origin\"] == None:\n",
    "            continue\n",
    "        \n",
    "        for place in places:\n",
    "            if place[\"location\"] == person[\"origin\"]:\n",
    "                person[\"origin\"] = place[\"id\"]\n",
    "                break\n",
    "        \n",
    "    #bracket missing or incomplete event dates\n",
    "    \n",
    "    incomplete_dates = []\n",
    "    last_year = None\n",
    "    last_month = None\n",
    "    last_day = None\n",
    "    \n",
    "    for e in range(len(events)):\n",
    "        curr_year = events[e][\"date\"][:4]\n",
    "        curr_month = events[e][\"date\"][5:7]\n",
    "        curr_day = events[e][\"date\"][8:]\n",
    "        \n",
    "        #fix incompletely extracted years\n",
    "        if (curr_year != \"????\") and (last_year != None) and (abs(int(curr_year) - int(last_year)) > 1):\n",
    "            if (curr_year[3] == last_year[3]):\n",
    "                curr_year = last_year                \n",
    "            elif (curr_month == \"01\") and (last_month == \"12\"):\n",
    "                curr_year = str(int(last_year) + 1)                \n",
    "            else:\n",
    "                curr_year = last_year\n",
    "            events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day\n",
    "        \n",
    "        if (curr_year == \"????\") or (curr_month == \"??\") or (curr_day == \"??\"):\n",
    "            #logic to assign dates for birth events based on associated baptism\n",
    "            if events[e][\"type\"] == \"birth\":\n",
    "                if (events[e][\"id\"][:events[e][\"id\"].find('E')] == events[e - 1][\"id\"][:events[e - 1][\"id\"].find('E')]) and (events[e - 1][\"type\"] == \"baptism\") and ('?' not in events[e - 1][\"date\"]):\n",
    "                        if (curr_month != \"??\") and (curr_day != \"??\"):\n",
    "                            if (curr_month == \"12\") and (last_month == \"01\"):\n",
    "                                curr_year = str(int(last_year) - 1)                                \n",
    "                            elif (30 * int(last_month) + int(last_day) - 30 * int(curr_month) - int(curr_day)) < 21:\n",
    "                                curr_year = last_year\n",
    "                            events[e][\"date\"] = curr_year + '-' + events[e][\"date\"][5:7] + '-' + events[e][\"date\"][8:]\n",
    "                        elif curr_month != \"??\":\n",
    "                            if (curr_month == \"12\"):\n",
    "                                curr_day = \"01\"\n",
    "                                curr_year = str(int(last_year) - 1)\n",
    "                                events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-01-01'\n",
    "                            elif (curr_month == last_month):\n",
    "                                curr_day = \"01\"\n",
    "                                curr_year = last_year\n",
    "                                events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-' + last_month + '-' + last_day\n",
    "                            elif int(curr_month) == (int(last_month) - 1):\n",
    "                                curr_day = \"01\"\n",
    "                                curr_year = last_year\n",
    "                                events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-' + last_month + '-01'                            \n",
    "                        elif curr_day != \"??\":\n",
    "                            if curr_day <= last_day:\n",
    "                                curr_year = last_year\n",
    "                                curr_month = last_month                                \n",
    "                            else:\n",
    "                                if last_month == \"01\":\n",
    "                                    curr_month = \"12\"\n",
    "                                    curr_year = str(int(last_year) - 1)\n",
    "                                else:\n",
    "                                    curr_month = str(int(last_month) - 1)                                    \n",
    "                                    if len(curr_month) < 2:\n",
    "                                        curr_month = '0' + curr_month\n",
    "                                    curr_year = last_year\n",
    "                            events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day\n",
    "                        else:\n",
    "                            if (last_month == '01') and (int(last_day) < 21):\n",
    "                                curr_year = str(int(last_year) - 1)\n",
    "                                curr_month = \"12\"\n",
    "                                curr_day = str(int(last_day) + 9)                               \n",
    "                            elif int(last_day) < 21:\n",
    "                                curr_year = last_year\n",
    "                                curr_month = str(int(last_month) - 1)\n",
    "                                if len(curr_month) < 2:\n",
    "                                    curr_month = '0' + curr_month\n",
    "                                curr_day = str(int(last_day) + 9)\n",
    "                            else:\n",
    "                                curr_year = last_year\n",
    "                                curr_month = last_month\n",
    "                                curr_day = str(int(last_day) - 20)\n",
    "                                if len(curr_day) < 2:\n",
    "                                    curr_day = '0' + curr_day\n",
    "                            events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-' + last_month + '-' + last_day\n",
    "                            \n",
    "            if (curr_year == \"????\") or (curr_month == \"??\") or (curr_day == \"??\"):\n",
    "                incomplete_dates.append(e)\n",
    "        elif last_year == None:\n",
    "            for date in incomplete_dates:\n",
    "                events[date][\"date\"] = complete_date(events[date][\"date\"], None, curr_year + '-' + curr_month + '-' + curr_day)\n",
    "            \n",
    "            incomplete_dates = []\n",
    "            last_year = curr_year\n",
    "            last_month = curr_month\n",
    "            last_day = curr_day\n",
    "        elif (compare_dates(int(curr_year), int(curr_month), int(curr_day), int(last_year), int(last_month), int(last_day)) == '>') or (compare_dates(int(curr_year), int(curr_month), int(curr_day), int(last_year), int(last_month), int(last_day)) == '='):\n",
    "            for date in incomplete_dates:\n",
    "                events[date][\"date\"] = complete_date(events[date][\"date\"], last_year + '-' + last_month + '-' + last_day, curr_year + '-' + curr_month + '-' + curr_day)\n",
    "            \n",
    "            incomplete_dates = []\n",
    "            last_year = curr_year\n",
    "            last_month = curr_month\n",
    "            last_day = curr_day                    \n",
    "    \n",
    "    if last_year != None:\n",
    "        for date in incomplete_dates:\n",
    "            events[date][\"date\"] = complete_date(events[date][\"date\"], last_year + '-' + last_month + '-' + last_day, None)\n",
    "        \n",
    "    #merging any date brackets with equal endpoints\n",
    "    for event in events:\n",
    "        interval = event[\"date\"].split('/')\n",
    "        if (len(interval) == 2) and (interval[0] == interval[1]):\n",
    "            event[\"date\"] == interval[0]            \n",
    "        \n",
    "    print(\"Events configured.\")    \n",
    "    \n",
    "    for person in people:        \n",
    "        #strip titles and/or ranks from names\n",
    "        if person[\"name\"] != None:\n",
    "            name_parts = person[\"name\"].split(' ')\n",
    "\n",
    "            if len(name_parts) >= 2:\n",
    "                while ((name_parts[0].lower() + ' ' + name_parts[1].lower()) in vocabularies[\"titles\"]) or ((name_parts[0].lower() + ' ' + name_parts[1].lower()) in vocabularies[\"ranks\"]):\n",
    "                    if len(name_parts) == 2:\n",
    "                        person[\"name\"] = None\n",
    "                    else:\n",
    "                        person[\"name\"] = name_parts[2]\n",
    "                        for i in range(3, len(name_parts)):\n",
    "                            person[\"name\"] += ' ' + name_parts[i]\n",
    "\n",
    "                    if (name_parts[0].lower() + ' ' + name_parts[1].lower()) in vocabularies[\"titles\"]:\n",
    "                        if person[\"titles\"] != None:\n",
    "                            person[\"titles\"] += ';' + name_parts[0] + ' ' + name_parts[1]\n",
    "                        else:\n",
    "                            person[\"titles\"] = name_parts[0] + ' ' + name_parts[1]\n",
    "                    else:\n",
    "                        if person[\"ranks\"] != None:\n",
    "                            person[\"ranks\"] += ';' + name_parts[0] + ' ' + name_parts[1]\n",
    "                        else:\n",
    "                            person[\"ranks\"] = name_parts[0] + ' ' + name_parts[1]\n",
    "\n",
    "                    if person[\"name\"] == None:\n",
    "                        break\n",
    "                    name_parts = person[\"name\"].split(' ')\n",
    "                    if len(name_parts) < 2:\n",
    "                        break\n",
    "\n",
    "            if person[\"name\"] != None:\n",
    "                while (name_parts[0].lower() in vocabularies[\"titles\"]) or (name_parts[0].lower() in vocabularies[\"ranks\"]):\n",
    "                    if len(name_parts) == 1:\n",
    "                        person[\"name\"] = None\n",
    "                    else:\n",
    "                        person[\"name\"] = name_parts[1]\n",
    "                        for i in range(2, len(name_parts)):\n",
    "                            person[\"name\"] += ' ' + name_parts[i]\n",
    "\n",
    "                    if name_parts[0].lower() in vocabularies[\"titles\"]:\n",
    "                        if person[\"titles\"] != None:\n",
    "                            person[\"titles\"] += ';' + name_parts[0]\n",
    "                        else:\n",
    "                            person[\"titles\"] = name_parts[0]\n",
    "                    else:\n",
    "                        if person[\"ranks\"] != None:\n",
    "                            person[\"ranks\"] += ';' + name_parts[0]\n",
    "                        else:\n",
    "                            person[\"ranks\"] = name_parts[0]\n",
    "\n",
    "                    if person[\"name\"] == None:\n",
    "                        break\n",
    "                    name_parts = person[\"name\"].split(' ')\n",
    "                    \n",
    "    #normalize names and all characteristics\n",
    "    names = []\n",
    "    name_counts = []\n",
    "    ethnonym_vocab = retrieve_json_vocab(\"synonyms.json\", \"ethnonyms\")\n",
    "    phenotype_vocab = retrieve_json_vocab(\"synonyms.json\", \"phenotypes\", language=\"spanish\")\n",
    "    \n",
    "    for person in people:\n",
    "        #normalize characteristics and translate to English\n",
    "        for key in person:\n",
    "            if person[key] == None:\n",
    "                continue\n",
    "            if key == \"name\":\n",
    "                person[key] = normalize_text(person[key], \"synonyms.json\", context=\"name\")\n",
    "                #check extracted name for ethnonyms and/or attributed phenotypes        \n",
    "                if (person[\"name\"] != None) and (person[\"name\"] != normalize_text(person[\"name\"], \"synonyms.json\", context=\"ethnonym\")):\n",
    "                    for token in person[\"name\"].split(' '):\n",
    "                        eth_norm = normalize_text(token, \"synonyms.json\", context=\"ethnonym\")\n",
    "                        if token != eth_norm:\n",
    "                            if (person[\"ethnicities\"] == None) or (not (eth_norm in person[\"ethnicities\"])):\n",
    "                                if person[\"ethnicities\"] == None:\n",
    "                                    person[\"ethnicities\"] = eth_norm\n",
    "                                else:\n",
    "                                    person[\"ethnicities\"] = person[\"ethnicities\"] + ';' + eth_norm\n",
    "                    person[\"name\"] = normalize_text(person[\"name\"], \"synonyms.json\", context=\"ethnonym\")\n",
    "                else:\n",
    "                    for ethnonym in ethnonym_vocab:\n",
    "                        if ethnonym in person[\"name\"]:\n",
    "                            if person[\"ethnicities\"] == None:\n",
    "                                person[\"ethnicities\"] = ethnonym\n",
    "                            else:\n",
    "                                person[\"ethnicities\"] = person[\"ethnicities\"] + ';' + ethnonym\n",
    "                for phenotype in phenotype_vocab:\n",
    "                    if phenotype in normalize_text(person[key], \"synonyms.json\", context=\"characteristic\"):                    \n",
    "                        if person[\"phenotype\"] == None:\n",
    "                            person[\"phenotype\"] = phenotype\n",
    "                        else:\n",
    "                            person[\"phenotype\"] = person[\"phenotype\"] + ';' + phenotype\n",
    "                        if phenotype[-1] == 's':\n",
    "                            for token in person[\"name\"].split(' '):\n",
    "                                if normalize_text(token, \"synonyms.json\", context=\"characteristic\") == phenotype:\n",
    "                                    person[\"name\"] = person[\"name\"].replace(' ' + token, '')\n",
    "            elif key == \"ethnicities\":                \n",
    "                if person[key].find(';') == -1:\n",
    "                    person[key] = normalize_text(person[key], \"synonyms.json\", context=\"ethnonym\")                    \n",
    "                else:\n",
    "                    char_comp = person[key].split(';')\n",
    "                    person[key] = \"\"\n",
    "                    #strip out duplicate characteristics\n",
    "                    for char in char_comp:\n",
    "                        char = normalize_text(char, \"synonyms.json\", context=\"ethnonym\")                       \n",
    "                                          \n",
    "                        if not (char in person[key]):\n",
    "                            if person[key] == \"\":\n",
    "                                person[key] = char\n",
    "                            else:\n",
    "                                person[key] = person[key] + ';' + char\n",
    "            elif (key != \"id\") and (key != \"relationships\"):\n",
    "                if person[key].find(';') == -1:\n",
    "                    person[key] = normalize_text(person[key], \"synonyms.json\", context=\"characteristic\")\n",
    "                    person[key] = translate_characteristic(person[key], \"synonyms.json\", language)\n",
    "                else:\n",
    "                    char_comp = person[key].split(';')\n",
    "                    person[key] = \"\"\n",
    "                    #strip out duplicate characteristics\n",
    "                    for char in char_comp:\n",
    "                        char = normalize_text(char, \"synonyms.json\", context=\"characteristic\")                        \n",
    "                        char = translate_characteristic(char, \"synonyms.json\", language)                        \n",
    "                        if not (char in person[key]):\n",
    "                            if person[key] == \"\":\n",
    "                                person[key] = char\n",
    "                            else:\n",
    "                                person[key] = person[key] + ';' + char           \n",
    "        \n",
    "        #future improvement: find additional references for plural characteristics\n",
    "        \n",
    "        #count name frequency\n",
    "        if person[\"name\"] != None:\n",
    "            if person[\"name\"] in names:\n",
    "                name_counts[names.index(person['name'])] += 1\n",
    "            else:\n",
    "                names.append(person[\"name\"])\n",
    "                name_counts.append(1)   \n",
    "    \n",
    "    #disambiguate and merge people across the volume\n",
    "    redundant_records = []\n",
    "    merged_records = []    \n",
    "    for i in range(len(name_counts)):\n",
    "        if (name_counts[i] > .1 * len(images)) and (len(names[i].split(' ')) > 1) and (names[i] != \"Unknown principal\"):\n",
    "            records_to_merge = []            \n",
    "            for j in range(len(people)):\n",
    "                if people[j][\"name\"] == names[i]:\n",
    "                    redundant_records.append(people[j])\n",
    "                    records_to_merge.append(people[j])                    \n",
    "            merged_records.append(merge_records(records_to_merge))            \n",
    "    people = [person for person in people if person not in redundant_records]\n",
    "    for person in merged_records:\n",
    "        people.append(person)    \n",
    "    \n",
    "    print(\"People records enhanced and disambiguated.\")\n",
    "    \n",
    "    #reduce compound person IDs to single ID, add references field\n",
    "    people, events = compact_references(people, events)\n",
    "    \n",
    "    print(\"Single ID generated for each individual.\")\n",
    "    \n",
    "    #convert dictionaries into JSON    \n",
    "    with open(\"volume_records\\\\\" + volume_metadata[\"id\"] + \".json\", \"w\") as outfile:\n",
    "        outfile.write('{\\n\\\"volume\\\": \\n')\n",
    "        json.dump(volume_metadata, outfile)\n",
    "        outfile.write(',')\n",
    "        outfile.write('\\n\\\"images\\\": [\\n')\n",
    "        first_img = True\n",
    "        for image in images:\n",
    "            if first_img:\n",
    "                first_img = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")\n",
    "            json.dump(image, outfile)\n",
    "        outfile.write(\"\\n],\\n\")\n",
    "        outfile.write('\\n\\\"people\\\": [\\n')\n",
    "        first_person = True\n",
    "        for person in people:\n",
    "            if first_person:\n",
    "                first_person = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")            \n",
    "            json.dump(person, outfile)            \n",
    "        outfile.write(\"\\n],\\n\")\n",
    "        outfile.write(\"\\\"places\\\": [\\n\")\n",
    "        first_place = True\n",
    "        for place in places:\n",
    "            if first_place:\n",
    "                first_place = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")\n",
    "            json.dump(place, outfile)\n",
    "        outfile.write(\"\\n],\\n\")\n",
    "        outfile.write(\"\\\"events\\\": [\\n\")\n",
    "        first_event = True\n",
    "        for event in events:\n",
    "            if first_event:\n",
    "                first_event = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")\n",
    "            json.dump(event, outfile)\n",
    "        outfile.write(\"\\n]\\n\")\n",
    "        outfile.write('}')\n",
    "            \n",
    "    print(\"JSON built, processing completed.\")\n",
    "            \n",
    "    return people, places, events, volume_metadata[\"id\"] + \"_ppe.json\", entitiesRunning, noCategoryRunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14193\\anaconda3\\lib\\site-packages\\spacy\\util.py:707: UserWarning: [W095] Model 'en_pipeline' (0.0.0) requires spaCy >=3.0.5,<3.1.0 and is incompatible with the current version (3.0.3). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'models/15834'\n",
      "Entities extracted.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-546fa4eacd37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#no_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpeople\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoCategory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_volume\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"transcriptions\\\\15834.xml\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"models/15834\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-6de401ce3664>\u001b[0m in \u001b[0;36mprocess_volume\u001b[1;34m(path_to_transcription, path_to_model)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0mvalidation_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_entry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry_people\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry_places\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry_events\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Entites df:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-5617a8bbd590>\u001b[0m in \u001b[0;36mvalidate_entry\u001b[1;34m(entry_entities, entry_people, entry_places, entry_events, entry_type)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m#is there a baptism event?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m#for idx in range(len(entry_events)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0misBaptism\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentry_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'baptism'\u001b[0m \u001b[1;31m#Verify that this is indeed the string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "people, places, events, json_path, entities, noCategory = process_volume(\"transcriptions\\\\15834.xml\", \"models/15834\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>entry_no</th>\n",
       "      <th>pred_entity</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>pred_start</th>\n",
       "      <th>pred_end</th>\n",
       "      <th>assigned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1033-1</td>\n",
       "      <td>Juana</td>\n",
       "      <td>PER</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index entry_no pred_entity pred_label pred_start pred_end  assigned\n",
       "0      0   1033-1       Juana        PER         10       15      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14135, 7)\n",
      "------------------------------------------------\n",
      "Unassigned CHAR/RELs found:\n",
      "char_df shape: (1010, 7)\n",
      "rel_df shape: (172, 7)\n",
      "unassigned_df shape: (1182, 6)\n",
      "------------------------------------------------\n",
      "unassigned_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry_no</th>\n",
       "      <th>pred_entity</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>pred_start</th>\n",
       "      <th>pred_end</th>\n",
       "      <th>assigned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1228-1</td>\n",
       "      <td>adverti</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1227-1</td>\n",
       "      <td>adverti</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1121-1</td>\n",
       "      <td>paroq.l</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1210-1</td>\n",
       "      <td>acien[roto] las part.das de los indios</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>23</td>\n",
       "      <td>61</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1202-4</td>\n",
       "      <td>part.da aunq</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>17</td>\n",
       "      <td>29</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entry_no                             pred_entity pred_label pred_start  \\\n",
       "0   1228-1                                 adverti       CHAR         14   \n",
       "1   1227-1                                 adverti       CHAR         14   \n",
       "2   1121-1                                 paroq.l       CHAR         19   \n",
       "3   1210-1  acien[roto] las part.das de los indios       CHAR         23   \n",
       "4   1202-4                            part.da aunq       CHAR         17   \n",
       "\n",
       "  pred_end  assigned  \n",
       "0       21     False  \n",
       "1       21     False  \n",
       "2       26     False  \n",
       "3       61     False  \n",
       "4       29     False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noCategory\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry_no</th>\n",
       "      <th>pred_entity</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>pred_start</th>\n",
       "      <th>pred_end</th>\n",
       "      <th>assigned</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1033-2</td>\n",
       "      <td>h. l.16</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>151</td>\n",
       "      <td>158</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1033-3</td>\n",
       "      <td>h. l.</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>142</td>\n",
       "      <td>147</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1034-4</td>\n",
       "      <td>lucumi</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>160</td>\n",
       "      <td>166</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1036-1</td>\n",
       "      <td>lucumi</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>225</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1037-1</td>\n",
       "      <td>Yngeniero mili</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>229</td>\n",
       "      <td>243</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entry_no     pred_entity pred_label  pred_start  pred_end  assigned category\n",
       "0   1033-2         h. l.16       CHAR         151       158     False     None\n",
       "1   1033-3           h. l.       CHAR         142       147     False     None\n",
       "2   1034-4          lucumi       CHAR         160       166     False     None\n",
       "3   1036-1          lucumi       CHAR         225       231     False     None\n",
       "4   1037-1  Yngeniero mili       CHAR         229       243     False     None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "print(\"entities:\")\n",
    "display(entities.head(1))\n",
    "print(entities.shape)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "char_df = entities[(entities['pred_label'] == 'CHAR')]\n",
    "char_df = copy.deepcopy(char_df[char_df['assigned'] == False])\n",
    "rel_df = entities[(entities['pred_label'] == 'REL')]\n",
    "rel_df = copy.deepcopy(rel_df[rel_df['assigned'] == False])\n",
    "\n",
    "unassigned_df = char_df.append(rel_df)\n",
    "unassigned_df.sort_values(by='index', inplace = True)\n",
    "unassigned_df.drop(['index'], axis=1, inplace = True)\n",
    "unassigned_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "print(\"Unassigned CHAR/RELs found:\")\n",
    "print(\"char_df shape: \" + str(char_df.shape))\n",
    "print(\"rel_df shape: \" + str(rel_df.shape))\n",
    "print(\"unassigned_df shape: \" + str(unassigned_df.shape))\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "print(\"unassigned_df\")\n",
    "if unassigned_df.shape[0] < 1:\n",
    "    print(\"No unassigned characteristics or relationships found...\")\n",
    "else:\n",
    "    display(unassigned_df.head())\n",
    "    \n",
    "print(\"noCategory\")\n",
    "if noCategory.shape[0] < 1:\n",
    "    print(\"No characteristics without categories found...\")\n",
    "else:\n",
    "    #noCategory.drop(['assgnmt_status'], axis=1, inplace = True) #Just a bunch of 1s for some reason\n",
    "    #noCategory.drop(['level_0'], axis=1, inplace = True) #Also just a bunch of 1s for some reason\n",
    "    #noCategory.reset_index(inplace = True, drop = True)\n",
    "    #noCategory.drop(['index'], axis=1, inplace = True)\n",
    "    display(noCategory.head())\n",
    "    #display(noCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def flatten_volume_json(path_to_volume_json, csv_root=''):\n",
    "    '''\n",
    "    flattens JSON record for a volume into six separate CSVs (volume, entries, people, relationships, places, and events)\n",
    "        path_to_volume_json: path to a volume JSON record\n",
    "        csv_root: specify directory for CSV output, including trailing /\n",
    "    \n",
    "        returns: root directory for CSVs\n",
    "    '''    \n",
    "    \n",
    "    with open(path_to_volume_json, encoding=\"utf-8\") as jsonfile:\n",
    "        data = json.load(jsonfile)\n",
    "        \n",
    "    volume_id = data[\"volume\"][\"id\"]\n",
    "    \n",
    "    with open(csv_root + volume_id + \"_volume.csv\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        keys = 0\n",
    "        for key in data[\"volume\"]:\n",
    "            outfile.write(key)\n",
    "            keys += 1\n",
    "            if keys == len(data[\"volume\"]):\n",
    "                outfile.write('\\n')\n",
    "            else:\n",
    "                outfile.write(',')\n",
    "        keys = 0\n",
    "        for key in data[\"volume\"]:\n",
    "            outfile.write('\"' + data[\"volume\"][key] + '\"')            \n",
    "            keys += 1\n",
    "            if keys == len(data[\"volume\"]):\n",
    "                break\n",
    "            else:\n",
    "                outfile.write(',')\n",
    "                \n",
    "    with open(csv_root + volume_id + \"_entries.csv\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        outfile.write(\"entry id,entry text\\n\")\n",
    "        for image in data[\"images\"]:                        \n",
    "            image_id = volume_id + '-' + image[\"id\"]            \n",
    "            for entry in image[\"entries\"]:\n",
    "                entry_id = image_id + '-' + str(entry[\"id\"])\n",
    "                entry_text = entry[\"text\"]\n",
    "                outfile.write(entry_id + ',' + '\"' + entry_text + '\"\\n')\n",
    "                \n",
    "    with open(csv_root + volume_id + \"_people.csv\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        outfile.write(\"id,name,origin,ethnicity,age,legitimacy,occupation,phenotype,status,titles,ranks,references\\n\")\n",
    "        relationships = []\n",
    "        for person in data[\"people\"]:\n",
    "            for key in person:\n",
    "                if key == \"relationships\": \n",
    "                    if person[key] == None:\n",
    "                        continue\n",
    "                    for relationship in person[key]:                       \n",
    "                        if relationship[\"relationship_type\"] == \"godchild\":\n",
    "                            inverse_relationship_type = \"godparent\"\n",
    "                        elif relationship[\"relationship_type\"] == \"godparent\":\n",
    "                            inverse_relationship_type = \"godchild\"\n",
    "                        elif relationship[\"relationship_type\"] == \"grandparent\":\n",
    "                            inverse_relationship_type = \"grandchild\"\n",
    "                        elif relationship[\"relationship_type\"] == \"grandchild\":\n",
    "                            inverse_relationship_type = \"grandparent\"\n",
    "                        elif relationship[\"relationship_type\"] == \"parent\":\n",
    "                            inverse_relationship_type = \"child\"\n",
    "                        elif relationship[\"relationship_type\"] == \"child\":\n",
    "                            inverse_relationship_type = \"parent\"\n",
    "                        elif relationship[\"relationship_type\"] == \"slave\":\n",
    "                            inverse_relationship_type = \"enslaver\"\n",
    "                        elif relationship[\"relationship_type\"] == \"enslaver\":\n",
    "                            inverse_relationship_type = \"slave\"\n",
    "                        else:\n",
    "                            inverse_relationship_type = relationship[\"relationship_type\"]\n",
    "                            \n",
    "                        inverse_relationship = {\"from\": relationship[\"related_person\"], \"to\": person[\"id\"], \"type\": inverse_relationship_type}\n",
    "                        if not (inverse_relationship in relationships):\n",
    "                            relationships.append({\"from\": person[\"id\"], \"to\": relationship[\"related_person\"], \"type\": relationship[\"relationship_type\"]})\n",
    "                        \n",
    "                elif key == \"references\":\n",
    "                    references = person[key][0]\n",
    "                    for index in range(1, len(person[key])):\n",
    "                        references += ';' + person[key][index]\n",
    "                    outfile.write(references + '\\n')\n",
    "                elif person[key] == None:\n",
    "                    outfile.write(',')\n",
    "                else:\n",
    "                    outfile.write(person[key] + ',')\n",
    "                    \n",
    "    with open(csv_root + volume_id + \"_relationships.csv\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        outfile.write(\"from id,to id,relationship type\\n\")\n",
    "        for relationship in relationships:\n",
    "            outfile.write(relationship[\"from\"] + ',' + relationship[\"to\"] + ',' + relationship[\"type\"] + '\\n')\n",
    "            \n",
    "    with open(csv_root + volume_id + \"_places.csv\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        outfile.write(\"id,location\\n\")\n",
    "        for place in data[\"places\"]:\n",
    "            outfile.write(place[\"id\"] + ',' + place[\"location\"] + '\\n')\n",
    "            \n",
    "    with open(csv_root + volume_id + \"_events.csv\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        outfile.write(\"id,type,principal,date,location id,cleric\\n\")\n",
    "        for event in data[\"events\"]:\n",
    "            for key in event:\n",
    "                if event[key] == None:\n",
    "                    event[key] = ''\n",
    "            outfile.write(event[\"id\"] + ',' + event[\"type\"] + ',' + event[\"principal\"] + ',' + event[\"date\"] + ',' + event[\"location\"] + ',' + event[\"cleric\"] + '\\n')   \n",
    "                \n",
    "    return csv_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'volume_records/csv/'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "flatten_volume_json(\"volume_records/15834.json\", csv_root = \"volume_records/csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 12-ssda-xml-parser.ipynb.\n",
      "Converted 31-collate-xml-entities-spans.ipynb.\n",
      "Converted 33-split-data.ipynb.\n",
      "Converted 41-generic-framework-for-spacy-training.ipynb.\n",
      "Converted 42-initial-model.ipynb.\n",
      "Converted 51-data-preprocessing.ipynb.\n",
      "Converted 52-unstructured-to-markup.ipynb.\n",
      "Converted 53-markup-to-spatial-historian.ipynb.\n",
      "Converted 54-utility-functions.ipynb.\n",
      "Converted 61-prodigy-output-training-demo.ipynb.\n",
      "Converted 62-full-model-application-demo.ipynb.\n",
      "Converted 63-pt-model-training.ipynb.\n",
      "Converted 64-es-model-training.ipynb.\n",
      "Converted 65-all-annotations-model-training.ipynb.\n",
      "Converted 66-es-guatemala-model-training.ipynb.\n",
      "Converted 67-death-and-birth-records-together.ipynb.\n",
      "Converted 70-exhaustive-training.ipynb.\n",
      "Converted 71-relationship-builder.ipynb.\n",
      "Converted 72-full-volume-processor.ipynb.\n",
      "Converted 73-table-output.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
