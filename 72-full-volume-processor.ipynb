{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp full_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#dependencies\n",
    "\n",
    "#nlp packages\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "#manipulation of tables/arrays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "\n",
    "#internal imports\n",
    "from ssda_nlp.collate import *\n",
    "from ssda_nlp.split_data import *\n",
    "from ssda_nlp.modeling import *\n",
    "from ssda_nlp.model_performance_utils import *\n",
    "from ssda_nlp.xml_parser import *\n",
    "from ssda_nlp.unstructured2markup import *\n",
    "from ssda_nlp.utility import *\n",
    "from ssda_nlp.relationships import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def process_volume(path_to_transcription, path_to_model):\n",
    "    '''\n",
    "    runs the transcription of a single volume (formatted according to SSDA markup 2.0 specs) through the ML entity extraction\n",
    "    and rules-based relationship linking pipelines, then formats resulting data for export into SQL\n",
    "        path_to_transcription: path to an XML file containing the transcription of a single volume\n",
    "        path_to_model: path to a spaCy model trained to extract entities from the proper type of volume\n",
    "    \n",
    "        returns: final people, place, and event dictionaries as well as the\n",
    "        path to a JSON file containing volume metadata as well as people, place, and event records\n",
    "    '''\n",
    "    \n",
    "    #retrieve volume metadata and controlled vocabularies\n",
    "    \n",
    "    volume_metadata = retrieve_volume_metadata(path_to_transcription)\n",
    "    images = xml_v2_to_json(path_to_transcription)\n",
    "    vocabularies = retrieve_controlled_vocabularies()\n",
    "    \n",
    "    if volume_metadata[\"country\"] == \"Brazil\":\n",
    "        lang = \"pt\"\n",
    "        language = \"portuguese\"\n",
    "    else:\n",
    "        lang = \"es\"\n",
    "        language = \"spanish\"\n",
    "        \n",
    "    #load and apply trained model\n",
    "    \n",
    "    trained_model = load_model(path_to_model, language=lang, verbose='True')\n",
    "    \n",
    "    entry_df = parse_xml_v2(path_to_transcription)\n",
    "    \n",
    "    ent_preds_df, metrics_df, per_ent_metrics = test_model(trained_model, entry_df, \"entry_no\", \"text\", score_model=False)\n",
    "    print(\"Entities extracted.\")\n",
    "    \n",
    "    #development\n",
    "    #pd.set_option(\"display.max_rows\", 101)\n",
    "    #display(ent_preds_df.head(100))\n",
    "    \n",
    "    #iterate through each entry and build relationships\n",
    "    \n",
    "    people = []\n",
    "    places = []\n",
    "    events = []\n",
    "    \n",
    "    entitiesRunning = pd.DataFrame()\n",
    "    noCategoryRunning = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(entry_df.index)):\n",
    "        \n",
    "        entry_no = entry_df['entry_no'][i]\n",
    "        entry_text = entry_df['text'][i]    \n",
    "    \n",
    "        entities = ent_preds_df.loc[ent_preds_df['entry_no'] == entry_no]\n",
    "    \n",
    "        #Get the size \n",
    "        entities_shape = entities.shape\n",
    "        #Now define a column vector that is the approriate size, True by default\n",
    "        truths_list = [True] * entities_shape[0] #[0] is the number of rows\n",
    "        #Now add that column to entities\n",
    "        entities.insert(0, \"assgnmt_status\", truths_list)\n",
    "        \n",
    "        entry_people, entry_places, entry_events, entities, characteristics_df, categorized_characteristics = build_entry_metadata(entry_text, entities, path_to_transcription, entry_no)             \n",
    "        \n",
    "        \"\"\"\n",
    "        noCategory_df = copy.deepcopy(characteristics_df[characteristics_df['category'] == None])\n",
    "        if noCategory_df.shape[0]<1:\n",
    "            print(\"First pass, no categories found\")\n",
    "            noCategory_df = copy.deepcopy(characteristics_df[characteristics_df['category'] == \"None\"])\n",
    "        if noCategory_df.shape[0]<1:\n",
    "            noCategory_df = copy.deepcopy(characteristics_df[characteristics_df['category'] == characteristics_df['assignment']])\n",
    "        if noCategory_df.shape[0]<1:\n",
    "            print(\"No categories listed as false apparently\")\n",
    "            display(characteristics_df.head(10))\n",
    "        else:\n",
    "            print(\"None category found!\")\n",
    "        \"\"\"\n",
    "        \n",
    "        for ent_index, ent_row in entities.iterrows():\n",
    "            for char_index, char_row in characteristics_df.iterrows():\n",
    "                #If CATEGORY is unassigned:\n",
    "                if (char_row.loc[\"category\"] == None) and (ent_row.loc[\"pred_label\"] == char_row.loc[\"pred_label\"]) and (ent_row.loc[\"pred_start\"] == char_row.loc[\"pred_start\"]) and (ent_row.loc[\"pred_entity\"] == char_row.loc[\"pred_entity\"]):\n",
    "                    noCategoryRunning = noCategoryRunning.append(char_row)\n",
    "                #Catergory is assigned BUT CHARACTERISTICS IS UNASSIGNED\n",
    "                #(not (char_row.loc[\"category\"] == None)) and \n",
    "                elif (ent_row.loc[\"pred_label\"] == char_row.loc[\"pred_label\"]) and (ent_row.loc[\"pred_start\"] == char_row.loc[\"pred_start\"]) and (ent_row.loc[\"pred_entity\"] == char_row.loc[\"pred_entity\"]):\n",
    "                    if (char_row.loc[\"assignment\"] == None):\n",
    "                        entities.loc[ent_index,\"assgnmt_status\"] = False\n",
    "                        characteristics_df.loc[char_index,\"assgnmt_status\"] = False\n",
    "\n",
    "        #noCategoryRunning = noCategoryRunning.append(noCategory_df)\n",
    "        entitiesRunning = entitiesRunning.append(entities)   \n",
    "                \n",
    "        people += entry_people\n",
    "        places += entry_places\n",
    "        events += entry_events\n",
    "    \n",
    "    print(\"Relationships linked.\")\n",
    "    \n",
    "    #disambiguate locations and assign unique ids\n",
    "    \n",
    "    unique_places = []\n",
    "    for place in places:\n",
    "        if (place != None) and (place not in unique_places):\n",
    "            unique_places.append(place)\n",
    "            \n",
    "    for person in people:        \n",
    "        if (person[\"origin\"] != None) and (person[\"origin\"] not in unique_places):\n",
    "            unique_places.append(person[\"origin\"])\n",
    "    \n",
    "    places = []\n",
    "    curr_place = 1\n",
    "    for unique_place in unique_places:\n",
    "        place_record = {\"id\":volume_metadata[\"id\"] + '-L' + str(curr_place), \"location\":unique_place}\n",
    "        places.append(place_record)\n",
    "        curr_place += 1\n",
    "        \n",
    "    #incorporate location ids into event metadata and person records\n",
    "    \n",
    "    for event in events:\n",
    "        location = event[\"location\"]\n",
    "        loc_id = \"unknown\"\n",
    "        if location != None:\n",
    "            for place in places:\n",
    "                if place[\"location\"] == location:\n",
    "                    loc_id = place[\"id\"]\n",
    "        if (loc_id == \"unknown\") and (location != None):\n",
    "            print(\"Failed to find location ID for \" + location)\n",
    "            event[\"location\"] = None\n",
    "        else:\n",
    "            event[\"location\"] = loc_id\n",
    "            \n",
    "        if event[\"location\"] == \"unknown\":\n",
    "            event[\"location\"] = None\n",
    "            \n",
    "    for person in people:\n",
    "        if person[\"origin\"] == None:\n",
    "            continue\n",
    "        \n",
    "        for place in places:\n",
    "            if place[\"location\"] == person[\"origin\"]:\n",
    "                person[\"origin\"] = place[\"id\"]\n",
    "                break\n",
    "        \n",
    "    #bracket missing or incomplete event dates\n",
    "    \n",
    "    incomplete_dates = []\n",
    "    last_year = None\n",
    "    last_month = None\n",
    "    last_day = None\n",
    "    \n",
    "    for e in range(len(events)):\n",
    "        curr_year = events[e][\"date\"][:4]\n",
    "        curr_month = events[e][\"date\"][5:7]\n",
    "        curr_day = events[e][\"date\"][8:]\n",
    "        \n",
    "        #fix incompletely extracted years\n",
    "        if (curr_year != \"????\") and (last_year != None) and (abs(int(curr_year) - int(last_year)) > 1):\n",
    "            if (curr_year[3] == last_year[3]):\n",
    "                curr_year = last_year                \n",
    "            elif (curr_month == \"01\") and (last_month == \"12\"):\n",
    "                curr_year = str(int(last_year) + 1)                \n",
    "            else:\n",
    "                curr_year = last_year\n",
    "            events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day\n",
    "        \n",
    "        if (curr_year == \"????\") or (curr_month == \"??\") or (curr_day == \"??\"):\n",
    "            #logic to assign dates for birth events based on associated baptism\n",
    "            if events[e][\"type\"] == \"birth\":\n",
    "                if (events[e][\"id\"][:events[e][\"id\"].find('E')] == events[e - 1][\"id\"][:events[e - 1][\"id\"].find('E')]) and (events[e - 1][\"type\"] == \"baptism\") and ('?' not in events[e - 1][\"date\"]):\n",
    "                        if (curr_month != \"??\") and (curr_day != \"??\"):\n",
    "                            if (curr_month == \"12\") and (last_month == \"01\"):\n",
    "                                curr_year = str(int(last_year) - 1)                                \n",
    "                            elif (30 * int(last_month) + int(last_day) - 30 * int(curr_month) - int(curr_day)) < 21:\n",
    "                                curr_year = last_year\n",
    "                            events[e][\"date\"] = curr_year + '-' + events[e][\"date\"][5:7] + '-' + events[e][\"date\"][8:]\n",
    "                        elif curr_month != \"??\":\n",
    "                            if (curr_month == \"12\"):\n",
    "                                curr_day = \"01\"\n",
    "                                curr_year = str(int(last_year) - 1)\n",
    "                                events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-01-01'\n",
    "                            elif (curr_month == last_month):\n",
    "                                curr_day = \"01\"\n",
    "                                curr_year = last_year\n",
    "                                events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-' + last_month + '-' + last_day\n",
    "                            elif int(curr_month) == (int(last_month) - 1):\n",
    "                                curr_day = \"01\"\n",
    "                                curr_year = last_year\n",
    "                                events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-' + last_month + '-01'                            \n",
    "                        elif curr_day != \"??\":\n",
    "                            if curr_day <= last_day:\n",
    "                                curr_year = last_year\n",
    "                                curr_month = last_month                                \n",
    "                            else:\n",
    "                                if last_month == \"01\":\n",
    "                                    curr_month = \"12\"\n",
    "                                    curr_year = str(int(last_year) - 1)\n",
    "                                else:\n",
    "                                    curr_month = str(int(last_month) - 1)                                    \n",
    "                                    if len(curr_month) < 2:\n",
    "                                        curr_month = '0' + curr_month\n",
    "                                    curr_year = last_year\n",
    "                            events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day\n",
    "                        else:\n",
    "                            if (last_month == '01') and (int(last_day) < 21):\n",
    "                                curr_year = str(int(last_year) - 1)\n",
    "                                curr_month = \"12\"\n",
    "                                curr_day = str(int(last_day) + 9)                               \n",
    "                            elif int(last_day) < 21:\n",
    "                                curr_year = last_year\n",
    "                                curr_month = str(int(last_month) - 1)\n",
    "                                if len(curr_month) < 2:\n",
    "                                    curr_month = '0' + curr_month\n",
    "                                curr_day = str(int(last_day) + 9)\n",
    "                            else:\n",
    "                                curr_year = last_year\n",
    "                                curr_month = last_month\n",
    "                                curr_day = str(int(last_day) - 20)\n",
    "                                if len(curr_day) < 2:\n",
    "                                    curr_day = '0' + curr_day\n",
    "                            events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-' + last_month + '-' + last_day\n",
    "                            \n",
    "            if (curr_year == \"????\") or (curr_month == \"??\") or (curr_day == \"??\"):\n",
    "                incomplete_dates.append(e)\n",
    "        elif last_year == None:\n",
    "            for date in incomplete_dates:\n",
    "                events[date][\"date\"] = complete_date(events[date][\"date\"], None, curr_year + '-' + curr_month + '-' + curr_day)\n",
    "            \n",
    "            incomplete_dates = []\n",
    "            last_year = curr_year\n",
    "            last_month = curr_month\n",
    "            last_day = curr_day\n",
    "        elif (compare_dates(int(curr_year), int(curr_month), int(curr_day), int(last_year), int(last_month), int(last_day)) == '>') or (compare_dates(int(curr_year), int(curr_month), int(curr_day), int(last_year), int(last_month), int(last_day)) == '='):\n",
    "            for date in incomplete_dates:\n",
    "                events[date][\"date\"] = complete_date(events[date][\"date\"], last_year + '-' + last_month + '-' + last_day, curr_year + '-' + curr_month + '-' + curr_day)\n",
    "            \n",
    "            incomplete_dates = []\n",
    "            last_year = curr_year\n",
    "            last_month = curr_month\n",
    "            last_day = curr_day                    \n",
    "    \n",
    "    if last_year != None:\n",
    "        for date in incomplete_dates:\n",
    "            events[date][\"date\"] = complete_date(events[date][\"date\"], last_year + '-' + last_month + '-' + last_day, None)\n",
    "        \n",
    "    #merging any date brackets with equal endpoints\n",
    "    for event in events:\n",
    "        interval = event[\"date\"].split('/')\n",
    "        if (len(interval) == 2) and (interval[0] == interval[1]):\n",
    "            event[\"date\"] == interval[0]            \n",
    "        \n",
    "    print(\"Events configured.\")    \n",
    "    \n",
    "    for person in people:        \n",
    "        #strip titles and/or ranks from names\n",
    "        if person[\"name\"] != None:\n",
    "            name_parts = person[\"name\"].split(' ')\n",
    "\n",
    "            if len(name_parts) >= 2:\n",
    "                while ((name_parts[0].lower() + ' ' + name_parts[1].lower()) in vocabularies[\"titles\"]) or ((name_parts[0].lower() + ' ' + name_parts[1].lower()) in vocabularies[\"ranks\"]):\n",
    "                    if len(name_parts) == 2:\n",
    "                        person[\"name\"] = None\n",
    "                    else:\n",
    "                        person[\"name\"] = name_parts[2]\n",
    "                        for i in range(3, len(name_parts)):\n",
    "                            person[\"name\"] += ' ' + name_parts[i]\n",
    "\n",
    "                    if (name_parts[0].lower() + ' ' + name_parts[1].lower()) in vocabularies[\"titles\"]:\n",
    "                        if person[\"titles\"] != None:\n",
    "                            person[\"titles\"] += ';' + name_parts[0] + ' ' + name_parts[1]\n",
    "                        else:\n",
    "                            person[\"titles\"] = name_parts[0] + ' ' + name_parts[1]\n",
    "                    else:\n",
    "                        if person[\"ranks\"] != None:\n",
    "                            person[\"ranks\"] += ';' + name_parts[0] + ' ' + name_parts[1]\n",
    "                        else:\n",
    "                            person[\"ranks\"] = name_parts[0] + ' ' + name_parts[1]\n",
    "\n",
    "                    if person[\"name\"] == None:\n",
    "                        break\n",
    "                    name_parts = person[\"name\"].split(' ')\n",
    "                    if len(name_parts) < 2:\n",
    "                        break\n",
    "\n",
    "            if person[\"name\"] != None:\n",
    "                while (name_parts[0].lower() in vocabularies[\"titles\"]) or (name_parts[0].lower() in vocabularies[\"ranks\"]):\n",
    "                    if len(name_parts) == 1:\n",
    "                        person[\"name\"] = None\n",
    "                    else:\n",
    "                        person[\"name\"] = name_parts[1]\n",
    "                        for i in range(2, len(name_parts)):\n",
    "                            person[\"name\"] += ' ' + name_parts[i]\n",
    "\n",
    "                    if name_parts[0].lower() in vocabularies[\"titles\"]:\n",
    "                        if person[\"titles\"] != None:\n",
    "                            person[\"titles\"] += ';' + name_parts[0]\n",
    "                        else:\n",
    "                            person[\"titles\"] = name_parts[0]\n",
    "                    else:\n",
    "                        if person[\"ranks\"] != None:\n",
    "                            person[\"ranks\"] += ';' + name_parts[0]\n",
    "                        else:\n",
    "                            person[\"ranks\"] = name_parts[0]\n",
    "\n",
    "                    if person[\"name\"] == None:\n",
    "                        break\n",
    "                    name_parts = person[\"name\"].split(' ')\n",
    "                    \n",
    "    #normalize names and all characteristics\n",
    "    names = []\n",
    "    name_counts = []\n",
    "    ethnonym_vocab = retrieve_json_vocab(\"synonyms.json\", \"ethnonyms\")\n",
    "    phenotype_vocab = retrieve_json_vocab(\"synonyms.json\", \"phenotypes\", language=\"spanish\")\n",
    "    \n",
    "    for person in people:\n",
    "        #normalize characteristics and translate to English\n",
    "        for key in person:\n",
    "            if person[key] == None:\n",
    "                continue\n",
    "            if key == \"name\":\n",
    "                person[key] = normalize_text(person[key], \"synonyms.json\", context=\"name\")\n",
    "                #check extracted name for ethnonyms and/or attributed phenotypes        \n",
    "                if (person[\"name\"] != None) and (person[\"name\"] != normalize_text(person[\"name\"], \"synonyms.json\", context=\"ethnonym\")):\n",
    "                    for token in person[\"name\"].split(' '):\n",
    "                        eth_norm = normalize_text(token, \"synonyms.json\", context=\"ethnonym\")\n",
    "                        if token != eth_norm:\n",
    "                            if (person[\"ethnicities\"] == None) or (not (eth_norm in person[\"ethnicities\"])):\n",
    "                                if person[\"ethnicities\"] == None:\n",
    "                                    person[\"ethnicities\"] = eth_norm\n",
    "                                else:\n",
    "                                    person[\"ethnicities\"] = person[\"ethnicities\"] + ';' + eth_norm\n",
    "                    person[\"name\"] = normalize_text(person[\"name\"], \"synonyms.json\", context=\"ethnonym\")\n",
    "                else:\n",
    "                    for ethnonym in ethnonym_vocab:\n",
    "                        if ethnonym in person[\"name\"]:\n",
    "                            if person[\"ethnicities\"] == None:\n",
    "                                person[\"ethnicities\"] = ethnonym\n",
    "                            else:\n",
    "                                person[\"ethnicities\"] = person[\"ethnicities\"] + ';' + ethnonym\n",
    "                for phenotype in phenotype_vocab:\n",
    "                    if phenotype in normalize_text(person[key], \"synonyms.json\", context=\"characteristic\"):                    \n",
    "                        if person[\"phenotype\"] == None:\n",
    "                            person[\"phenotype\"] = phenotype\n",
    "                        else:\n",
    "                            person[\"phenotype\"] = person[\"phenotype\"] + ';' + phenotype\n",
    "                        if phenotype[-1] == 's':\n",
    "                            for token in person[\"name\"].split(' '):\n",
    "                                if normalize_text(token, \"synonyms.json\", context=\"characteristic\") == phenotype:\n",
    "                                    person[\"name\"] = person[\"name\"].replace(' ' + token, '')\n",
    "            elif key == \"ethnicities\":                \n",
    "                if person[key].find(';') == -1:\n",
    "                    person[key] = normalize_text(person[key], \"synonyms.json\", context=\"ethnonym\")                    \n",
    "                else:\n",
    "                    char_comp = person[key].split(';')\n",
    "                    person[key] = \"\"\n",
    "                    #strip out duplicate characteristics\n",
    "                    for char in char_comp:\n",
    "                        char = normalize_text(char, \"synonyms.json\", context=\"ethnonym\")                       \n",
    "                                          \n",
    "                        if not (char in person[key]):\n",
    "                            if person[key] == \"\":\n",
    "                                person[key] = char\n",
    "                            else:\n",
    "                                person[key] = person[key] + ';' + char\n",
    "            elif (key != \"id\") and (key != \"relationships\"):\n",
    "                if person[key].find(';') == -1:\n",
    "                    person[key] = normalize_text(person[key], \"synonyms.json\", context=\"characteristic\")\n",
    "                    person[key] = translate_characteristic(person[key], \"synonyms.json\", language)\n",
    "                else:\n",
    "                    char_comp = person[key].split(';')\n",
    "                    person[key] = \"\"\n",
    "                    #strip out duplicate characteristics\n",
    "                    for char in char_comp:\n",
    "                        char = normalize_text(char, \"synonyms.json\", context=\"characteristic\")                        \n",
    "                        char = translate_characteristic(char, \"synonyms.json\", language)                        \n",
    "                        if not (char in person[key]):\n",
    "                            if person[key] == \"\":\n",
    "                                person[key] = char\n",
    "                            else:\n",
    "                                person[key] = person[key] + ';' + char           \n",
    "        \n",
    "        #future improvement: find additional references for plural characteristics\n",
    "        \n",
    "        #count name frequency\n",
    "        if person[\"name\"] != None:\n",
    "            if person[\"name\"] in names:\n",
    "                name_counts[names.index(person['name'])] += 1\n",
    "            else:\n",
    "                names.append(person[\"name\"])\n",
    "                name_counts.append(1)   \n",
    "    \n",
    "    #disambiguate and merge people across the volume\n",
    "    redundant_records = []\n",
    "    merged_records = []    \n",
    "    for i in range(len(name_counts)):\n",
    "        if (name_counts[i] > .1 * len(images)) and (len(names[i].split(' ')) > 1) and (names[i] != \"Unknown principal\"):\n",
    "            records_to_merge = []            \n",
    "            for j in range(len(people)):\n",
    "                if people[j][\"name\"] == names[i]:\n",
    "                    redundant_records.append(people[j])\n",
    "                    records_to_merge.append(people[j])                    \n",
    "            merged_records.append(merge_records(records_to_merge))            \n",
    "    people = [person for person in people if person not in redundant_records]\n",
    "    for person in merged_records:\n",
    "        people.append(person)    \n",
    "    \n",
    "    print(\"People records enhanced and disambiguated.\")\n",
    "    \n",
    "    #reduce compound person IDs to single ID, add references field\n",
    "    people, events = compact_references(people, events)\n",
    "    \n",
    "    print(\"Single ID generated for each individual.\")\n",
    "    \n",
    "    #convert dictionaries into JSON    \n",
    "    with open(\"volume_records\\\\\" + volume_metadata[\"id\"] + \".json\", \"w\") as outfile:\n",
    "        outfile.write('{\\n\\\"volume\\\": \\n')\n",
    "        json.dump(volume_metadata, outfile)\n",
    "        outfile.write(',')\n",
    "        outfile.write('\\n\\\"images\\\": [\\n')\n",
    "        first_img = True\n",
    "        for image in images:\n",
    "            if first_img:\n",
    "                first_img = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")\n",
    "            json.dump(image, outfile)\n",
    "        outfile.write(\"\\n],\\n\")\n",
    "        outfile.write('\\n\\\"people\\\": [\\n')\n",
    "        first_person = True\n",
    "        for person in people:\n",
    "            if first_person:\n",
    "                first_person = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")            \n",
    "            json.dump(person, outfile)            \n",
    "        outfile.write(\"\\n],\\n\")\n",
    "        outfile.write(\"\\\"places\\\": [\\n\")\n",
    "        first_place = True\n",
    "        for place in places:\n",
    "            if first_place:\n",
    "                first_place = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")\n",
    "            json.dump(place, outfile)\n",
    "        outfile.write(\"\\n],\\n\")\n",
    "        outfile.write(\"\\\"events\\\": [\\n\")\n",
    "        first_event = True\n",
    "        for event in events:\n",
    "            if first_event:\n",
    "                first_event = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")\n",
    "            json.dump(event, outfile)\n",
    "        outfile.write(\"\\n]\\n\")\n",
    "        outfile.write('}')\n",
    "            \n",
    "    print(\"JSON built, processing completed.\")\n",
    "            \n",
    "    return people, places, events, volume_metadata[\"id\"] + \"_ppe.json\", entitiesRunning, noCategoryRunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14193\\anaconda3\\lib\\site-packages\\spacy\\util.py:707: UserWarning: [W095] Model 'en_pipeline' (0.0.0) requires spaCy >=3.0.5,<3.1.0 and is incompatible with the current version (3.0.3). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'models/15834'\n",
      "Entities extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14193\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find a category for relationship: hija\n",
      "Failed to find a category for relationship: Padri no\n",
      "Failed to find a category for relationship: hija\n",
      "Failed to find a category for relationship: hija\n",
      "Failed to find a category for relationship: Po\n",
      "Failed to find a category for relationship: hijo\n",
      "Failed to find a category for relationship: P el\n",
      "Failed to find a category for relationship: PP\n",
      "Failed to find a category for relationship: h.\n",
      "Failed to find a category for relationship: M.\n",
      "Failed to find a category for relationship: hijo\n",
      "Failed to find a category for relationship: Pa drino\n",
      "Failed to find a category for relationship: hija\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-546fa4eacd37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#no_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpeople\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoCategory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_volume\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"transcriptions\\\\15834.xml\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"models/15834\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-46ebb24d6606>\u001b[0m in \u001b[0;36mprocess_volume\u001b[1;34m(path_to_transcription, path_to_model)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mentities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"assgnmt_status\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruths_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mentry_people\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry_places\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry_events\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharacteristics_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorized_characteristics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_entry_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_transcription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \"\"\"\n",
      "\u001b[1;32m~\\Desktop\\MetaFolder\\SSDA\\ssda-nlp\\ssda_nlp\\relationships.py\u001b[0m in \u001b[0;36mbuild_entry_metadata\u001b[1;34m(entry_text, entities, path_to_volume_xml, entry_number)\u001b[0m\n\u001b[0;32m   1524\u001b[0m     \u001b[0mevents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1526\u001b[1;33m     \u001b[0mvolume_metadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mretrieve_volume_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_volume_xml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1527\u001b[0m     \u001b[0mpeople_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pred_label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'PER'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1528\u001b[0m     \u001b[0mpeople_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MetaFolder\\SSDA\\ssda-nlp\\ssda_nlp\\xml_parser.py\u001b[0m in \u001b[0;36mretrieve_volume_metadata\u001b[1;34m(path_to_xml)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mretrieve_volume_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_xml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m     \u001b[0mxml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_xml\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m     \u001b[0mvolume_metadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[0mmetadata_fields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"country\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"state\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"city\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"institution\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"title\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[0mbyte\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[1;31m# undecoded input that is kept between calls to decode()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "people, places, events, json_path, entities, noCategory = process_volume(\"transcriptions\\\\15834.xml\", \"models/15834\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>assgnmt_status</th>\n",
       "      <th>entry_no</th>\n",
       "      <th>pred_entity</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>pred_start</th>\n",
       "      <th>pred_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1033-1</td>\n",
       "      <td>Juana</td>\n",
       "      <td>PER</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  assgnmt_status entry_no pred_entity pred_label pred_start pred_end\n",
       "0      0            True   1033-1       Juana        PER         10       15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14135, 7)\n",
      "------------------------------------------------\n",
      "Unassigned CHAR/RELs found:\n",
      "char_df shape: (1010, 7)\n",
      "rel_df shape: (172, 7)\n",
      "unassigned_df shape: (1182, 6)\n",
      "------------------------------------------------\n",
      "unassigned_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assgnmt_status</th>\n",
       "      <th>entry_no</th>\n",
       "      <th>pred_entity</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>pred_start</th>\n",
       "      <th>pred_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>1035-4</td>\n",
       "      <td>esclavo</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>284</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>1038-3</td>\n",
       "      <td>Libre</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>1038-3</td>\n",
       "      <td>libre</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>168</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>1038-3</td>\n",
       "      <td>h[ocultado]</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>210</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>1038-3</td>\n",
       "      <td>libres</td>\n",
       "      <td>CHAR</td>\n",
       "      <td>286</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   assgnmt_status entry_no  pred_entity pred_label pred_start pred_end\n",
       "0           False   1035-4      esclavo       CHAR        284      291\n",
       "1           False   1038-3        Libre       CHAR         28       33\n",
       "2           False   1038-3        libre       CHAR        168      173\n",
       "3           False   1038-3  h[ocultado]       CHAR        210      221\n",
       "4           False   1038-3       libres       CHAR        286      292"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noCategory\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['assgnmt_status'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-066254ff7477>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No characteristics without categories found...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mnoCategory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'assgnmt_status'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Just a bunch of 1s for some reason\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mnoCategory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'level_0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Also just a bunch of 1s for some reason\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mnoCategory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3995\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3996\u001b[0m             \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3997\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3998\u001b[0m         )\n\u001b[0;32m   3999\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3934\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3935\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3936\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3938\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3968\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3969\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3970\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3971\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5016\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5017\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5018\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5019\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5020\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['assgnmt_status'] not found in axis\""
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "print(\"entities:\")\n",
    "display(entities.head(1))\n",
    "print(entities.shape)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "char_df = entities[(entities['pred_label'] == 'CHAR')]\n",
    "char_df = copy.deepcopy(char_df[char_df['assgnmt_status'] == False])\n",
    "rel_df = entities[(entities['pred_label'] == 'REL')]\n",
    "rel_df = copy.deepcopy(rel_df[rel_df['assgnmt_status'] == False])\n",
    "\n",
    "unassigned_df = char_df.append(rel_df)\n",
    "unassigned_df.sort_values(by='index', inplace = True)\n",
    "unassigned_df.drop(['index'], axis=1, inplace = True)\n",
    "unassigned_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "print(\"Unassigned CHAR/RELs found:\")\n",
    "print(\"char_df shape: \" + str(char_df.shape))\n",
    "print(\"rel_df shape: \" + str(rel_df.shape))\n",
    "print(\"unassigned_df shape: \" + str(unassigned_df.shape))\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "print(\"unassigned_df\")\n",
    "if unassigned_df.shape[0]<1:\n",
    "    print(\"No unassigned characteristics or relationships found...\")\n",
    "else:\n",
    "    display(unassigned_df.head())\n",
    "    \n",
    "print(\"noCategory\")\n",
    "if noCategory.shape[0]<1:\n",
    "    print(\"No characteristics without categories found...\")\n",
    "else:\n",
    "    noCategory.drop(['assgnmt_status'], axis=1, inplace = True) #Just a bunch of 1s for some reason\n",
    "    noCategory.drop(['level_0'], axis=1, inplace = True) #Also just a bunch of 1s for some reason\n",
    "    noCategory.reset_index(inplace = True)\n",
    "    noCategory.drop(['index'], axis=1, inplace = True)\n",
    "    display(noCategory.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def flatten_volume_json(path_to_volume_json, csv_root=''):\n",
    "    '''\n",
    "    flattens JSON record for a volume into six separate CSVs (volume, entries, people, relationships, places, and events)\n",
    "        path_to_volume_json: path to a volume JSON record\n",
    "        csv_root: specify directory for CSV output, including trailing /\n",
    "    \n",
    "        returns: root directory for CSVs\n",
    "    '''    \n",
    "    \n",
    "    with open(path_to_volume_json, encoding=\"utf-8\") as jsonfile:\n",
    "        data = json.load(jsonfile)\n",
    "        \n",
    "    volume_id = data[\"volume\"][\"id\"]\n",
    "    \n",
    "    with open(csv_root + volume_id + \"_volume.csv\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        keys = 0\n",
    "        for key in data[\"volume\"]:\n",
    "            outfile.write(key)\n",
    "            keys += 1\n",
    "            if keys == len(data[\"volume\"]):\n",
    "                outfile.write('\\n')\n",
    "            else:\n",
    "                outfile.write(',')\n",
    "        keys = 0\n",
    "        for key in data[\"volume\"]:\n",
    "            outfile.write('\"' + data[\"volume\"][key] + '\"')            \n",
    "            keys += 1\n",
    "            if keys == len(data[\"volume\"]):\n",
    "                break\n",
    "            else:\n",
    "                outfile.write(',')\n",
    "                \n",
    "    with open(csv_root + volume_id + \"_entries.csv\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        outfile.write(\"entry id,entry text\\n\")\n",
    "        for image in data[\"images\"]:                        \n",
    "            image_id = volume_id + '-' + image[\"id\"]            \n",
    "            for entry in image[\"entries\"]:\n",
    "                entry_id = image_id + '-' + str(entry[\"id\"])\n",
    "                entry_text = entry[\"text\"]\n",
    "                outfile.write(entry_id + ',' + '\"' + entry_text + '\"\\n')\n",
    "                \n",
    "    with open(csv_root + volume_id + \"_people.csv\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        outfile.write(\"id,name,origin,ethnicity,age,legitimacy,occupation,phenotype,status,titles,ranks,references\\n\")\n",
    "        relationships = []\n",
    "        for person in data[\"people\"]:\n",
    "            for key in person:\n",
    "                if key == \"relationships\": \n",
    "                    if person[key] == None:\n",
    "                        continue\n",
    "                    for relationship in person[key]:                       \n",
    "                        if relationship[\"relationship_type\"] == \"godchild\":\n",
    "                            inverse_relationship_type = \"godparent\"\n",
    "                        elif relationship[\"relationship_type\"] == \"godparent\":\n",
    "                            inverse_relationship_type = \"godchild\"\n",
    "                        elif relationship[\"relationship_type\"] == \"grandparent\":\n",
    "                            inverse_relationship_type = \"grandchild\"\n",
    "                        elif relationship[\"relationship_type\"] == \"grandchild\":\n",
    "                            inverse_relationship_type = \"grandparent\"\n",
    "                        elif relationship[\"relationship_type\"] == \"parent\":\n",
    "                            inverse_relationship_type = \"child\"\n",
    "                        elif relationship[\"relationship_type\"] == \"child\":\n",
    "                            inverse_relationship_type = \"parent\"\n",
    "                        elif relationship[\"relationship_type\"] == \"slave\":\n",
    "                            inverse_relationship_type = \"enslaver\"\n",
    "                        elif relationship[\"relationship_type\"] == \"enslaver\":\n",
    "                            inverse_relationship_type = \"slave\"\n",
    "                        else:\n",
    "                            inverse_relationship_type = relationship[\"relationship_type\"]\n",
    "                            \n",
    "                        inverse_relationship = {\"from\": relationship[\"related_person\"], \"to\": person[\"id\"], \"type\": inverse_relationship_type}\n",
    "                        if not (inverse_relationship in relationships):\n",
    "                            relationships.append({\"from\": person[\"id\"], \"to\": relationship[\"related_person\"], \"type\": relationship[\"relationship_type\"]})\n",
    "                        \n",
    "                elif key == \"references\":\n",
    "                    references = person[key][0]\n",
    "                    for index in range(1, len(person[key])):\n",
    "                        references += ';' + person[key][index]\n",
    "                    outfile.write(references + '\\n')\n",
    "                elif person[key] == None:\n",
    "                    outfile.write(',')\n",
    "                else:\n",
    "                    outfile.write(person[key] + ',')\n",
    "                    \n",
    "    with open(csv_root + volume_id + \"_relationships.csv\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        outfile.write(\"from id,to id,relationship type\\n\")\n",
    "        for relationship in relationships:\n",
    "            outfile.write(relationship[\"from\"] + ',' + relationship[\"to\"] + ',' + relationship[\"type\"] + '\\n')\n",
    "            \n",
    "    with open(csv_root + volume_id + \"_places.csv\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        outfile.write(\"id,location\\n\")\n",
    "        for place in data[\"places\"]:\n",
    "            outfile.write(place[\"id\"] + ',' + place[\"location\"] + '\\n')\n",
    "            \n",
    "    with open(csv_root + volume_id + \"_events.csv\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        outfile.write(\"id,type,principal,date,location id,cleric\\n\")\n",
    "        for event in data[\"events\"]:\n",
    "            for key in event:\n",
    "                if event[key] == None:\n",
    "                    event[key] = ''\n",
    "            outfile.write(event[\"id\"] + ',' + event[\"type\"] + ',' + event[\"principal\"] + ',' + event[\"date\"] + ',' + event[\"location\"] + ',' + event[\"cleric\"] + '\\n')   \n",
    "                \n",
    "    return csv_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'volume_records/csv/'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "flatten_volume_json(\"volume_records/15834.json\", csv_root = \"volume_records/csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 12-ssda-xml-parser.ipynb.\n",
      "Converted 31-collate-xml-entities-spans.ipynb.\n",
      "Converted 33-split-data.ipynb.\n",
      "Converted 41-generic-framework-for-spacy-training.ipynb.\n",
      "Converted 42-initial-model.ipynb.\n",
      "Converted 51-data-preprocessing.ipynb.\n",
      "Converted 52-unstructured-to-markup.ipynb.\n",
      "Converted 53-markup-to-spatial-historian.ipynb.\n",
      "Converted 54-utility-functions.ipynb.\n",
      "Converted 61-prodigy-output-training-demo.ipynb.\n",
      "Converted 62-full-model-application-demo.ipynb.\n",
      "Converted 63-pt-model-training.ipynb.\n",
      "Converted 64-es-model-training.ipynb.\n",
      "Converted 65-all-annotations-model-training.ipynb.\n",
      "Converted 66-es-guatemala-model-training.ipynb.\n",
      "Converted 67-death-and-birth-records-together.ipynb.\n",
      "Converted 70-exhaustive-training.ipynb.\n",
      "Converted 71-relationship-builder.ipynb.\n",
      "Converted 72-full-volume-processor.ipynb.\n",
      "Converted 73-table-output.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
