{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp full_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#dependencies\n",
    "\n",
    "#nlp packages\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "#manipulation of tables/arrays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "\n",
    "import difflib\n",
    "\n",
    "#internal imports\n",
    "from ssda_nlp.collate import *\n",
    "from ssda_nlp.split_data import *\n",
    "from ssda_nlp.modeling import *\n",
    "from ssda_nlp.model_performance_utils import *\n",
    "from ssda_nlp.xml_parser import *\n",
    "from ssda_nlp.unstructured2markup import *\n",
    "from ssda_nlp.utility import *\n",
    "from ssda_nlp.relationships import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def check_unassigned(entities, isVerbose=0):\n",
    "    char_df = entities[(entities['pred_label'] == 'CHAR')]\n",
    "    char_df = copy.deepcopy(char_df[char_df['assigned'] == False])\n",
    "    rel_df = entities[(entities['pred_label'] == 'REL')]\n",
    "    rel_df = copy.deepcopy(rel_df[rel_df['assigned'] == False])\n",
    "\n",
    "    unassigned_df = char_df.append(rel_df)\n",
    "    unassigned_df.sort_values(by='index', inplace = True)\n",
    "    unassigned_df.drop(['index'], axis=1, inplace = True)\n",
    "    unassigned_df.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    if isVerbose:\n",
    "        print(\"entities:\")\n",
    "        display(entities.head(1))\n",
    "        print(entities.shape)\n",
    "        print(\"------------------------------------------------\")\n",
    "        \n",
    "        print(\"Unassigned CHAR/RELs found:\")\n",
    "        print(\"char_df shape: \" + str(char_df.shape))\n",
    "        print(\"rel_df shape: \" + str(rel_df.shape))\n",
    "        print(\"unassigned_df shape: \" + str(unassigned_df.shape))\n",
    "        print(\"------------------------------------------------\")\n",
    "\n",
    "        print(\"unassigned_df\")\n",
    "        if unassigned_df.shape[0] < 1:\n",
    "            print(\"No unassigned characteristics or relationships found...\")\n",
    "        else:\n",
    "            display(unassigned_df.head())\n",
    "\n",
    "        print(\"noCategory\")\n",
    "        if noCategory.shape[0] < 1:\n",
    "            print(\"No characteristics without categories found...\")\n",
    "        else:\n",
    "            display(noCategory.head())\n",
    "            \n",
    "    return unassigned_df\n",
    "\n",
    "def check_lengths(name_list, idx, idx2):\n",
    "    name1 = name_list[idx]\n",
    "    name2 = name_list[idx2]\n",
    "    if len(name1)<=(len(name2)+1) and len(name1)>=(len(name2)-1):\n",
    "        my_ratio = int(difflib.SequenceMatcher(None, name1, name2).ratio()*100)\n",
    "        if my_ratio > 80:\n",
    "            #Then these could potentially be the same name but with small differences\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_test\n",
    "\n",
    "#Original pseudo code for validation:\n",
    "\n",
    "#if entry_type == \"baptism\":\n",
    "        #is there a baptism event?\n",
    "        #if so:\n",
    "            #does baptism event have a complete date?\n",
    "            #does baptism event have a location?\n",
    "        \n",
    "        #is there an identified cleric?\n",
    "        \n",
    "        #is there an identified principal?\n",
    "        #if so:\n",
    "            #are principal's godparent(s) identified?\n",
    "        #if so, and if principal is an infant:                \n",
    "            #are principal's parent(s) identified?\n",
    "            #is there a birth event?\n",
    "            #if so:\n",
    "                #does birth event have a complete date?\n",
    "                #does birth event have a location?\n",
    "        #if so, and if principal is enslaved:\n",
    "            #is principal's enslaver identified?\n",
    "            \n",
    "        #other questions re people:\n",
    "        #are there any entities labeled PER who do not have an explicit role in the baptism (e.g. principal, cleric, parent, enslaver, etc.)\n",
    "        #are there likely couples (e.g. parents, godparents) not explicitly flagged as such\n",
    "        #are there any people with very similar names that still appear separately\n",
    "        \n",
    "        #questions re characteristics:\n",
    "        #are there characteristics that were not categorized\n",
    "        #are there characteristics or relationships that were not assigned\n",
    "        #for ethnicities that were assigned, confirm that they were *not* assigned to slaveholders or clerics \n",
    "        \n",
    "#My pseudocode was misleading, the cleric check is independent of principal sub-checks (I adjusted)\n",
    "#Is it possible for principal/cleric to be assigned in the absence of a defined event? Is it possible for there not to be a defined event? Results here make it look like the answer to one or both of these is no (why? I need to check). If/when the answer to one or both is yes, validate_entry will need to be refactored (since it will break due to undefined vars).\n",
    "#I think that the issue with hasEnslaver is that the previous approach to baptism_princ was typically grabbing the reference to the principal at the beginning of each entry, whereas a subsequent reference was typically the one that was linked to the enslaver. If so, this should be fixed by the above.\n",
    "#              Spoiler: it didn’t\n",
    "#General note: some of the “desirable” checks are going to be low b/c they are conditional to start with (i.e. we don’t care if there isn’t a birth event if the principal is an adult).\n",
    "#I think there’s a problem with the hasWrongEthAssgnt_ensl check. As previously written, I believe that it flags if the person in question is an enslaver and that person’s slave has an ethnicity. I’ve flipped the second condition but left the original statement commented in case I’ve missed something b/c it doesn’t look like it was flagging as much as I would expect if it was incorrect in the way that I’m interpreting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def validate_entry(entry_entities, entry_people, entry_places, entry_events, uncategorized_characteristics, all_first_names, isVerbose = 0, entry_type = \"baptism\"):\n",
    "    '''\n",
    "    analyzes data extracted from a single ecclesiastical entry to assess accuracy of automated entity and relationship extraction\n",
    "        entry_type: baptism, marriage, or burial\n",
    "        entry_entities: df containing all entities extracted from entry (with assignment status)\n",
    "        entry_people: list of dictionaries, each containing people who appear in entry\n",
    "        entry_places: list of dictionaries, each containing places that appear in entry\n",
    "        entry_events: list of dictionaries, each containing events that appear in entry\n",
    "        \n",
    "        returns: dictionary in which key-value pairs encode various details regarding performance of automation\n",
    "    '''\n",
    "    \n",
    "    validation_dict = {}\n",
    "    \n",
    "    #Reinitialize all booleans\n",
    "    #Original base checks related to the event\n",
    "    isBaptism=0; isDateComplete=0; hasLocation=0; hasCleric=0; hasPrincipal=0; \n",
    "    hasRelations=0; hasGodparents=0; hasEnslaver=0;\n",
    "    isInfant=0; hasParents=0; isBirthEvent=0; isDateComplete_birth=0; hasLocation_birth=0;\n",
    "    isEnslaved=0; similarNames=0;\n",
    "    #\"Extra\" checks related to people\n",
    "    hasWrongEthAssgnt_ensl = 0; hasWrongEthAssgnt_cler = 0; hasUncatChars = 0; hasUnassgnEnts = 0\n",
    "    hasUncoupledParents = 0; hasUncoupledGodparents = 0; hasUnrelatedPersons=0\n",
    "    \n",
    "    my_keys = ['isBaptism', 'isDateComplete', 'hasLocation', 'hasCleric', 'hasPrincipal',\n",
    "              'hasEnslaver', 'isEnslaved', 'hasRelations', 'hasGodparents', \n",
    "              'isInfant', 'hasParents', 'isBirthEvent', 'isDateComplete_birth', 'hasLocation_birth', \n",
    "              'hasUnrelatedPersons', 'hasUncoupledParents', 'hasUncoupledGodparents','similarNames', \n",
    "              'hasWrongEthAssgnt_ensl', 'hasWrongEthAssgnt_cler', 'hasUncatChars', 'hasUnassgnEnts']\n",
    "    \n",
    "    if isVerbose:\n",
    "        print(\"Entry entities:\")\n",
    "        display(entry_entities.head())\n",
    "        print(\"Entry people:\")\n",
    "        display(entry_people)\n",
    "        print(\"Entry places:\")\n",
    "        display(entry_places)\n",
    "        print(\"Entry events:\")\n",
    "        display(entry_events)\n",
    "        #print(\"Uncategorized characteristics:\") #This is a dataframe\n",
    "    \n",
    "    if entry_type == \"baptism\":\n",
    "        #is there a baptism event? ##############################################################################\n",
    "        baptism_event = {} #Initialization\n",
    "        for bapt_idx in range(len(entry_events)):\n",
    "            isBaptism = entry_events[bapt_idx].get('type')=='baptism'\n",
    "            if isBaptism:\n",
    "                baptism_event = entry_events[bapt_idx]\n",
    "                break\n",
    "        \n",
    "        #if so:\n",
    "        if isBaptism:\n",
    "            #does baptism event have a complete date? #########################################################################\n",
    "            event_date = baptism_event.get('date')\n",
    "            isDateComplete = not ('?' in event_date) # Ex unfinished date: '????-??-22'\n",
    "            #does baptism event have a location? ##############################################################################\n",
    "            event_location = baptism_event.get('location')\n",
    "            hasLocation = (entry_places is not None) and (len(entry_places)>0)\n",
    "            \n",
    "        #is there an identified cleric? ##############################################################################\n",
    "        hasCleric = not (type(baptism_event.get('cleric'))=='NoneType')\n",
    "        if hasCleric:\n",
    "            cleric_PID = baptism_event.get('cleric')\n",
    "            \n",
    "        #is there an identified principal? ##############################################################################\n",
    "        hasPrincipal = not (type(baptism_event.get('principal'))=='NoneType')\n",
    "            \n",
    "        #if so:\n",
    "        if hasPrincipal:\n",
    "            principal_PID = baptism_event.get('principal')            \n",
    "            \n",
    "            #Loop through to find the principal's entry in entry_people\n",
    "            baptism_princ = [] #Initialization\n",
    "            for princ_idx in range(len(entry_people)):\n",
    "                foundPrinc = principal_PID in entry_people[princ_idx].get('id')\n",
    "                #Need to do \"in\" because, as per the first case, the listed PID is actually 2 PIDs appended together (separated by a ;)\n",
    "                if foundPrinc:\n",
    "                    baptism_princ.append(entry_people[princ_idx])                    \n",
    "            \n",
    "            #is the principal an infant? ##############################################################################\n",
    "            for ref in baptism_princ:\n",
    "                princ_age = ref.get('age')\n",
    "                if princ_age == 'infant': \n",
    "                    isInfant = 1\n",
    "                    break\n",
    "            \n",
    "            #are principal's godparent(s) identified? ##################################################################\n",
    "            count_godparents = 0\n",
    "            godparents_list = []\n",
    "            count_parents = 0\n",
    "            parents_list = []\n",
    "            relations = []\n",
    "            for ref in baptism_princ:\n",
    "                if ref.get(\"relationships\") is not None:\n",
    "                    for rel in ref.get(\"relationships\"):\n",
    "                        relations.append(rel)            \n",
    "            if len(relations) > 0:\n",
    "                hasRelations = 1\n",
    "                for relation in relations:\n",
    "                    if relation.get('relationship_type')=='godparent':\n",
    "                        hasGodparents = 1 \n",
    "                        godparents_list.append(relation.get('related_person'))\n",
    "                        count_godparents += 1\n",
    "                    if (relation.get('relationship_type')=='parent') and (isInfant):\n",
    "                        hasParents = 1 \n",
    "                        parents_list.append(relation.get('related_person'))\n",
    "                        count_parents += 1\n",
    "                    if relation.get('relationship_type')=='enslaver':\n",
    "                        hasEnslaver = 1\n",
    "                        #print(\"Enslaver found.  This should lead to a true for isEnslaved\")\n",
    "                        #print(relations)\n",
    "                        #print(\"------------------\")\n",
    "\n",
    "            #are there likely couples (e.g. parents, godparents) not explicitly flagged as such\n",
    "            if count_godparents>1:\n",
    "                for godparent in godparents_list:\n",
    "                    relations = [person.get('relationships') for person in entry_people if person.get('id')==godparent]\n",
    "                    if relations is not None: \n",
    "                        for rel_idx in range(len(relations)):\n",
    "                            if relations[0][rel_idx].get('relationship_type')=='husband' or relations[0][rel_idx].get('relationship_type')=='wife':\n",
    "                                #They are coupled to someone\n",
    "                                pass\n",
    "                            else:\n",
    "                                hasUncoupledGodparents = 1\n",
    "                del relations\n",
    "                \n",
    "            if count_parents>1:\n",
    "                for parent in parents_list:\n",
    "                    relations = [person.get('relationships') for person in entry_people if person.get('id')==parent]\n",
    "                    if relations is not None: \n",
    "                        for rel_idx in range(len(relations)):\n",
    "                            if relations[0][rel_idx].get('relationship_type')=='husband' or relations[0][rel_idx].get('relationship_type')=='wife':\n",
    "                                continue\n",
    "                            else:\n",
    "                                hasUncoupledParents = 1\n",
    "                                print(\"Contains uncoupled parents:\")\n",
    "                                print(relations)\n",
    "                                print(\"-------------------------\")\n",
    "                del relations\n",
    "            \n",
    "        status = [\"propiedad\", \"escrava\", \"escravos\", \"esclabo\", \"esclaba\", \"escl.a\", \"escl.o\", \"clavo\", \"clava\", \"escl\", \"escl.\", \"escl.s\", \"clabo\", \"claba\", \"esc.va\", \"esc.ba\", \"esc.vo\", \"escvo\", \"esclavo\", \"esclava\", \"escva\", \"esc.bo\", \"esclabos\", \"esclavos\", \"esc.os\", \"esc.a\", \"esc.o\", \"libre\", \"esc.s\", \"esco\", \"esca\"]\n",
    "        for ref in baptism_princ:\n",
    "            if ref.get('status') is not None:\n",
    "                #print(\"Printing principal status:\")\n",
    "                #print(baptism_princ.get('status'))\n",
    "                #print(\"---------------------\")\n",
    "                for term in status:\n",
    "                    if term in ref.get('status'):\n",
    "                        isEnslaved = 1\n",
    "                        break\n",
    "            if isEnslaved:\n",
    "                break\n",
    "        \n",
    "        ##########################################\n",
    "        if hasEnslaver and not isEnslaved:\n",
    "            print(\"Enslaver found, but thinks principal is not enslaved... Principal dict:\")\n",
    "            print(baptism_princ)\n",
    "            print(\"-------------------------\")\n",
    "        \n",
    "        #IF PRINCIPAL IS AN INFANT:###########################################################################\n",
    "        if isInfant:\n",
    "            #are principal's parent(s) identified? #Took care of this above\n",
    "            #is there a birth event? ##############################################################################\n",
    "            birth_event = {} #Initialization\n",
    "            if len(entry_events)>1:\n",
    "                for birth_idx in range(len(entry_events)):\n",
    "                    isBirthEvent = entry_events[birth_idx].get('type')=='birth'\n",
    "                    if isBirthEvent:\n",
    "                        birth_event = entry_events[birth_idx]\n",
    "                        #e.g. this assumes there is only one birth event per entry (max)\n",
    "                        break\n",
    "            #if so:\n",
    "            if isBirthEvent:\n",
    "                #does birth event have a complete date? ##################################################################\n",
    "                event_date_birth = birth_event.get('date')\n",
    "                isDateComplete_birth = not ( ('?' in event_date_birth) ) # Unfinished date: '????-??-22'\n",
    "                #does birth event have a location? #######################################################################\n",
    "                event_location_birth = birth_event.get('location')\n",
    "                if (birth_event.get('location') is not None) and (entry_places is not None) and (len(entry_places)>1):\n",
    "                    hasLocation_birth = 1\n",
    "                elif (birth_event.get('location') is not None) or (entry_places is not None) or (len(entry_places)>1):\n",
    "                    hasLocation_birth = 1\n",
    "                    print(\"Double check birth location\")\n",
    "                    print(entry_events)\n",
    "                    print(entry_places)\n",
    "                    print(\"--------------------\")\n",
    "                                \n",
    "        name_list = []        \n",
    "        #if so, and if principal is enslaved: ##########################################################################\n",
    "        #is principal's enslaver identified? ########################################################################        \n",
    "        for person_idx in range(len(entry_people)):\n",
    "                relationships = entry_people[person_idx].get('relationships')\n",
    "                if relationships is not None:\n",
    "                    for relation in relationships:\n",
    "                        #Check if enslaver has been assigned any ethnicities\n",
    "                        if (relation.get('relationship_type')=='slave') and (entry_people[person_idx].get('ethnicities') is not None) and (isEnslaved):                            \n",
    "                            hasWrongEthAssgnt_ensl = 1\n",
    "                            ################################################\n",
    "                            ################################################\n",
    "                            print(\"Enslaver with assigned ethnicities?\")\n",
    "                            print(entry_people[person_idx])\n",
    "                            print(\"-------------------------\")\n",
    "                            ################################################\n",
    "                            ################################################\n",
    "                            break\n",
    "                #Check if cleric has been assigned any ethnicities\n",
    "                elif (relationships is None) and (entry_people[person_idx].get('id') is cleric_PID) and (entry_people[person_idx].get('ethnicities') is not None):\n",
    "                    hasWrongEthAssgnt_cler = 1\n",
    "                    ################################################\n",
    "                    ################################################\n",
    "                    print(\"Cleric with assigned ethnicities?\")\n",
    "                    print(entry_people[person_idx])\n",
    "                    print(\"-------------------------\")\n",
    "                    ################################################\n",
    "                    ################################################\n",
    "                #Check for people without any role in the event (i.e. no relations, and not the cleric)\n",
    "                elif (relationships is None) and (entry_people[person_idx].get('id') is not cleric_PID):\n",
    "                    hasUnrelatedPersons = 1\n",
    "                    \n",
    "                #are there any people with very similar names that still appear separately\n",
    "                nameTemp = entry_people[person_idx].get('name')\n",
    "                name_list.append(nameTemp)\n",
    "                #1: Short name that could be a first name\n",
    "                #Make a boolean list to flag which names may be first names (based on length)\n",
    "                possibleFirstNames = []\n",
    "                fullNames = []\n",
    "                for name in name_list:\n",
    "                    if name in all_first_names:\n",
    "                        possibleFirstNames.append(name)\n",
    "                    else:\n",
    "                        fullNames.append(name)\n",
    "                #Check to see if names appear within each other (i.e. is a person double counted)\n",
    "                doubleCountedNames = []\n",
    "                for idx in range(len(possibleFirstNames)):\n",
    "                    doubleCountedNames= doubleCountedNames + ([name for name in fullNames if possibleFirstNames[idx] in name])\n",
    "                if not len(doubleCountedNames)==0:\n",
    "                    #print(\"Possible double count (first name appears in a second instance (full name))\")\n",
    "                    #print(name_list)\n",
    "                    similarNames = 1\n",
    "                #2: Two similarly-sized names, that could be variations (i.e. missing hypens or have #'s for unknown letters')\n",
    "                for name_idx in range(len(name_list)):\n",
    "                    if name_idx==0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        for idx in range(len(name_list)):\n",
    "                            for idx2 in range(len(name_list)-idx-1):\n",
    "                                idx2 = idx2+idx+1\n",
    "                                if check_lengths(name_list, idx, idx2):\n",
    "                                    #print(name_list)\n",
    "                                    similarNames = 1       \n",
    "                                    \n",
    "        #questions re characteristics:\n",
    "        #are there characteristics that were not categorized\n",
    "        num_rowsC, num_colsC = uncategorized_characteristics.shape\n",
    "        if num_rowsC>0:\n",
    "            hasUncatChars = 1\n",
    "        \n",
    "        #are there characteristics or relationships that were not assigned\n",
    "        unassigned_df = check_unassigned(entry_entities)\n",
    "        num_rowsA, num_colsA = unassigned_df.shape\n",
    "        #display(unassigned_df)\n",
    "        if num_rowsA>0:\n",
    "            hasUnassgnEnts = 1\n",
    "    else:\n",
    "        print(\"Entry is not a baptism, and is not yet supported\")\n",
    "        return {}\n",
    "    \n",
    "    my_values = [isBaptism, isDateComplete, hasLocation, hasCleric, hasPrincipal,\n",
    "              hasEnslaver, isEnslaved, hasRelations, hasGodparents, \n",
    "              isInfant, hasParents, isBirthEvent, isDateComplete_birth, hasLocation_birth, \n",
    "              hasUnrelatedPersons, hasUncoupledParents, hasUncoupledGodparents, similarNames, \n",
    "              hasWrongEthAssgnt_ensl, hasWrongEthAssgnt_cler, hasUncatChars, hasUnassgnEnts]\n",
    "    \n",
    "    val_bools = [True if elem==True else False for elem in my_values]\n",
    "    \n",
    "    validation_dict = dict(zip(my_keys,val_bools))\n",
    "    \n",
    "    #if isVerbose:\n",
    "        #print(validation_dict)\n",
    "    \n",
    "    return validation_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def process_volume(path_to_transcription, path_to_model):\n",
    "    '''\n",
    "    runs the transcription of a single volume (formatted according to SSDA markup 2.0 specs) through the ML entity extraction\n",
    "    and rules-based relationship linking pipelines, then formats resulting data for export into SQL\n",
    "        path_to_transcription: path to an XML file containing the transcription of a single volume\n",
    "        path_to_model: path to a spaCy model trained to extract entities from the proper type of volume\n",
    "    \n",
    "        returns: final people, place, and event dictionaries as well as the\n",
    "        path to a JSON file containing volume metadata as well as people, place, and event records\n",
    "    '''\n",
    "    \n",
    "    #retrieve volume metadata and controlled vocabularies\n",
    "    \n",
    "    volume_metadata = retrieve_volume_metadata(path_to_transcription)\n",
    "    images = xml_v2_to_json(path_to_transcription)\n",
    "    vocabularies = retrieve_controlled_vocabularies()\n",
    "    \n",
    "    if volume_metadata[\"country\"] == \"Brazil\":\n",
    "        lang = \"pt\"\n",
    "        language = \"portuguese\"\n",
    "    else:\n",
    "        lang = \"es\"\n",
    "        language = \"spanish\"\n",
    "        \n",
    "    #load and apply trained model\n",
    "    \n",
    "    trained_model = load_model(path_to_model, language=lang, verbose='True')\n",
    "    \n",
    "    entry_df = parse_xml_v2(path_to_transcription)\n",
    "    \n",
    "    ent_preds_df, metrics_df, per_ent_metrics = test_model(trained_model, entry_df, \"entry_no\", \"text\", score_model=False)\n",
    "    print(\"Entities extracted.\")\n",
    "    \n",
    "    #development\n",
    "    #pd.set_option(\"display.max_rows\", 101)\n",
    "    #display(ent_preds_df.head(100))\n",
    "    \n",
    "    #iterate through each entry and build relationships\n",
    "    \n",
    "    people = []\n",
    "    places = []\n",
    "    events = []\n",
    "    \n",
    "    entitiesRunning = pd.DataFrame()\n",
    "    noCategoryRunning = pd.DataFrame()\n",
    "    \n",
    "    validation_dict_ALL = []\n",
    "    \n",
    "    #file path could be passed as parameter, as could language (eventually)\n",
    "    with open(\"names.json\", encoding=\"utf-8\") as infile:\n",
    "        name_file = json.load(infile)\n",
    "        \n",
    "    names = name_file[\"names\"]\n",
    "    all_first_names = []\n",
    "    for name in names:\n",
    "        all_first_names.append(name[\"name\"])        \n",
    "    \n",
    "    for i in range(len(entry_df.index)):\n",
    "        \n",
    "        entry_no = entry_df['entry_no'][i]\n",
    "        entry_text = entry_df['text'][i]    \n",
    "    \n",
    "        entities = copy.deepcopy(ent_preds_df[ent_preds_df['entry_no'] == entry_no])\n",
    "        \n",
    "        entities[\"assigned\"] = True\n",
    "        \n",
    "        entry_people, entry_places, entry_events, entities, characteristics_df, categorized_characteristics, uncategorized_characteristics = build_entry_metadata(entry_text, entities, path_to_transcription, entry_no)             \n",
    "        \n",
    "        if uncategorized_characteristics.shape[0] > 0:\n",
    "            noCategoryRunning = noCategoryRunning.append(uncategorized_characteristics)\n",
    "        \n",
    "        #FIND ENTITIES THAT ARE UNASSIGNED OR UNCATEGORIZED\n",
    "        entity_index = 0\n",
    "        for ent_data in entities.itertuples():\n",
    "            for char_data in characteristics_df.itertuples():\n",
    "                char_index = 0\n",
    "                #characteristic is not categorized:\n",
    "                if (char_data.category == None) and (ent_data.pred_start == char_data.pred_start) and (ent_data.pred_entity == char_data.pred_entity):\n",
    "                    continue #Already dealth with\n",
    "                #characteristic is categorized but not assigned\n",
    "                elif (ent_data.pred_label == char_data.pred_label) and (ent_data.pred_start == char_data.pred_start) and (ent_data.pred_entity == char_data.pred_entity):\n",
    "                    if (char_data.assignment == None):\n",
    "                        entities.at[entity_index, \"assigned\"] = False\n",
    "                char_index += 1\n",
    "            entity_index += 1\n",
    "\n",
    "        entitiesRunning = entitiesRunning.append(entities)  \n",
    "        \n",
    "        verbosity = 0\n",
    "        \n",
    "        entry_validation_dict = validate_entry(entities, entry_people, entry_places, entry_events, uncategorized_characteristics, all_first_names, isVerbose = verbosity)\n",
    "        validation_dict_ALL.append(entry_validation_dict)\n",
    "        \n",
    "        people += entry_people\n",
    "        places += entry_places\n",
    "        events += entry_events\n",
    "    \n",
    "    noCategoryRunning.reset_index(drop = True, inplace = True)\n",
    "    noCategoryRunning[\"assigned\"] = False\n",
    "    print(\"Relationships linked.\")\n",
    "    \n",
    "    #disambiguate locations and assign unique ids\n",
    "    \n",
    "    unique_places = []\n",
    "    for place in places:\n",
    "        if (place != None) and (place not in unique_places):\n",
    "            unique_places.append(place)\n",
    "            \n",
    "    for person in people:        \n",
    "        if (person[\"origin\"] != None) and (person[\"origin\"] not in unique_places):\n",
    "            unique_places.append(person[\"origin\"])\n",
    "    \n",
    "    places = []\n",
    "    curr_place = 1\n",
    "    for unique_place in unique_places:\n",
    "        place_record = {\"id\":volume_metadata[\"id\"] + '-L' + str(curr_place), \"location\":unique_place}\n",
    "        places.append(place_record)\n",
    "        curr_place += 1\n",
    "        \n",
    "    #incorporate location ids into event metadata and person records\n",
    "    \n",
    "    for event in events:\n",
    "        location = event[\"location\"]\n",
    "        loc_id = \"unknown\"\n",
    "        if location != None:\n",
    "            for place in places:\n",
    "                if place[\"location\"] == location:\n",
    "                    loc_id = place[\"id\"]\n",
    "        if (loc_id == \"unknown\") and (location != None):\n",
    "            print(\"Failed to find location ID for \" + location)\n",
    "            event[\"location\"] = None\n",
    "        else:\n",
    "            event[\"location\"] = loc_id\n",
    "            \n",
    "        if event[\"location\"] == \"unknown\":\n",
    "            event[\"location\"] = None\n",
    "            \n",
    "    for person in people:\n",
    "        if person[\"origin\"] == None:\n",
    "            continue\n",
    "        \n",
    "        for place in places:\n",
    "            if place[\"location\"] == person[\"origin\"]:\n",
    "                person[\"origin\"] = place[\"id\"]\n",
    "                break\n",
    "        \n",
    "    #bracket missing or incomplete event dates\n",
    "    \n",
    "    incomplete_dates = []\n",
    "    last_year = None\n",
    "    last_month = None\n",
    "    last_day = None\n",
    "    \n",
    "    for e in range(len(events)):\n",
    "        curr_year = events[e][\"date\"][:4]\n",
    "        curr_month = events[e][\"date\"][5:7]\n",
    "        curr_day = events[e][\"date\"][8:]\n",
    "        \n",
    "        #fix incompletely extracted years\n",
    "        if (curr_year != \"????\") and (last_year != None) and (abs(int(curr_year) - int(last_year)) > 1):\n",
    "            if (curr_year[3] == last_year[3]):\n",
    "                curr_year = last_year                \n",
    "            elif (curr_month == \"01\") and (last_month == \"12\"):\n",
    "                curr_year = str(int(last_year) + 1)                \n",
    "            else:\n",
    "                curr_year = last_year\n",
    "            events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day\n",
    "        \n",
    "        if (curr_year == \"????\") or (curr_month == \"??\") or (curr_day == \"??\"):\n",
    "            #logic to assign dates for birth events based on associated baptism\n",
    "            if events[e][\"type\"] == \"birth\":\n",
    "                if (events[e][\"id\"][:events[e][\"id\"].find('E')] == events[e - 1][\"id\"][:events[e - 1][\"id\"].find('E')]) and (events[e - 1][\"type\"] == \"baptism\") and ('?' not in events[e - 1][\"date\"]):\n",
    "                        if (curr_month != \"??\") and (curr_day != \"??\"):\n",
    "                            if (curr_month == \"12\") and (last_month == \"01\"):\n",
    "                                curr_year = str(int(last_year) - 1)                                \n",
    "                            elif (30 * int(last_month) + int(last_day) - 30 * int(curr_month) - int(curr_day)) < 21:\n",
    "                                curr_year = last_year\n",
    "                            events[e][\"date\"] = curr_year + '-' + events[e][\"date\"][5:7] + '-' + events[e][\"date\"][8:]\n",
    "                        elif curr_month != \"??\":\n",
    "                            if (curr_month == \"12\"):\n",
    "                                curr_day = \"01\"\n",
    "                                curr_year = str(int(last_year) - 1)\n",
    "                                events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-01-01'\n",
    "                            elif (curr_month == last_month):\n",
    "                                curr_day = \"01\"\n",
    "                                curr_year = last_year\n",
    "                                events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-' + last_month + '-' + last_day\n",
    "                            elif int(curr_month) == (int(last_month) - 1):\n",
    "                                curr_day = \"01\"\n",
    "                                curr_year = last_year\n",
    "                                events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-' + last_month + '-01'                            \n",
    "                        elif curr_day != \"??\":\n",
    "                            if curr_day <= last_day:\n",
    "                                curr_year = last_year\n",
    "                                curr_month = last_month                                \n",
    "                            else:\n",
    "                                if last_month == \"01\":\n",
    "                                    curr_month = \"12\"\n",
    "                                    curr_year = str(int(last_year) - 1)\n",
    "                                else:\n",
    "                                    curr_month = str(int(last_month) - 1)                                    \n",
    "                                    if len(curr_month) < 2:\n",
    "                                        curr_month = '0' + curr_month\n",
    "                                    curr_year = last_year\n",
    "                            events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day\n",
    "                        else:\n",
    "                            if (last_month == '01') and (int(last_day) < 21):\n",
    "                                curr_year = str(int(last_year) - 1)\n",
    "                                curr_month = \"12\"\n",
    "                                curr_day = str(int(last_day) + 9)                               \n",
    "                            elif int(last_day) < 21:\n",
    "                                curr_year = last_year\n",
    "                                curr_month = str(int(last_month) - 1)\n",
    "                                if len(curr_month) < 2:\n",
    "                                    curr_month = '0' + curr_month\n",
    "                                curr_day = str(int(last_day) + 9)\n",
    "                            else:\n",
    "                                curr_year = last_year\n",
    "                                curr_month = last_month\n",
    "                                curr_day = str(int(last_day) - 20)\n",
    "                                if len(curr_day) < 2:\n",
    "                                    curr_day = '0' + curr_day\n",
    "                            events[e][\"date\"] = curr_year + '-' + curr_month + '-' + curr_day + '/' + last_year + '-' + last_month + '-' + last_day\n",
    "                            \n",
    "            if (curr_year == \"????\") or (curr_month == \"??\") or (curr_day == \"??\"):\n",
    "                incomplete_dates.append(e)\n",
    "        elif last_year == None:\n",
    "            for date in incomplete_dates:\n",
    "                events[date][\"date\"] = complete_date(events[date][\"date\"], None, curr_year + '-' + curr_month + '-' + curr_day)\n",
    "            \n",
    "            incomplete_dates = []\n",
    "            last_year = curr_year\n",
    "            last_month = curr_month\n",
    "            last_day = curr_day\n",
    "        elif (compare_dates(int(curr_year), int(curr_month), int(curr_day), int(last_year), int(last_month), int(last_day)) == '>') or (compare_dates(int(curr_year), int(curr_month), int(curr_day), int(last_year), int(last_month), int(last_day)) == '='):\n",
    "            for date in incomplete_dates:\n",
    "                events[date][\"date\"] = complete_date(events[date][\"date\"], last_year + '-' + last_month + '-' + last_day, curr_year + '-' + curr_month + '-' + curr_day)\n",
    "            \n",
    "            incomplete_dates = []\n",
    "            last_year = curr_year\n",
    "            last_month = curr_month\n",
    "            last_day = curr_day                    \n",
    "    \n",
    "    if last_year != None:\n",
    "        for date in incomplete_dates:\n",
    "            events[date][\"date\"] = complete_date(events[date][\"date\"], last_year + '-' + last_month + '-' + last_day, None)\n",
    "        \n",
    "    #merging any date brackets with equal endpoints\n",
    "    for event in events:\n",
    "        interval = event[\"date\"].split('/')\n",
    "        if (len(interval) == 2) and (interval[0] == interval[1]):\n",
    "            event[\"date\"] == interval[0]            \n",
    "        \n",
    "    print(\"Events configured.\")    \n",
    "    \n",
    "    for person in people:        \n",
    "        #strip titles and/or ranks from names\n",
    "        if person[\"name\"] != None:\n",
    "            name_parts = person[\"name\"].split(' ')\n",
    "\n",
    "            if len(name_parts) >= 2:\n",
    "                while ((name_parts[0].lower() + ' ' + name_parts[1].lower()) in vocabularies[\"titles\"]) or ((name_parts[0].lower() + ' ' + name_parts[1].lower()) in vocabularies[\"ranks\"]):\n",
    "                    if len(name_parts) == 2:\n",
    "                        person[\"name\"] = None\n",
    "                    else:\n",
    "                        person[\"name\"] = name_parts[2]\n",
    "                        for i in range(3, len(name_parts)):\n",
    "                            person[\"name\"] += ' ' + name_parts[i]\n",
    "\n",
    "                    if (name_parts[0].lower() + ' ' + name_parts[1].lower()) in vocabularies[\"titles\"]:\n",
    "                        if person[\"titles\"] != None:\n",
    "                            person[\"titles\"] += ';' + name_parts[0] + ' ' + name_parts[1]\n",
    "                        else:\n",
    "                            person[\"titles\"] = name_parts[0] + ' ' + name_parts[1]\n",
    "                    else:\n",
    "                        if person[\"ranks\"] != None:\n",
    "                            person[\"ranks\"] += ';' + name_parts[0] + ' ' + name_parts[1]\n",
    "                        else:\n",
    "                            person[\"ranks\"] = name_parts[0] + ' ' + name_parts[1]\n",
    "\n",
    "                    if person[\"name\"] == None:\n",
    "                        break\n",
    "                    name_parts = person[\"name\"].split(' ')\n",
    "                    if len(name_parts) < 2:\n",
    "                        break\n",
    "\n",
    "            if person[\"name\"] != None:\n",
    "                while (name_parts[0].lower() in vocabularies[\"titles\"]) or (name_parts[0].lower() in vocabularies[\"ranks\"]):\n",
    "                    if len(name_parts) == 1:\n",
    "                        person[\"name\"] = None\n",
    "                    else:\n",
    "                        person[\"name\"] = name_parts[1]\n",
    "                        for i in range(2, len(name_parts)):\n",
    "                            person[\"name\"] += ' ' + name_parts[i]\n",
    "\n",
    "                    if name_parts[0].lower() in vocabularies[\"titles\"]:\n",
    "                        if person[\"titles\"] != None:\n",
    "                            person[\"titles\"] += ';' + name_parts[0]\n",
    "                        else:\n",
    "                            person[\"titles\"] = name_parts[0]\n",
    "                    else:\n",
    "                        if person[\"ranks\"] != None:\n",
    "                            person[\"ranks\"] += ';' + name_parts[0]\n",
    "                        else:\n",
    "                            person[\"ranks\"] = name_parts[0]\n",
    "\n",
    "                    if person[\"name\"] == None:\n",
    "                        break\n",
    "                    name_parts = person[\"name\"].split(' ')\n",
    "                    \n",
    "    #normalize names and all characteristics\n",
    "    names = []\n",
    "    name_counts = []\n",
    "    ethnonym_vocab = retrieve_json_vocab(\"synonyms.json\", \"ethnonyms\")\n",
    "    phenotype_vocab = retrieve_json_vocab(\"synonyms.json\", \"phenotypes\", language=\"spanish\")\n",
    "    \n",
    "    for person in people:\n",
    "        #normalize characteristics and translate to English\n",
    "        for key in person:\n",
    "            if person[key] == None:\n",
    "                continue\n",
    "            if key == \"name\":\n",
    "                person[key] = normalize_text(person[key], \"synonyms.json\", context=\"name\")\n",
    "                #check extracted name for ethnonyms and/or attributed phenotypes        \n",
    "                if (person[\"name\"] != None) and (person[\"name\"] != normalize_text(person[\"name\"], \"synonyms.json\", context=\"ethnonym\")):\n",
    "                    for token in person[\"name\"].split(' '):\n",
    "                        eth_norm = normalize_text(token, \"synonyms.json\", context=\"ethnonym\")\n",
    "                        if token != eth_norm:\n",
    "                            if (person[\"ethnicities\"] == None) or (not (eth_norm in person[\"ethnicities\"])):\n",
    "                                if person[\"ethnicities\"] == None:\n",
    "                                    person[\"ethnicities\"] = eth_norm\n",
    "                                else:\n",
    "                                    person[\"ethnicities\"] = person[\"ethnicities\"] + ';' + eth_norm\n",
    "                    person[\"name\"] = normalize_text(person[\"name\"], \"synonyms.json\", context=\"ethnonym\")\n",
    "                else:\n",
    "                    for ethnonym in ethnonym_vocab:\n",
    "                        if ethnonym in person[\"name\"]:\n",
    "                            if person[\"ethnicities\"] == None:\n",
    "                                person[\"ethnicities\"] = ethnonym\n",
    "                            else:\n",
    "                                person[\"ethnicities\"] = person[\"ethnicities\"] + ';' + ethnonym\n",
    "                for phenotype in phenotype_vocab:\n",
    "                    if phenotype in normalize_text(person[key], \"synonyms.json\", context=\"characteristic\"):                    \n",
    "                        if person[\"phenotype\"] == None:\n",
    "                            person[\"phenotype\"] = phenotype\n",
    "                        else:\n",
    "                            person[\"phenotype\"] = person[\"phenotype\"] + ';' + phenotype\n",
    "                        if phenotype[-1] == 's':\n",
    "                            for token in person[\"name\"].split(' '):\n",
    "                                if normalize_text(token, \"synonyms.json\", context=\"characteristic\") == phenotype:\n",
    "                                    person[\"name\"] = person[\"name\"].replace(' ' + token, '')\n",
    "            elif key == \"ethnicities\":                \n",
    "                if person[key].find(';') == -1:\n",
    "                    person[key] = normalize_text(person[key], \"synonyms.json\", context=\"ethnonym\")                    \n",
    "                else:\n",
    "                    char_comp = person[key].split(';')\n",
    "                    person[key] = \"\"\n",
    "                    #strip out duplicate characteristics\n",
    "                    for char in char_comp:\n",
    "                        char = normalize_text(char, \"synonyms.json\", context=\"ethnonym\")                       \n",
    "                                          \n",
    "                        if not (char in person[key]):\n",
    "                            if person[key] == \"\":\n",
    "                                person[key] = char\n",
    "                            else:\n",
    "                                person[key] = person[key] + ';' + char\n",
    "            elif (key != \"id\") and (key != \"relationships\"):\n",
    "                if person[key].find(';') == -1:\n",
    "                    person[key] = normalize_text(person[key], \"synonyms.json\", context=\"characteristic\")\n",
    "                    person[key] = translate_characteristic(person[key], \"synonyms.json\", language)\n",
    "                else:\n",
    "                    char_comp = person[key].split(';')\n",
    "                    person[key] = \"\"\n",
    "                    #strip out duplicate characteristics\n",
    "                    for char in char_comp:\n",
    "                        char = normalize_text(char, \"synonyms.json\", context=\"characteristic\")                        \n",
    "                        char = translate_characteristic(char, \"synonyms.json\", language)                        \n",
    "                        if not (char in person[key]):\n",
    "                            if person[key] == \"\":\n",
    "                                person[key] = char\n",
    "                            else:\n",
    "                                person[key] = person[key] + ';' + char           \n",
    "        \n",
    "        #future improvement: find additional references for plural characteristics\n",
    "        \n",
    "        #count name frequency\n",
    "        if person[\"name\"] != None:\n",
    "            if person[\"name\"] in names:\n",
    "                name_counts[names.index(person['name'])] += 1\n",
    "            else:\n",
    "                names.append(person[\"name\"])\n",
    "                name_counts.append(1)   \n",
    "    \n",
    "    #disambiguate and merge people across the volume\n",
    "    redundant_records = []\n",
    "    merged_records = []    \n",
    "    for i in range(len(name_counts)):\n",
    "        if (name_counts[i] > .1 * len(images)) and (len(names[i].split(' ')) > 1) and (names[i] != \"Unknown principal\"):\n",
    "            records_to_merge = []            \n",
    "            for j in range(len(people)):\n",
    "                if people[j][\"name\"] == names[i]:\n",
    "                    redundant_records.append(people[j])\n",
    "                    records_to_merge.append(people[j])                    \n",
    "            merged_records.append(merge_records(records_to_merge))            \n",
    "    people = [person for person in people if person not in redundant_records]\n",
    "    for person in merged_records:\n",
    "        people.append(person)    \n",
    "    \n",
    "    print(\"People records enhanced and disambiguated.\")\n",
    "    \n",
    "    #reduce compound person IDs to single ID, add references field\n",
    "    people, events = compact_references(people, events)\n",
    "    \n",
    "    print(\"Single ID generated for each individual.\")\n",
    "    \n",
    "    #convert dictionaries into JSON    \n",
    "    with open(\"volume_records\\\\\" + volume_metadata[\"id\"] + \".json\", \"w\") as outfile:\n",
    "        outfile.write('{\\n\\\"volume\\\": \\n')\n",
    "        json.dump(volume_metadata, outfile)\n",
    "        outfile.write(',')\n",
    "        outfile.write('\\n\\\"images\\\": [\\n')\n",
    "        first_img = True\n",
    "        for image in images:\n",
    "            if first_img:\n",
    "                first_img = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")\n",
    "            json.dump(image, outfile)\n",
    "        outfile.write(\"\\n],\\n\")\n",
    "        outfile.write('\\n\\\"people\\\": [\\n')\n",
    "        first_person = True\n",
    "        for person in people:\n",
    "            if first_person:\n",
    "                first_person = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")            \n",
    "            json.dump(person, outfile)            \n",
    "        outfile.write(\"\\n],\\n\")\n",
    "        outfile.write(\"\\\"places\\\": [\\n\")\n",
    "        first_place = True\n",
    "        for place in places:\n",
    "            if first_place:\n",
    "                first_place = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")\n",
    "            json.dump(place, outfile)\n",
    "        outfile.write(\"\\n],\\n\")\n",
    "        outfile.write(\"\\\"events\\\": [\\n\")\n",
    "        first_event = True\n",
    "        for event in events:\n",
    "            if first_event:\n",
    "                first_event = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")\n",
    "            json.dump(event, outfile)\n",
    "        outfile.write(\"\\n]\\n\")\n",
    "        outfile.write('}')\n",
    "        \n",
    "    #dump validation dictionaries\n",
    "    with open(\"validation\\\\\" + volume_metadata[\"id\"] + \".json\", \"w\") as outfile:\n",
    "        outfile.write('{\\n\\\"entries\\\": [\\n')\n",
    "        first_entry = True\n",
    "        for entry in validation_dict_ALL:\n",
    "            if first_entry:\n",
    "                first_entry = False\n",
    "            else:\n",
    "                outfile.write(\",\\n\")\n",
    "            json.dump(entry, outfile)\n",
    "        outfile.write(\"\\n]\\n\")\n",
    "        outfile.write('}')\n",
    "            \n",
    "    print(\"JSON built, processing completed.\")\n",
    "            \n",
    "    return people, places, events, volume_metadata[\"id\"] + \"_ppe.json\", entitiesRunning, noCategoryRunning, validation_dict_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdmen\\miniconda3\\envs\\ssda\\lib\\site-packages\\spacy\\util.py:732: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.1 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'models/15834'\n",
      "Entities extracted.\n",
      "Enslaver found, but thinks principal is not enslaved... Principal dict:\n",
      "[{'id': '15834-1034-3-P1;15834-1034-3-P3', 'name': 'Antonia', 'origin': None, 'ethnicities': 'mina', 'age': 'adulta', 'legitimacy': None, 'occupation': None, 'phenotype': 'negra', 'status': 'Esc.va', 'titles': None, 'ranks': None, 'relationships': [{'related_person': '15834-1034-3-P5', 'relationship_type': 'godparent'}, {'related_person': '15834-1034-3-P4', 'relationship_type': 'enslaver'}]}]\n",
      "-------------------------\n",
      "Enslaver with assigned ethnicities?\n",
      "{'id': '15834-1039-4-P4', 'name': 'D. Ju.o Joseph de Justis', 'origin': None, 'ethnicities': 'Mina', 'age': None, 'legitimacy': None, 'occupation': None, 'phenotype': None, 'status': None, 'titles': None, 'ranks': None, 'relationships': [{'related_person': '15834-1039-4-P3', 'relationship_type': 'slave'}]}\n",
      "-------------------------\n",
      "Enslaver found, but thinks principal is not enslaved... Principal dict:\n",
      "[{'id': '15834-1048-2-P1;15834-1048-2-P3', 'name': 'Fernando', 'origin': None, 'ethnicities': 'Arara', 'age': 'adulto', 'legitimacy': None, 'occupation': None, 'phenotype': None, 'status': 'Esc.vo', 'titles': None, 'ranks': None, 'relationships': [{'related_person': '15834-1048-2-P5', 'relationship_type': 'godparent'}, {'related_person': '15834-1048-2-P4', 'relationship_type': 'enslaver'}]}]\n",
      "-------------------------\n",
      "Enslaver found, but thinks principal is not enslaved... Principal dict:\n",
      "[{'id': '15834-1048-3-P1;15834-1048-3-P3', 'name': 'Santiago', 'origin': None, 'ethnicities': 'Arara', 'age': 'adulto', 'legitimacy': None, 'occupation': None, 'phenotype': None, 'status': 'Esc.vo', 'titles': None, 'ranks': None, 'relationships': [{'related_person': '15834-1048-3-P5', 'relationship_type': 'godparent'}, {'related_person': '15834-1048-3-P4', 'relationship_type': 'enslaver'}]}]\n",
      "-------------------------\n",
      "Enslaver found, but thinks principal is not enslaved... Principal dict:\n",
      "[{'id': '15834-1049-1-P1;15834-1049-1-P3', 'name': 'Theresa', 'origin': None, 'ethnicities': 'conga', 'age': 'adulta', 'legitimacy': None, 'occupation': None, 'phenotype': 'negra', 'status': 'Esc.va', 'titles': None, 'ranks': None, 'relationships': [{'related_person': '15834-1049-1-P4', 'relationship_type': 'enslaver'}]}]\n",
      "-------------------------\n",
      "Enslaver found, but thinks principal is not enslaved... Principal dict:\n",
      "[{'id': '15834-1049-2-P1;15834-1049-2-P3', 'name': 'Mariana', 'origin': None, 'ethnicities': None, 'age': 'adulta', 'legitimacy': None, 'occupation': None, 'phenotype': 'negra', 'status': 'Esc.va', 'titles': None, 'ranks': None, 'relationships': [{'related_person': '15834-1049-2-P5', 'relationship_type': 'godparent'}, {'related_person': '15834-1049-2-P6', 'relationship_type': 'godparent'}, {'related_person': '15834-1049-2-P4', 'relationship_type': 'enslaver'}]}]\n",
      "-------------------------\n",
      "Enslaver found, but thinks principal is not enslaved... Principal dict:\n",
      "[{'id': '15834-1092-2-P1;15834-1092-2-P3', 'name': 'Domingo', 'origin': None, 'ethnicities': 'Carabali', 'age': None, 'legitimacy': None, 'occupation': None, 'phenotype': None, 'status': 'Esc.o', 'titles': None, 'ranks': None, 'relationships': [{'related_person': '15834-1092-2-P5', 'relationship_type': 'godparent'}, {'related_person': '15834-1092-2-P4', 'relationship_type': 'enslaver'}]}]\n",
      "-------------------------\n",
      "Enslaver found, but thinks principal is not enslaved... Principal dict:\n",
      "[{'id': '15834-1155-3-P1;15834-1155-3-P5', 'name': 'Juo Antonio', 'origin': None, 'ethnicities': None, 'age': 'niño', 'legitimacy': None, 'occupation': None, 'phenotype': None, 'status': None, 'titles': None, 'ranks': None, 'relationships': [{'related_person': '15834-1155-3-P6', 'relationship_type': 'godparent'}, {'related_person': '15834-1155-3-P7', 'relationship_type': 'enslaver'}]}]\n",
      "-------------------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\miniconda3\\envs\\ssda\\lib\\site-packages\\pandas\\core\\indexes\\range.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    384\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 10 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14740/1613804152.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#no_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpeople\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoCategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_volume\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"transcriptions\\\\15834.xml\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"models/15834\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14740/3139066896.py\u001b[0m in \u001b[0;36mprocess_volume\u001b[1;34m(path_to_transcription, path_to_model)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mentities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"assigned\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mentry_people\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry_places\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry_events\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharacteristics_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorized_characteristics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muncategorized_characteristics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_entry_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_transcription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muncategorized_characteristics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MetaFolder\\SSDA\\ssda-nlp\\ssda_nlp\\relationships.py\u001b[0m in \u001b[0;36mbuild_entry_metadata\u001b[1;34m(entry_text, entities, path_to_volume_xml, entry_number)\u001b[0m\n\u001b[0;32m   1544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1545\u001b[0m         \u001b[0mcharacteristics_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muncategorized_characteristics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategorize_characteristics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharacteristics_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1546\u001b[1;33m         \u001b[0mpeople\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorized_characteristics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0massign_characteristics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcharacteristics_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpeople_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvolume_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1547\u001b[0m         \u001b[1;31m#############################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1548\u001b[0m         \u001b[1;31m### KAI EDIT: added entities here as output ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MetaFolder\\SSDA\\ssda-nlp\\ssda_nlp\\relationships.py\u001b[0m in \u001b[0;36massign_characteristics\u001b[1;34m(entry_text, entities_df, characteristics_df, unique_individuals, volume_metadata)\u001b[0m\n\u001b[0;32m    686\u001b[0m                     \u001b[0msignal_entity_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mentities_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pred_label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msignal_entity_index\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"PER\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mentities_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pred_label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msignal_entity_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"LOC\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mentities_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pred_start\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msignal_entity_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mentities_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pred_end\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msignal_entity_index\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m                 \u001b[0mplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentities_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pred_entity\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msignal_entity_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m                 \u001b[0mmultiple\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ssda\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ssda\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ssda\\lib\\site-packages\\pandas\\core\\indexes\\range.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    385\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 10"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "people, places, events, json_path, entities, noCategory, validation_list = process_volume(\"transcriptions\\\\15834.xml\", \"models/15834\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_test\n",
    "\n",
    "unassigned_df = check_unassigned(entities, isVerbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def flatten_volume_json(path_to_volume_json, csv_root=''):\n",
    "    '''\n",
    "    flattens JSON record for a volume into six separate CSVs (volume, entries, people, relationships, places, and events)\n",
    "        path_to_volume_json: path to a volume JSON record\n",
    "        csv_root: specify directory for CSV output, including trailing /\n",
    "    \n",
    "        returns: root directory for CSVs\n",
    "    '''    \n",
    "    \n",
    "    with open(path_to_volume_json, encoding=\"utf-8\") as jsonfile:\n",
    "        data = json.load(jsonfile)\n",
    "        \n",
    "    volume_id = data[\"volume\"][\"id\"]\n",
    "    \n",
    "    with open(csv_root + volume_id + \"_volume.csv\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        keys = 0\n",
    "        for key in data[\"volume\"]:\n",
    "            outfile.write(key)\n",
    "            keys += 1\n",
    "            if keys == len(data[\"volume\"]):\n",
    "                outfile.write('\\n')\n",
    "            else:\n",
    "                outfile.write(',')\n",
    "        keys = 0\n",
    "        for key in data[\"volume\"]:\n",
    "            outfile.write('\"' + data[\"volume\"][key] + '\"')            \n",
    "            keys += 1\n",
    "            if keys == len(data[\"volume\"]):\n",
    "                break\n",
    "            else:\n",
    "                outfile.write(',')\n",
    "                \n",
    "    with open(csv_root + volume_id + \"_entries.csv\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        outfile.write(\"entry id,entry text\\n\")\n",
    "        for image in data[\"images\"]:                        \n",
    "            image_id = volume_id + '-' + image[\"id\"]            \n",
    "            for entry in image[\"entries\"]:\n",
    "                entry_id = image_id + '-' + str(entry[\"id\"])\n",
    "                entry_text = entry[\"text\"]\n",
    "                outfile.write(entry_id + ',' + '\"' + entry_text + '\"\\n')\n",
    "                \n",
    "    with open(csv_root + volume_id + \"_people.csv\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        outfile.write(\"id,name,origin,ethnicity,age,legitimacy,occupation,phenotype,status,titles,ranks,references\\n\")\n",
    "        relationships = []\n",
    "        for person in data[\"people\"]:\n",
    "            for key in person:\n",
    "                if key == \"relationships\": \n",
    "                    if person[key] == None:\n",
    "                        continue\n",
    "                    for relationship in person[key]:                       \n",
    "                        if relationship[\"relationship_type\"] == \"godchild\":\n",
    "                            inverse_relationship_type = \"godparent\"\n",
    "                        elif relationship[\"relationship_type\"] == \"godparent\":\n",
    "                            inverse_relationship_type = \"godchild\"\n",
    "                        elif relationship[\"relationship_type\"] == \"grandparent\":\n",
    "                            inverse_relationship_type = \"grandchild\"\n",
    "                        elif relationship[\"relationship_type\"] == \"grandchild\":\n",
    "                            inverse_relationship_type = \"grandparent\"\n",
    "                        elif relationship[\"relationship_type\"] == \"parent\":\n",
    "                            inverse_relationship_type = \"child\"\n",
    "                        elif relationship[\"relationship_type\"] == \"child\":\n",
    "                            inverse_relationship_type = \"parent\"\n",
    "                        elif relationship[\"relationship_type\"] == \"slave\":\n",
    "                            inverse_relationship_type = \"enslaver\"\n",
    "                        elif relationship[\"relationship_type\"] == \"enslaver\":\n",
    "                            inverse_relationship_type = \"slave\"\n",
    "                        else:\n",
    "                            inverse_relationship_type = relationship[\"relationship_type\"]\n",
    "                            \n",
    "                        inverse_relationship = {\"from\": relationship[\"related_person\"], \"to\": person[\"id\"], \"type\": inverse_relationship_type}\n",
    "                        if not (inverse_relationship in relationships):\n",
    "                            relationships.append({\"from\": person[\"id\"], \"to\": relationship[\"related_person\"], \"type\": relationship[\"relationship_type\"]})\n",
    "                        \n",
    "                elif key == \"references\":\n",
    "                    references = person[key][0]\n",
    "                    for index in range(1, len(person[key])):\n",
    "                        references += ';' + person[key][index]\n",
    "                    outfile.write(references + '\\n')\n",
    "                elif person[key] == None:\n",
    "                    outfile.write(',')\n",
    "                else:\n",
    "                    outfile.write(person[key] + ',')\n",
    "                    \n",
    "    with open(csv_root + volume_id + \"_relationships.csv\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        outfile.write(\"from id,to id,relationship type\\n\")\n",
    "        for relationship in relationships:\n",
    "            outfile.write(relationship[\"from\"] + ',' + relationship[\"to\"] + ',' + relationship[\"type\"] + '\\n')\n",
    "            \n",
    "    with open(csv_root + volume_id + \"_places.csv\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        outfile.write(\"id,location\\n\")\n",
    "        for place in data[\"places\"]:\n",
    "            outfile.write(place[\"id\"] + ',' + place[\"location\"] + '\\n')\n",
    "            \n",
    "    with open(csv_root + volume_id + \"_events.csv\", 'w', encoding=\"utf-8\") as outfile:\n",
    "        outfile.write(\"id,type,principal,date,location id,cleric\\n\")\n",
    "        for event in data[\"events\"]:\n",
    "            for key in event:\n",
    "                if event[key] == None:\n",
    "                    event[key] = ''\n",
    "            outfile.write(event[\"id\"] + ',' + event[\"type\"] + ',' + event[\"principal\"] + ',' + event[\"date\"] + ',' + event[\"location\"] + ',' + event[\"cleric\"] + '\\n')   \n",
    "                \n",
    "    return csv_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_test\n",
    "\n",
    "flatten_volume_json(\"volume_records/15834.json\", csv_root = \"volume_records/csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_test\n",
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
